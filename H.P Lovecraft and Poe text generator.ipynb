{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks for H.P Lovecraft text generation\n",
    "\n",
    "\"The color out of space\" is one of my favorite tales from Lovecraft, i will use it(as well as others as the call of cthulhu) to create a recurrent neural network in tensorflow that learns his style and generates new text in his style\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn) and an example from \"Deep Learning Nanodegree\" on udacity. Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. \n",
    "\n",
    "## General architecture using \"Long short term memory\" units in the recurrent layers\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime ,localtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run only the  first time nltk is used to download language\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define conf variables and hyper parameteters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = \"characters\" #characters or words\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 128       # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 768         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.0001# Learning rate\n",
    "keep_prob = 0.7       # Dropout keep probability\n",
    "\n",
    "resume_from_checkpoint = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define base text\n",
    "Once trained ,the network can take base text and a sequence size and generate new text using base text as first characters in the sequence. For every element in base text wi will create a list that will store generated text as training goes, to be able to compare results between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_to_try = [\"In the first place\",\"the night before\",\"horror\",\"creature\",\"night\",\"dream\",\"thing\",\"That night\",\"mountain\",\"Ammi\",\"Cthulhu\",\"raven\",\"bird\",\"nevermore\",\"dead\",\"The bird\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that separates text into tokens(for whitespace characters, only new line is implemented, missing tabs and others="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " 'o',\n",
       " ',',\n",
       " ' ',\n",
       " 'm',\n",
       " 'y',\n",
       " ' ',\n",
       " 'n',\n",
       " 'a',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'L',\n",
       " 'u',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'L',\n",
       " 'e',\n",
       " 'a',\n",
       " 'l',\n",
       " '!',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'F',\n",
       " 'r',\n",
       " 'o',\n",
       " 'm',\n",
       " ' ',\n",
       " 'G',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " 'm',\n",
       " 'a',\n",
       " 'l',\n",
       " 'a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_by_words(text):\n",
    "    text = text.replace(\"\\n\",\" new_line_token \")\n",
    "    tokens = []\n",
    "    splitted =[[word_tokenize(w),' ']for w in text.split()]\n",
    "    splitted = list(itertools.chain(*list(itertools.chain(*splitted))))\n",
    "    \n",
    "    token_list = []\n",
    "    i = 0\n",
    "    while i < len(splitted):\n",
    "        if splitted[i] == \"new_line_token\":\n",
    "            if   token_list[-1]==\" \":\n",
    "                token_list[-1] = splitted[i]\n",
    "            else:\n",
    "                token_list.append(splitted[i])\n",
    "            i+=1\n",
    "        else:\n",
    "            token_list.append(splitted[i])\n",
    "        i+=1\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "def tokenize_by_characters(text):\n",
    "    return list(text)\n",
    "\n",
    "def tokenize_text(text,mode=\"characters\"):\n",
    "    if mode == \"characters\":\n",
    "        return tokenize_by_characters(text)\n",
    "    elif mode == \"words\":\n",
    "        return tokenize_by_words(text)\n",
    "    \n",
    "tokenize_text(\"Hello, my name is Luis Leal!\\n\\nFrom Guatemala\",mode)\n",
    "#tokenize_text(\"Hello, my name is Luis Leal!\\n\\nFrom Guatemala\",\"words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('corpus.txt', 'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(tokenize_text(text,mode))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a little portion of text for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_text = tokenize_text(text,mode)\n",
    "encoded_dataset = np.array([vocab_to_int[c] for c in tokenized_text if c in vocab_to_int], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size = batch_size * num_steps #create a single baty\n",
    "validation_start_index = len(encoded_dataset) - validation_size\n",
    "\n",
    "encoded = encoded_dataset[:validation_start_index]\n",
    "encoded_val = encoded_dataset[validation_start_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arefully embalmed; leaving instructions to his executors pro tem., that they should cause him to be \n"
     ]
    }
   ],
   "source": [
    "def encoded_to_text(encoded):\n",
    "    return \"\".join([int_to_vocab[number] for number in encoded])\n",
    "\n",
    "print(encoded_to_text(encoded_val[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_text =encoded_to_text(encoded_val)\n",
    "text = encoded_to_text(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters of train and validation, make sure everything is peachy.  line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE COLOUR OUT OF SPACE\\n\\nWest of Arkham the hills rise wild, and there are valleys with deep woods t'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arefully embalmed; leaving instructions to his executors pro tem., that they should cause him to be '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integersin both train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 90, 59, 96, 69, 17, 41, 17, 84, 83, 96, 17, 84, 44, 96, 17, 86,\n",
       "       96,  1, 38, 53, 69, 59, 52, 52, 25, 23, 27, 45, 96, 35, 81, 96, 53,\n",
       "       97, 18, 29, 68, 94, 96, 45, 29, 23, 96, 29, 19, 37, 37, 27, 96, 97,\n",
       "       19, 27, 23, 96,  8, 19, 37, 10,  5, 96, 68, 12, 10, 96, 45, 29, 23,\n",
       "       97, 23, 96, 68, 97, 23, 96,  6, 68, 37, 37, 23, 54, 27, 96,  8, 19,\n",
       "       45, 29, 96, 10, 23, 23, 78, 96,  8, 35, 35, 10, 27, 96, 45], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68, 97, 23, 81, 28, 37, 37, 54, 96, 23, 94, 34, 68, 37, 94, 23, 10,\n",
       "       93, 96, 37, 23, 68,  6, 19, 12, 95, 96, 19, 12, 27, 45, 97, 28, 98,\n",
       "       45, 19, 35, 12, 27, 96, 45, 35, 96, 29, 19, 27, 96, 23,  4, 23, 98,\n",
       "       28, 45, 35, 97, 27, 96, 78, 97, 35, 96, 45, 23, 94, 70,  5, 96, 45,\n",
       "       29, 68, 45, 96, 45, 29, 23, 54, 96, 27, 29, 35, 28, 37, 10, 96, 98,\n",
       "       68, 28, 27, 23, 96, 29, 19, 94, 96, 45, 35, 96, 34, 23, 96], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_val[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual english tokens, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "We have our text encoded as integers as one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array `arr`, you divide the length of `arr` by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`n_seqs` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "```python\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "```\n",
    "where `x` is the input batch and `y` is the target batch.\n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps \n",
    "    n_batches =  len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr =  arr[:n_batches*batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:,n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros(x.shape)\n",
    "        y[:,:-1],y[:,-1] = x[:,1:] ,x[:,0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[44 90 59 96 69 17 41 17 84 83]\n",
      " [96 45 29 23 96 37 68 97 95 23]\n",
      " [45 23 12 27 23 96 29 23 68 45]\n",
      " [23 96 98 35 28 37 10 96 27 23]\n",
      " [45 35 96 34 23 68 45  5 52 68]\n",
      " [35 28 97 12 68 37 96 28 12 45]\n",
      " [29 23 96 95 37 68 27 27 70 96]\n",
      " [12 10 96 34 23 81 35 97 23 96]\n",
      " [77 68 12 10 96 19 12 10 23 23]\n",
      " [19 98 29 96 94 68 12 54 96 27]]\n",
      "\n",
      "y\n",
      " [[ 90.  59.  96.  69.  17.  41.  17.  84.  83.  96.]\n",
      " [ 45.  29.  23.  96.  37.  68.  97.  95.  23.  96.]\n",
      " [ 23.  12.  27.  23.  96.  29.  23.  68.  45.  96.]\n",
      " [ 96.  98.  35.  28.  37.  10.  96.  27.  23.  23.]\n",
      " [ 35.  96.  34.  23.  68.  45.   5.  52.  68.  12.]\n",
      " [ 28.  97.  12.  68.  37.  96.  28.  12.  45.  19.]\n",
      " [ 23.  96.  95.  37.  68.  27.  27.  70.  96.  17.]\n",
      " [ 10.  96.  34.  23.  81.  35.  97.  23.  96.  37.]\n",
      " [ 68.  12.  10.  96.  19.  12.  10.  23.  23.  10.]\n",
      " [ 98.  29.  96.  94.  68.  12.  54.  96.  27.  23.]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32,[batch_size,num_steps],name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32,[batch_size,num_steps],name=\"targets\")\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. For example,\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow will create different weight matrices for all `cell` objects. Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    # Add dropout to the cell outputs\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper( tf.contrib.rnn.BasicLSTMCell(lstm_size),output_keep_prob = keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output,axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size),stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros([out_size]))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits =  tf.add(tf.matmul(x,softmax_w),softmax_b) \n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits,name =\"out\")\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets,num_classes)\n",
    "    y_reshaped =  tf.reshape(y_one_hot,logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip,global_step):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        global_step: to control the total number of train steps\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars),global_step)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.global_step_tensor = tf.Variable(0,trainable=False,name = \"global_step\")\n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size,num_steps)\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size,num_layers,batch_size,self.keep_prob)\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs,num_classes)\n",
    "        \n",
    "        self.grad_clip  = grad_clip\n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs,lstm_size,num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits,self.targets,lstm_size,num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss,learning_rate,grad_clip,self.global_step_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network:. \n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number_of_parameters():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        #print(shape)\n",
    "        #print(len(shape))\n",
    "        variable_parameters = 1\n",
    "        \n",
    "        for dim in shape:\n",
    "            #print(dim)\n",
    "            variable_parameters*=dim.value\n",
    "        #print(variable_parameters)\n",
    "        total_parameters+= variable_parameters\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = {\"train\":[],\"validation\":[]}\n",
    "x_steps = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting at time: 2017-10-30 09:38:12\n",
      "Number of parameters: 7475814 Dataset size: 2841832\n",
      "Epoch: 1/500...  Training Step: 1...  Training loss: 4.6311...  Val loss: 4.6144...  0.5177 sec/batch\n",
      "Epoch: 1/500...  Training Step: 26...  Training loss: 3.2051...  Val loss: 3.1578...  0.3615 sec/batch\n",
      "Epoch: 1/500...  Training Step: 51...  Training loss: 3.1351...  Val loss: 3.0869...  0.3615 sec/batch\n",
      "Epoch: 1/500...  Training Step: 76...  Training loss: 3.1061...  Val loss: 3.0694...  0.3652 sec/batch\n",
      "Epoch: 1/500...  Training Step: 101...  Training loss: 3.0976...  Val loss: 3.0620...  0.3616 sec/batch\n",
      "Epoch: 1/500...  Training Step: 126...  Training loss: 3.0892...  Val loss: 3.0591...  0.3620 sec/batch\n",
      "Epoch: 1/500...  Training Step: 151...  Training loss: 3.0560...  Val loss: 3.0510...  0.3630 sec/batch\n",
      "Epoch: 1/500...  Training Step: 176...  Training loss: 3.0786...  Val loss: 3.0451...  0.3626 sec/batch\n",
      "Epoch: 1/500...  Training Step: 201...  Training loss: 3.0555...  Val loss: 3.0360...  0.3625 sec/batch\n",
      "Epoch: 2/500...  Training Step: 226...  Training loss: 3.0231...  Val loss: 3.0113...  0.3657 sec/batch\n",
      "Epoch: 2/500...  Training Step: 251...  Training loss: 2.9875...  Val loss: 2.9858...  0.3624 sec/batch\n",
      "Epoch: 2/500...  Training Step: 276...  Training loss: 2.9647...  Val loss: 2.9408...  0.3631 sec/batch\n",
      "Epoch: 2/500...  Training Step: 301...  Training loss: 2.9042...  Val loss: 2.9033...  0.3635 sec/batch\n",
      "Epoch: 2/500...  Training Step: 326...  Training loss: 2.8377...  Val loss: 2.8230...  0.3631 sec/batch\n",
      "Epoch: 2/500...  Training Step: 351...  Training loss: 2.8242...  Val loss: 2.7669...  0.3666 sec/batch\n",
      "Epoch: 2/500...  Training Step: 376...  Training loss: 2.7402...  Val loss: 2.7051...  0.3678 sec/batch\n",
      "Epoch: 2/500...  Training Step: 401...  Training loss: 2.6825...  Val loss: 2.6486...  0.3630 sec/batch\n",
      "Epoch: 2/500...  Training Step: 426...  Training loss: 2.6437...  Val loss: 2.6088...  0.3631 sec/batch\n",
      "Epoch: 3/500...  Training Step: 451...  Training loss: 2.6238...  Val loss: 2.5669...  0.3627 sec/batch\n",
      "Epoch: 3/500...  Training Step: 476...  Training loss: 2.5644...  Val loss: 2.5375...  0.3805 sec/batch\n",
      "Epoch: 3/500...  Training Step: 501...  Training loss: 2.5528...  Val loss: 2.5237...  0.3716 sec/batch\n",
      "Epoch: 3/500...  Training Step: 526...  Training loss: 2.5457...  Val loss: 2.4988...  0.3769 sec/batch\n",
      "Epoch: 3/500...  Training Step: 551...  Training loss: 2.4922...  Val loss: 2.4802...  0.3950 sec/batch\n",
      "Epoch: 3/500...  Training Step: 576...  Training loss: 2.4920...  Val loss: 2.4665...  0.3632 sec/batch\n",
      "Epoch: 3/500...  Training Step: 601...  Training loss: 2.4850...  Val loss: 2.4492...  0.3634 sec/batch\n",
      "Epoch: 3/500...  Training Step: 626...  Training loss: 2.4710...  Val loss: 2.4483...  0.3724 sec/batch\n",
      "Epoch: 3/500...  Training Step: 651...  Training loss: 2.4574...  Val loss: 2.4337...  0.3635 sec/batch\n",
      "Epoch: 4/500...  Training Step: 676...  Training loss: 2.4764...  Val loss: 2.4226...  0.3633 sec/batch\n",
      "Epoch: 4/500...  Training Step: 701...  Training loss: 2.4288...  Val loss: 2.4084...  0.3635 sec/batch\n",
      "Epoch: 4/500...  Training Step: 726...  Training loss: 2.4384...  Val loss: 2.3950...  0.3640 sec/batch\n",
      "Epoch: 4/500...  Training Step: 751...  Training loss: 2.4119...  Val loss: 2.3837...  0.3667 sec/batch\n",
      "Epoch: 4/500...  Training Step: 776...  Training loss: 2.3979...  Val loss: 2.3775...  0.3635 sec/batch\n",
      "Epoch: 4/500...  Training Step: 801...  Training loss: 2.3819...  Val loss: 2.3642...  0.3631 sec/batch\n",
      "Epoch: 4/500...  Training Step: 826...  Training loss: 2.4038...  Val loss: 2.3589...  0.3666 sec/batch\n",
      "Epoch: 4/500...  Training Step: 851...  Training loss: 2.3744...  Val loss: 2.3461...  0.3637 sec/batch\n",
      "Epoch: 4/500...  Training Step: 876...  Training loss: 2.3707...  Val loss: 2.3374...  0.3654 sec/batch\n",
      "Epoch: 5/500...  Training Step: 901...  Training loss: 2.3498...  Val loss: 2.3360...  0.3638 sec/batch\n",
      "Epoch: 5/500...  Training Step: 926...  Training loss: 2.3421...  Val loss: 2.3203...  0.3631 sec/batch\n",
      "Epoch: 5/500...  Training Step: 951...  Training loss: 2.3421...  Val loss: 2.3177...  0.3682 sec/batch\n",
      "Epoch: 5/500...  Training Step: 976...  Training loss: 2.3026...  Val loss: 2.3092...  0.3639 sec/batch\n",
      "Epoch: 5/500...  Training Step: 1001...  Training loss: 2.3308...  Val loss: 2.3023...  0.3640 sec/batch\n",
      "Epoch: 5/500...  Training Step: 1026...  Training loss: 2.3101...  Val loss: 2.2985...  0.3634 sec/batch\n",
      "Epoch: 5/500...  Training Step: 1051...  Training loss: 2.2893...  Val loss: 2.2896...  0.3681 sec/batch\n",
      "Epoch: 5/500...  Training Step: 1076...  Training loss: 2.3207...  Val loss: 2.2765...  0.3634 sec/batch\n",
      "Epoch: 5/500...  Training Step: 1101...  Training loss: 2.3177...  Val loss: 2.2735...  0.3651 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1126...  Training loss: 2.2798...  Val loss: 2.2675...  0.3631 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1151...  Training loss: 2.2813...  Val loss: 2.2574...  0.3640 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1176...  Training loss: 2.2487...  Val loss: 2.2512...  0.3670 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1201...  Training loss: 2.2684...  Val loss: 2.2478...  0.3636 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1226...  Training loss: 2.2829...  Val loss: 2.2400...  0.3644 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1251...  Training loss: 2.2452...  Val loss: 2.2334...  0.3635 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1276...  Training loss: 2.2739...  Val loss: 2.2257...  0.3636 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1301...  Training loss: 2.2265...  Val loss: 2.2219...  0.3703 sec/batch\n",
      "Epoch: 6/500...  Training Step: 1326...  Training loss: 2.2400...  Val loss: 2.2182...  0.3631 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1351...  Training loss: 2.2137...  Val loss: 2.2110...  0.3624 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1376...  Training loss: 2.2026...  Val loss: 2.1990...  0.3840 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1401...  Training loss: 2.2144...  Val loss: 2.1961...  0.3636 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1426...  Training loss: 2.2090...  Val loss: 2.1884...  0.3637 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1451...  Training loss: 2.2110...  Val loss: 2.1841...  0.3637 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1476...  Training loss: 2.1911...  Val loss: 2.1755...  0.3632 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1501...  Training loss: 2.2200...  Val loss: 2.1755...  0.3668 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1526...  Training loss: 2.1911...  Val loss: 2.1702...  0.3634 sec/batch\n",
      "Epoch: 7/500...  Training Step: 1551...  Training loss: 2.1911...  Val loss: 2.1630...  0.3633 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1576...  Training loss: 2.1741...  Val loss: 2.1489...  0.3677 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1601...  Training loss: 2.1467...  Val loss: 2.1429...  0.3641 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1626...  Training loss: 2.1658...  Val loss: 2.1467...  0.3642 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1651...  Training loss: 2.1430...  Val loss: 2.1404...  0.3636 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1676...  Training loss: 2.1647...  Val loss: 2.1339...  0.3637 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1701...  Training loss: 2.1763...  Val loss: 2.1222...  0.3674 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1726...  Training loss: 2.1444...  Val loss: 2.1202...  0.3637 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1751...  Training loss: 2.1434...  Val loss: 2.1148...  0.3631 sec/batch\n",
      "Epoch: 8/500...  Training Step: 1776...  Training loss: 2.1340...  Val loss: 2.1066...  0.3640 sec/batch\n",
      "Epoch: 9/500...  Training Step: 1801...  Training loss: 2.1031...  Val loss: 2.1051...  0.3628 sec/batch\n",
      "Epoch: 9/500...  Training Step: 1826...  Training loss: 2.1413...  Val loss: 2.1023...  0.3653 sec/batch\n",
      "Epoch: 9/500...  Training Step: 1851...  Training loss: 2.0885...  Val loss: 2.0921...  0.3632 sec/batch\n",
      "Epoch: 9/500...  Training Step: 1876...  Training loss: 2.1091...  Val loss: 2.0884...  0.3633 sec/batch\n",
      "Epoch: 9/500...  Training Step: 1901...  Training loss: 2.1049...  Val loss: 2.0850...  0.3639 sec/batch\n",
      "Epoch: 9/500...  Training Step: 1926...  Training loss: 2.1173...  Val loss: 2.0770...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/500...  Training Step: 1951...  Training loss: 2.1159...  Val loss: 2.0774...  0.3656 sec/batch\n",
      "Epoch: 9/500...  Training Step: 1976...  Training loss: 2.1032...  Val loss: 2.0720...  0.3636 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2001...  Training loss: 2.0961...  Val loss: 2.0606...  0.3643 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2026...  Training loss: 2.0334...  Val loss: 2.0598...  0.3663 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2051...  Training loss: 2.0451...  Val loss: 2.0549...  0.3640 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2076...  Training loss: 2.0372...  Val loss: 2.0512...  0.3631 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2101...  Training loss: 2.0399...  Val loss: 2.0404...  0.3633 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2126...  Training loss: 2.0689...  Val loss: 2.0409...  0.3640 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2151...  Training loss: 2.0660...  Val loss: 2.0351...  0.3662 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2176...  Training loss: 2.0570...  Val loss: 2.0335...  0.3634 sec/batch\n",
      "Epoch: 10/500...  Training Step: 2201...  Training loss: 2.0699...  Val loss: 2.0278...  0.3632 sec/batch\n",
      "Epoch 10/500 time:82.0817244052887...  finished at 2017-10-30 09:51:54\n",
      "Epoch: 11/500...  Training Step: 2226...  Training loss: 2.0536...  Val loss: 2.0289...  0.3633 sec/batch\n",
      "Epoch: 11/500...  Training Step: 2251...  Training loss: 2.0239...  Val loss: 2.0169...  0.3636 sec/batch\n",
      "Epoch: 11/500...  Training Step: 2276...  Training loss: 2.0376...  Val loss: 2.0121...  0.3659 sec/batch\n",
      "Epoch: 11/500...  Training Step: 2301...  Training loss: 1.9925...  Val loss: 2.0085...  0.3638 sec/batch\n",
      "Epoch: 11/500...  Training Step: 2326...  Training loss: 2.0135...  Val loss: 2.0042...  0.3628 sec/batch\n",
      "Epoch: 11/500...  Training Step: 2351...  Training loss: 1.9873...  Val loss: 2.0067...  0.3636 sec/batch\n",
      "Epoch: 11/500...  Training Step: 2376...  Training loss: 2.0024...  Val loss: 1.9992...  0.3622 sec/batch\n",
      "Epoch: 11/500...  Training Step: 2401...  Training loss: 2.0131...  Val loss: 1.9945...  0.3661 sec/batch\n",
      "Epoch: 11/500...  Training Step: 2426...  Training loss: 1.9948...  Val loss: 1.9887...  0.3633 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2451...  Training loss: 1.9755...  Val loss: 1.9840...  0.3634 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2476...  Training loss: 1.9840...  Val loss: 1.9799...  0.3617 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2501...  Training loss: 2.0064...  Val loss: 1.9817...  0.3632 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2526...  Training loss: 1.9770...  Val loss: 1.9755...  0.3633 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2551...  Training loss: 1.9745...  Val loss: 1.9675...  0.3634 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2576...  Training loss: 1.9625...  Val loss: 1.9692...  0.3635 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2601...  Training loss: 1.9855...  Val loss: 1.9647...  0.3654 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2626...  Training loss: 1.9567...  Val loss: 1.9567...  0.3638 sec/batch\n",
      "Epoch: 12/500...  Training Step: 2651...  Training loss: 1.9383...  Val loss: 1.9576...  0.3635 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2676...  Training loss: 1.9666...  Val loss: 1.9529...  0.3630 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2701...  Training loss: 1.9441...  Val loss: 1.9431...  0.3631 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2726...  Training loss: 1.9672...  Val loss: 1.9452...  0.3659 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2751...  Training loss: 1.9571...  Val loss: 1.9412...  0.3631 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2776...  Training loss: 1.9415...  Val loss: 1.9376...  0.3639 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2801...  Training loss: 1.9395...  Val loss: 1.9340...  0.3636 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2826...  Training loss: 1.9416...  Val loss: 1.9335...  0.3634 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2851...  Training loss: 1.9436...  Val loss: 1.9285...  0.3654 sec/batch\n",
      "Epoch: 13/500...  Training Step: 2876...  Training loss: 1.9148...  Val loss: 1.9203...  0.3638 sec/batch\n",
      "Epoch: 14/500...  Training Step: 2901...  Training loss: 1.9310...  Val loss: 1.9178...  0.3632 sec/batch\n",
      "Epoch: 14/500...  Training Step: 2926...  Training loss: 1.8902...  Val loss: 1.9102...  0.3633 sec/batch\n",
      "Epoch: 14/500...  Training Step: 2951...  Training loss: 1.9184...  Val loss: 1.9122...  0.3641 sec/batch\n",
      "Epoch: 14/500...  Training Step: 2976...  Training loss: 1.8918...  Val loss: 1.9168...  0.3663 sec/batch\n",
      "Epoch: 14/500...  Training Step: 3001...  Training loss: 1.9065...  Val loss: 1.9143...  0.3634 sec/batch\n",
      "Epoch: 14/500...  Training Step: 3026...  Training loss: 1.9036...  Val loss: 1.9008...  0.3633 sec/batch\n",
      "Epoch: 14/500...  Training Step: 3051...  Training loss: 1.9028...  Val loss: 1.9052...  0.3657 sec/batch\n",
      "Epoch: 14/500...  Training Step: 3076...  Training loss: 1.9230...  Val loss: 1.9043...  0.3634 sec/batch\n",
      "Epoch: 14/500...  Training Step: 3101...  Training loss: 1.8963...  Val loss: 1.8938...  0.3637 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3126...  Training loss: 1.9141...  Val loss: 1.8877...  0.3635 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3151...  Training loss: 1.8759...  Val loss: 1.8840...  0.3637 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3176...  Training loss: 1.8629...  Val loss: 1.8842...  0.3639 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3201...  Training loss: 1.8447...  Val loss: 1.8808...  0.3629 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3226...  Training loss: 1.8799...  Val loss: 1.8805...  0.3628 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3251...  Training loss: 1.8665...  Val loss: 1.8809...  0.3633 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3276...  Training loss: 1.8688...  Val loss: 1.8766...  0.3638 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3301...  Training loss: 1.8663...  Val loss: 1.8746...  0.3646 sec/batch\n",
      "Epoch: 15/500...  Training Step: 3326...  Training loss: 1.8717...  Val loss: 1.8674...  0.3629 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3351...  Training loss: 1.8655...  Val loss: 1.8650...  0.3634 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3376...  Training loss: 1.8563...  Val loss: 1.8597...  0.3633 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3401...  Training loss: 1.8652...  Val loss: 1.8590...  0.3633 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3426...  Training loss: 1.8307...  Val loss: 1.8551...  0.3644 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3451...  Training loss: 1.9100...  Val loss: 1.8460...  0.3635 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3476...  Training loss: 1.8539...  Val loss: 1.8536...  0.3639 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3501...  Training loss: 1.8641...  Val loss: 1.8504...  0.3643 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3526...  Training loss: 1.8473...  Val loss: 1.8419...  0.3634 sec/batch\n",
      "Epoch: 16/500...  Training Step: 3551...  Training loss: 1.8492...  Val loss: 1.8438...  0.3629 sec/batch\n",
      "Epoch: 17/500...  Training Step: 3576...  Training loss: 1.8290...  Val loss: 1.8355...  0.3632 sec/batch\n",
      "Epoch: 17/500...  Training Step: 3601...  Training loss: 1.8374...  Val loss: 1.8339...  0.3638 sec/batch\n",
      "Epoch: 17/500...  Training Step: 3626...  Training loss: 1.7945...  Val loss: 1.8312...  0.3645 sec/batch\n",
      "Epoch: 17/500...  Training Step: 3651...  Training loss: 1.8147...  Val loss: 1.8328...  0.3639 sec/batch\n",
      "Epoch: 17/500...  Training Step: 3676...  Training loss: 1.8361...  Val loss: 1.8225...  0.3633 sec/batch\n",
      "Epoch: 17/500...  Training Step: 3701...  Training loss: 1.8207...  Val loss: 1.8238...  0.3630 sec/batch\n",
      "Epoch: 17/500...  Training Step: 3726...  Training loss: 1.8271...  Val loss: 1.8141...  0.3633 sec/batch\n",
      "Epoch: 17/500...  Training Step: 3751...  Training loss: 1.8310...  Val loss: 1.8187...  0.3646 sec/batch\n",
      "Epoch: 18/500...  Training Step: 3776...  Training loss: 1.7975...  Val loss: 1.8077...  0.3633 sec/batch\n",
      "Epoch: 18/500...  Training Step: 3801...  Training loss: 1.8083...  Val loss: 1.8068...  0.3635 sec/batch\n",
      "Epoch: 18/500...  Training Step: 3826...  Training loss: 1.7751...  Val loss: 1.8050...  0.3656 sec/batch\n",
      "Epoch: 18/500...  Training Step: 3851...  Training loss: 1.7820...  Val loss: 1.8017...  0.3634 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/500...  Training Step: 3876...  Training loss: 1.7847...  Val loss: 1.8005...  0.3645 sec/batch\n",
      "Epoch: 18/500...  Training Step: 3901...  Training loss: 1.7866...  Val loss: 1.8013...  0.3632 sec/batch\n",
      "Epoch: 18/500...  Training Step: 3926...  Training loss: 1.7842...  Val loss: 1.7943...  0.3631 sec/batch\n",
      "Epoch: 18/500...  Training Step: 3951...  Training loss: 1.8132...  Val loss: 1.7984...  0.3658 sec/batch\n",
      "Epoch: 18/500...  Training Step: 3976...  Training loss: 1.7506...  Val loss: 1.7926...  0.3631 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4001...  Training loss: 1.8018...  Val loss: 1.7777...  0.3641 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4026...  Training loss: 1.7547...  Val loss: 1.7848...  0.3634 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4051...  Training loss: 1.7585...  Val loss: 1.7846...  0.3628 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4076...  Training loss: 1.7614...  Val loss: 1.7828...  0.3641 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4101...  Training loss: 1.7776...  Val loss: 1.7737...  0.3633 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4126...  Training loss: 1.7554...  Val loss: 1.7761...  0.3633 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4151...  Training loss: 1.7541...  Val loss: 1.7755...  0.3645 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4176...  Training loss: 1.7712...  Val loss: 1.7689...  0.3639 sec/batch\n",
      "Epoch: 19/500...  Training Step: 4201...  Training loss: 1.7681...  Val loss: 1.7651...  0.3646 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4226...  Training loss: 1.7470...  Val loss: 1.7653...  0.3631 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4251...  Training loss: 1.7559...  Val loss: 1.7652...  0.3637 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4276...  Training loss: 1.7531...  Val loss: 1.7605...  0.3644 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4301...  Training loss: 1.7371...  Val loss: 1.7538...  0.3641 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4326...  Training loss: 1.7620...  Val loss: 1.7530...  0.3638 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4351...  Training loss: 1.7308...  Val loss: 1.7577...  0.3634 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4376...  Training loss: 1.7459...  Val loss: 1.7514...  0.3632 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4401...  Training loss: 1.7396...  Val loss: 1.7460...  0.3662 sec/batch\n",
      "Epoch: 20/500...  Training Step: 4426...  Training loss: 1.7248...  Val loss: 1.7447...  0.3639 sec/batch\n",
      "Epoch 20/500 time:81.74279475212097...  finished at 2017-10-30 10:05:33\n",
      "Epoch: 21/500...  Training Step: 4451...  Training loss: 1.6960...  Val loss: 1.7426...  0.3644 sec/batch\n",
      "Epoch: 21/500...  Training Step: 4476...  Training loss: 1.7042...  Val loss: 1.7316...  0.3635 sec/batch\n",
      "Epoch: 21/500...  Training Step: 4501...  Training loss: 1.7343...  Val loss: 1.7423...  0.3632 sec/batch\n",
      "Epoch: 21/500...  Training Step: 4526...  Training loss: 1.6987...  Val loss: 1.7290...  0.3644 sec/batch\n",
      "Epoch: 21/500...  Training Step: 4551...  Training loss: 1.7298...  Val loss: 1.7364...  0.3636 sec/batch\n",
      "Epoch: 21/500...  Training Step: 4576...  Training loss: 1.7070...  Val loss: 1.7319...  0.3638 sec/batch\n",
      "Epoch: 21/500...  Training Step: 4601...  Training loss: 1.7357...  Val loss: 1.7245...  0.3656 sec/batch\n",
      "Epoch: 21/500...  Training Step: 4626...  Training loss: 1.7345...  Val loss: 1.7278...  0.3631 sec/batch\n",
      "Epoch: 21/500...  Training Step: 4651...  Training loss: 1.7090...  Val loss: 1.7216...  0.3637 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4676...  Training loss: 1.6736...  Val loss: 1.7185...  0.3633 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4701...  Training loss: 1.6988...  Val loss: 1.7165...  0.3632 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4726...  Training loss: 1.7117...  Val loss: 1.7151...  0.3661 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4751...  Training loss: 1.6779...  Val loss: 1.7089...  0.3638 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4776...  Training loss: 1.6927...  Val loss: 1.7095...  0.3640 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4801...  Training loss: 1.6823...  Val loss: 1.7037...  0.3637 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4826...  Training loss: 1.6830...  Val loss: 1.7105...  0.3633 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4851...  Training loss: 1.6786...  Val loss: 1.7043...  0.3664 sec/batch\n",
      "Epoch: 22/500...  Training Step: 4876...  Training loss: 1.6808...  Val loss: 1.7014...  0.3634 sec/batch\n",
      "Epoch: 23/500...  Training Step: 4901...  Training loss: 1.7118...  Val loss: 1.6911...  0.3643 sec/batch\n",
      "Epoch: 23/500...  Training Step: 4926...  Training loss: 1.6741...  Val loss: 1.6939...  0.3632 sec/batch\n",
      "Epoch: 23/500...  Training Step: 4951...  Training loss: 1.6357...  Val loss: 1.6943...  0.3632 sec/batch\n",
      "Epoch: 23/500...  Training Step: 4976...  Training loss: 1.6594...  Val loss: 1.6928...  0.3659 sec/batch\n",
      "Epoch: 23/500...  Training Step: 5001...  Training loss: 1.6866...  Val loss: 1.6946...  0.3639 sec/batch\n",
      "Epoch: 23/500...  Training Step: 5026...  Training loss: 1.6802...  Val loss: 1.6865...  0.3640 sec/batch\n",
      "Epoch: 23/500...  Training Step: 5051...  Training loss: 1.6478...  Val loss: 1.6895...  0.3660 sec/batch\n",
      "Epoch: 23/500...  Training Step: 5076...  Training loss: 1.6804...  Val loss: 1.6864...  0.3631 sec/batch\n",
      "Epoch: 23/500...  Training Step: 5101...  Training loss: 1.6600...  Val loss: 1.6803...  0.3644 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5126...  Training loss: 1.6378...  Val loss: 1.6775...  0.3644 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5151...  Training loss: 1.6296...  Val loss: 1.6796...  0.3628 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5176...  Training loss: 1.6573...  Val loss: 1.6748...  0.3659 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5201...  Training loss: 1.6558...  Val loss: 1.6728...  0.3638 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5226...  Training loss: 1.6669...  Val loss: 1.6723...  0.3645 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5251...  Training loss: 1.6629...  Val loss: 1.6729...  0.3637 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5276...  Training loss: 1.6610...  Val loss: 1.6717...  0.3636 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5301...  Training loss: 1.6419...  Val loss: 1.6635...  0.3649 sec/batch\n",
      "Epoch: 24/500...  Training Step: 5326...  Training loss: 1.6452...  Val loss: 1.6624...  0.3644 sec/batch\n",
      "Epoch: 25/500...  Training Step: 5351...  Training loss: 1.6454...  Val loss: 1.6644...  0.3639 sec/batch\n",
      "Epoch: 25/500...  Training Step: 5376...  Training loss: 1.6112...  Val loss: 1.6597...  0.3630 sec/batch\n",
      "Epoch: 25/500...  Training Step: 5401...  Training loss: 1.6259...  Val loss: 1.6591...  0.3632 sec/batch\n",
      "Epoch: 25/500...  Training Step: 5426...  Training loss: 1.6118...  Val loss: 1.6620...  0.3656 sec/batch\n",
      "Epoch: 25/500...  Training Step: 5451...  Training loss: 1.6453...  Val loss: 1.6592...  0.3633 sec/batch\n",
      "Epoch: 25/500...  Training Step: 5476...  Training loss: 1.6409...  Val loss: 1.6520...  0.3639 sec/batch\n",
      "Epoch: 25/500...  Training Step: 5501...  Training loss: 1.6620...  Val loss: 1.6503...  0.3628 sec/batch\n",
      "Epoch: 25/500...  Training Step: 5526...  Training loss: 1.6438...  Val loss: 1.6532...  0.3632 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5551...  Training loss: 1.7523...  Val loss: 1.6531...  0.3642 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5576...  Training loss: 1.6067...  Val loss: 1.6439...  0.3634 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5601...  Training loss: 1.6135...  Val loss: 1.6470...  0.3634 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5626...  Training loss: 1.5967...  Val loss: 1.6437...  0.3663 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5651...  Training loss: 1.6468...  Val loss: 1.6417...  0.3650 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5676...  Training loss: 1.6208...  Val loss: 1.6396...  0.3661 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5701...  Training loss: 1.6153...  Val loss: 1.6437...  0.3636 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5726...  Training loss: 1.5842...  Val loss: 1.6427...  0.3634 sec/batch\n",
      "Epoch: 26/500...  Training Step: 5751...  Training loss: 1.6177...  Val loss: 1.6415...  0.3653 sec/batch\n",
      "Epoch: 27/500...  Training Step: 5776...  Training loss: 1.6282...  Val loss: 1.6295...  0.3632 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/500...  Training Step: 5801...  Training loss: 1.5829...  Val loss: 1.6364...  0.3642 sec/batch\n",
      "Epoch: 27/500...  Training Step: 5826...  Training loss: 1.5678...  Val loss: 1.6283...  0.3632 sec/batch\n",
      "Epoch: 27/500...  Training Step: 5851...  Training loss: 1.5557...  Val loss: 1.6305...  0.3633 sec/batch\n",
      "Epoch: 27/500...  Training Step: 5876...  Training loss: 1.5859...  Val loss: 1.6248...  0.3668 sec/batch\n",
      "Epoch: 27/500...  Training Step: 5901...  Training loss: 1.5926...  Val loss: 1.6249...  0.3636 sec/batch\n",
      "Epoch: 27/500...  Training Step: 5926...  Training loss: 1.5956...  Val loss: 1.6276...  0.3633 sec/batch\n",
      "Epoch: 27/500...  Training Step: 5951...  Training loss: 1.5751...  Val loss: 1.6287...  0.3637 sec/batch\n",
      "Epoch: 27/500...  Training Step: 5976...  Training loss: 1.5687...  Val loss: 1.6186...  0.3637 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6001...  Training loss: 1.5852...  Val loss: 1.6135...  0.3661 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6026...  Training loss: 1.5904...  Val loss: 1.6172...  0.3642 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6051...  Training loss: 1.5669...  Val loss: 1.6215...  0.3634 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6076...  Training loss: 1.5983...  Val loss: 1.6172...  0.3651 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6101...  Training loss: 1.5385...  Val loss: 1.6112...  0.3640 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6126...  Training loss: 1.5907...  Val loss: 1.6147...  0.3636 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6151...  Training loss: 1.5784...  Val loss: 1.6143...  0.3629 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6176...  Training loss: 1.5683...  Val loss: 1.6110...  0.3628 sec/batch\n",
      "Epoch: 28/500...  Training Step: 6201...  Training loss: 1.5697...  Val loss: 1.6103...  0.3648 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6226...  Training loss: 1.5663...  Val loss: 1.6021...  0.3634 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6251...  Training loss: 1.5921...  Val loss: 1.6055...  0.3640 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6276...  Training loss: 1.5671...  Val loss: 1.6023...  0.3634 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6301...  Training loss: 1.5893...  Val loss: 1.6035...  0.3639 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6326...  Training loss: 1.5674...  Val loss: 1.5995...  0.3667 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6351...  Training loss: 1.5562...  Val loss: 1.5971...  0.3642 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6376...  Training loss: 1.5911...  Val loss: 1.5994...  0.3646 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6401...  Training loss: 1.5485...  Val loss: 1.6003...  0.3637 sec/batch\n",
      "Epoch: 29/500...  Training Step: 6426...  Training loss: 1.5516...  Val loss: 1.5991...  0.3629 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6451...  Training loss: 1.5712...  Val loss: 1.5912...  0.3657 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6476...  Training loss: 1.5577...  Val loss: 1.5949...  0.3626 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6501...  Training loss: 1.5707...  Val loss: 1.5919...  0.3640 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6526...  Training loss: 1.5340...  Val loss: 1.5909...  0.3660 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6551...  Training loss: 1.5534...  Val loss: 1.5916...  0.3632 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6576...  Training loss: 1.5522...  Val loss: 1.5900...  0.3638 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6601...  Training loss: 1.5382...  Val loss: 1.5901...  0.3633 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6626...  Training loss: 1.5826...  Val loss: 1.5890...  0.3642 sec/batch\n",
      "Epoch: 30/500...  Training Step: 6651...  Training loss: 1.5943...  Val loss: 1.5960...  0.3665 sec/batch\n",
      "Epoch 30/500 time:82.10594129562378...  finished at 2017-10-30 10:19:12\n",
      "Epoch: 31/500...  Training Step: 6676...  Training loss: 1.5191...  Val loss: 1.5861...  0.3627 sec/batch\n",
      "Epoch: 31/500...  Training Step: 6701...  Training loss: 1.5312...  Val loss: 1.5759...  0.3639 sec/batch\n",
      "Epoch: 31/500...  Training Step: 6726...  Training loss: 1.5231...  Val loss: 1.5856...  0.3628 sec/batch\n",
      "Epoch: 31/500...  Training Step: 6751...  Training loss: 1.5394...  Val loss: 1.5864...  0.3635 sec/batch\n",
      "Epoch: 31/500...  Training Step: 6776...  Training loss: 1.5481...  Val loss: 1.5822...  0.3659 sec/batch\n",
      "Epoch: 31/500...  Training Step: 6801...  Training loss: 1.5214...  Val loss: 1.5772...  0.3637 sec/batch\n",
      "Epoch: 31/500...  Training Step: 6826...  Training loss: 1.5541...  Val loss: 1.5747...  0.3635 sec/batch\n",
      "Epoch: 31/500...  Training Step: 6851...  Training loss: 1.5161...  Val loss: 1.5824...  0.3628 sec/batch\n",
      "Epoch: 31/500...  Training Step: 6876...  Training loss: 1.5543...  Val loss: 1.5741...  0.3634 sec/batch\n",
      "Epoch: 32/500...  Training Step: 6901...  Training loss: 1.5284...  Val loss: 1.5713...  0.3654 sec/batch\n",
      "Epoch: 32/500...  Training Step: 6926...  Training loss: 1.5250...  Val loss: 1.5744...  0.3633 sec/batch\n",
      "Epoch: 32/500...  Training Step: 6951...  Training loss: 1.5228...  Val loss: 1.5741...  0.3639 sec/batch\n",
      "Epoch: 32/500...  Training Step: 6976...  Training loss: 1.5165...  Val loss: 1.5746...  0.3633 sec/batch\n",
      "Epoch: 32/500...  Training Step: 7001...  Training loss: 1.5288...  Val loss: 1.5737...  0.3632 sec/batch\n",
      "Epoch: 32/500...  Training Step: 7026...  Training loss: 1.5287...  Val loss: 1.5689...  0.3641 sec/batch\n",
      "Epoch: 32/500...  Training Step: 7051...  Training loss: 1.5658...  Val loss: 1.5685...  0.3635 sec/batch\n",
      "Epoch: 32/500...  Training Step: 7076...  Training loss: 1.5236...  Val loss: 1.5707...  0.3634 sec/batch\n",
      "Epoch: 32/500...  Training Step: 7101...  Training loss: 1.5350...  Val loss: 1.5612...  0.3668 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7126...  Training loss: 1.5148...  Val loss: 1.5615...  0.3637 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7151...  Training loss: 1.5107...  Val loss: 1.5597...  0.3642 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7176...  Training loss: 1.5051...  Val loss: 1.5625...  0.3631 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7201...  Training loss: 1.4747...  Val loss: 1.5633...  0.3631 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7226...  Training loss: 1.5391...  Val loss: 1.5632...  0.3664 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7251...  Training loss: 1.5609...  Val loss: 1.5576...  0.3633 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7276...  Training loss: 1.5346...  Val loss: 1.5580...  0.3643 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7301...  Training loss: 1.5280...  Val loss: 1.5563...  0.3632 sec/batch\n",
      "Epoch: 33/500...  Training Step: 7326...  Training loss: 1.5141...  Val loss: 1.5587...  0.3633 sec/batch\n",
      "Epoch: 34/500...  Training Step: 7351...  Training loss: 1.4719...  Val loss: 1.5558...  0.3664 sec/batch\n",
      "Epoch: 34/500...  Training Step: 7376...  Training loss: 1.5272...  Val loss: 1.5567...  0.3639 sec/batch\n",
      "Epoch: 34/500...  Training Step: 7401...  Training loss: 1.5087...  Val loss: 1.5537...  0.3647 sec/batch\n",
      "Epoch: 34/500...  Training Step: 7426...  Training loss: 1.4950...  Val loss: 1.5519...  0.3635 sec/batch\n",
      "Epoch: 34/500...  Training Step: 7451...  Training loss: 1.5080...  Val loss: 1.5554...  0.3634 sec/batch\n",
      "Epoch: 34/500...  Training Step: 7476...  Training loss: 1.5416...  Val loss: 1.5463...  0.3666 sec/batch\n",
      "Epoch: 34/500...  Training Step: 7501...  Training loss: 1.5146...  Val loss: 1.5494...  0.3637 sec/batch\n",
      "Epoch: 34/500...  Training Step: 7526...  Training loss: 1.4983...  Val loss: 1.5512...  0.3634 sec/batch\n",
      "Epoch: 35/500...  Training Step: 7551...  Training loss: 1.5119...  Val loss: 1.5428...  0.3655 sec/batch\n",
      "Epoch: 35/500...  Training Step: 7576...  Training loss: 1.4767...  Val loss: 1.5441...  0.3636 sec/batch\n",
      "Epoch: 35/500...  Training Step: 7601...  Training loss: 1.4607...  Val loss: 1.5425...  0.3640 sec/batch\n",
      "Epoch: 35/500...  Training Step: 7626...  Training loss: 1.4673...  Val loss: 1.5483...  0.3633 sec/batch\n",
      "Epoch: 35/500...  Training Step: 7651...  Training loss: 1.4718...  Val loss: 1.5440...  0.3632 sec/batch\n",
      "Epoch: 35/500...  Training Step: 7676...  Training loss: 1.4968...  Val loss: 1.5465...  0.3666 sec/batch\n",
      "Epoch: 35/500...  Training Step: 7701...  Training loss: 1.5086...  Val loss: 1.5393...  0.3629 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/500...  Training Step: 7726...  Training loss: 1.4864...  Val loss: 1.5471...  0.3632 sec/batch\n",
      "Epoch: 35/500...  Training Step: 7751...  Training loss: 1.5095...  Val loss: 1.5443...  0.3632 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7776...  Training loss: 1.5174...  Val loss: 1.5355...  0.3630 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7801...  Training loss: 1.4621...  Val loss: 1.5382...  0.3653 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7826...  Training loss: 1.5038...  Val loss: 1.5400...  0.3670 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7851...  Training loss: 1.4587...  Val loss: 1.5343...  0.3635 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7876...  Training loss: 1.4792...  Val loss: 1.5388...  0.3637 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7901...  Training loss: 1.4687...  Val loss: 1.5370...  0.3629 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7926...  Training loss: 1.5016...  Val loss: 1.5412...  0.3667 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7951...  Training loss: 1.4636...  Val loss: 1.5448...  0.3634 sec/batch\n",
      "Epoch: 36/500...  Training Step: 7976...  Training loss: 1.4607...  Val loss: 1.5349...  0.3631 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8001...  Training loss: 1.4780...  Val loss: 1.5355...  0.3633 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8026...  Training loss: 1.4587...  Val loss: 1.5302...  0.3633 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8051...  Training loss: 1.4784...  Val loss: 1.5435...  0.3637 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8076...  Training loss: 1.4934...  Val loss: 1.5319...  0.3635 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8101...  Training loss: 1.4718...  Val loss: 1.5313...  0.3635 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8126...  Training loss: 1.4688...  Val loss: 1.5276...  0.3673 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8151...  Training loss: 1.4731...  Val loss: 1.5314...  0.3633 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8176...  Training loss: 1.4559...  Val loss: 1.5270...  0.3635 sec/batch\n",
      "Epoch: 37/500...  Training Step: 8201...  Training loss: 1.4415...  Val loss: 1.5256...  0.3638 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8226...  Training loss: 1.4541...  Val loss: 1.5226...  0.3640 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8251...  Training loss: 1.4676...  Val loss: 1.5189...  0.3668 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8276...  Training loss: 1.4584...  Val loss: 1.5262...  0.3645 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8301...  Training loss: 1.4615...  Val loss: 1.5267...  0.3637 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8326...  Training loss: 1.4600...  Val loss: 1.5207...  0.3638 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8351...  Training loss: 1.4672...  Val loss: 1.5247...  0.3633 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8376...  Training loss: 1.4697...  Val loss: 1.5191...  0.3659 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8401...  Training loss: 1.4752...  Val loss: 1.5184...  0.3630 sec/batch\n",
      "Epoch: 38/500...  Training Step: 8426...  Training loss: 1.4446...  Val loss: 1.5184...  0.3630 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8451...  Training loss: 1.4680...  Val loss: 1.5123...  0.3634 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8476...  Training loss: 1.4358...  Val loss: 1.5153...  0.3630 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8501...  Training loss: 1.4804...  Val loss: 1.5174...  0.3663 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8526...  Training loss: 1.4352...  Val loss: 1.5229...  0.3638 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8551...  Training loss: 1.4766...  Val loss: 1.5206...  0.3637 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8576...  Training loss: 1.4480...  Val loss: 1.5125...  0.3649 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8601...  Training loss: 1.4702...  Val loss: 1.5164...  0.3634 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8626...  Training loss: 1.4594...  Val loss: 1.5182...  0.3636 sec/batch\n",
      "Epoch: 39/500...  Training Step: 8651...  Training loss: 1.4635...  Val loss: 1.5141...  0.3635 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8676...  Training loss: 1.4577...  Val loss: 1.5049...  0.3637 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8701...  Training loss: 1.4229...  Val loss: 1.5195...  0.3672 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8726...  Training loss: 1.4406...  Val loss: 1.5148...  0.3624 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8751...  Training loss: 1.4237...  Val loss: 1.5148...  0.3631 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8776...  Training loss: 1.4495...  Val loss: 1.5147...  0.3635 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8801...  Training loss: 1.4381...  Val loss: 1.5115...  0.3624 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8826...  Training loss: 1.4397...  Val loss: 1.5111...  0.3660 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8851...  Training loss: 1.4352...  Val loss: 1.5198...  0.3635 sec/batch\n",
      "Epoch: 40/500...  Training Step: 8876...  Training loss: 1.4571...  Val loss: 1.5107...  0.3638 sec/batch\n",
      "Epoch 40/500 time:81.72426867485046...  finished at 2017-10-30 10:32:51\n",
      "Epoch: 41/500...  Training Step: 8901...  Training loss: 1.4387...  Val loss: 1.5074...  0.3636 sec/batch\n",
      "Epoch: 41/500...  Training Step: 8926...  Training loss: 1.4211...  Val loss: 1.5122...  0.3668 sec/batch\n",
      "Epoch: 41/500...  Training Step: 8951...  Training loss: 1.4489...  Val loss: 1.5101...  0.3626 sec/batch\n",
      "Epoch: 41/500...  Training Step: 8976...  Training loss: 1.3997...  Val loss: 1.5075...  0.3632 sec/batch\n",
      "Epoch: 41/500...  Training Step: 9001...  Training loss: 1.4860...  Val loss: 1.5052...  0.3641 sec/batch\n",
      "Epoch: 41/500...  Training Step: 9026...  Training loss: 1.4574...  Val loss: 1.5167...  0.3630 sec/batch\n",
      "Epoch: 41/500...  Training Step: 9051...  Training loss: 1.4581...  Val loss: 1.5069...  0.3630 sec/batch\n",
      "Epoch: 41/500...  Training Step: 9076...  Training loss: 1.4363...  Val loss: 1.5003...  0.3634 sec/batch\n",
      "Epoch: 41/500...  Training Step: 9101...  Training loss: 1.4469...  Val loss: 1.5072...  0.3632 sec/batch\n",
      "Epoch: 42/500...  Training Step: 9126...  Training loss: 1.4211...  Val loss: 1.5081...  0.3632 sec/batch\n",
      "Epoch: 42/500...  Training Step: 9151...  Training loss: 1.4167...  Val loss: 1.5076...  0.3630 sec/batch\n",
      "Epoch: 42/500...  Training Step: 9176...  Training loss: 1.3882...  Val loss: 1.5055...  0.3631 sec/batch\n",
      "Epoch: 42/500...  Training Step: 9201...  Training loss: 1.4153...  Val loss: 1.5066...  0.3636 sec/batch\n",
      "Epoch: 42/500...  Training Step: 9226...  Training loss: 1.4400...  Val loss: 1.5034...  0.3632 sec/batch\n",
      "Epoch: 42/500...  Training Step: 9251...  Training loss: 1.4555...  Val loss: 1.5086...  0.3639 sec/batch\n",
      "Epoch: 42/500...  Training Step: 9276...  Training loss: 1.4467...  Val loss: 1.4975...  0.3636 sec/batch\n",
      "Epoch: 42/500...  Training Step: 9301...  Training loss: 1.4314...  Val loss: 1.5002...  0.3633 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9326...  Training loss: 1.4141...  Val loss: 1.4944...  0.3634 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9351...  Training loss: 1.4140...  Val loss: 1.4944...  0.3630 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9376...  Training loss: 1.4239...  Val loss: 1.4999...  0.3635 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9401...  Training loss: 1.3867...  Val loss: 1.4985...  0.3634 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9426...  Training loss: 1.4295...  Val loss: 1.4974...  0.3638 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9451...  Training loss: 1.4117...  Val loss: 1.4993...  0.3636 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9476...  Training loss: 1.4273...  Val loss: 1.4995...  0.3630 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9501...  Training loss: 1.4215...  Val loss: 1.4949...  0.3625 sec/batch\n",
      "Epoch: 43/500...  Training Step: 9526...  Training loss: 1.3718...  Val loss: 1.4942...  0.3638 sec/batch\n",
      "Epoch: 44/500...  Training Step: 9551...  Training loss: 1.4441...  Val loss: 1.4899...  0.3636 sec/batch\n",
      "Epoch: 44/500...  Training Step: 9576...  Training loss: 1.3955...  Val loss: 1.4947...  0.3636 sec/batch\n",
      "Epoch: 44/500...  Training Step: 9601...  Training loss: 1.3997...  Val loss: 1.4918...  0.3637 sec/batch\n",
      "Epoch: 44/500...  Training Step: 9626...  Training loss: 1.4155...  Val loss: 1.4938...  0.3630 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/500...  Training Step: 9651...  Training loss: 1.4157...  Val loss: 1.4870...  0.3634 sec/batch\n",
      "Epoch: 44/500...  Training Step: 9676...  Training loss: 1.4066...  Val loss: 1.4950...  0.3637 sec/batch\n",
      "Epoch: 44/500...  Training Step: 9701...  Training loss: 1.4101...  Val loss: 1.4991...  0.3639 sec/batch\n",
      "Epoch: 44/500...  Training Step: 9726...  Training loss: 1.4207...  Val loss: 1.4958...  0.3636 sec/batch\n",
      "Epoch: 44/500...  Training Step: 9751...  Training loss: 1.4081...  Val loss: 1.4918...  0.3632 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9776...  Training loss: 1.4023...  Val loss: 1.4797...  0.3638 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9801...  Training loss: 1.4071...  Val loss: 1.4907...  0.3635 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9826...  Training loss: 1.4129...  Val loss: 1.4957...  0.3633 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9851...  Training loss: 1.3859...  Val loss: 1.4881...  0.3640 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9876...  Training loss: 1.3999...  Val loss: 1.4898...  0.3662 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9901...  Training loss: 1.3958...  Val loss: 1.4918...  0.3639 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9926...  Training loss: 1.4182...  Val loss: 1.4824...  0.3633 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9951...  Training loss: 1.4069...  Val loss: 1.4819...  0.3628 sec/batch\n",
      "Epoch: 45/500...  Training Step: 9976...  Training loss: 1.3830...  Val loss: 1.4854...  0.3633 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10001...  Training loss: 1.3697...  Val loss: 1.4887...  0.3638 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10026...  Training loss: 1.3803...  Val loss: 1.4812...  0.3636 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10051...  Training loss: 1.4131...  Val loss: 1.4907...  0.3634 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10076...  Training loss: 1.3995...  Val loss: 1.4777...  0.3639 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10101...  Training loss: 1.4259...  Val loss: 1.4894...  0.3632 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10126...  Training loss: 1.3797...  Val loss: 1.4866...  0.3633 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10151...  Training loss: 1.4240...  Val loss: 1.4805...  0.3633 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10176...  Training loss: 1.4193...  Val loss: 1.4817...  0.3638 sec/batch\n",
      "Epoch: 46/500...  Training Step: 10201...  Training loss: 1.3997...  Val loss: 1.4834...  0.3632 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10226...  Training loss: 1.3689...  Val loss: 1.4853...  0.3636 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10251...  Training loss: 1.3821...  Val loss: 1.4833...  0.3628 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10276...  Training loss: 1.4012...  Val loss: 1.4825...  0.3630 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10301...  Training loss: 1.3750...  Val loss: 1.4776...  0.3633 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10326...  Training loss: 1.3951...  Val loss: 1.4787...  0.3639 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10351...  Training loss: 1.3805...  Val loss: 1.4824...  0.3634 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10376...  Training loss: 1.3754...  Val loss: 1.4824...  0.3634 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10401...  Training loss: 1.3807...  Val loss: 1.4818...  0.3630 sec/batch\n",
      "Epoch: 47/500...  Training Step: 10426...  Training loss: 1.3961...  Val loss: 1.4831...  0.3637 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10451...  Training loss: 1.3976...  Val loss: 1.4697...  0.3642 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10476...  Training loss: 1.3856...  Val loss: 1.4767...  0.3641 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10501...  Training loss: 1.3579...  Val loss: 1.4793...  0.3637 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10526...  Training loss: 1.3658...  Val loss: 1.4766...  0.3638 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10551...  Training loss: 1.3874...  Val loss: 1.4834...  0.3633 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10576...  Training loss: 1.3885...  Val loss: 1.4761...  0.3635 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10601...  Training loss: 1.3753...  Val loss: 1.4803...  0.3635 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10626...  Training loss: 1.3961...  Val loss: 1.4842...  0.3639 sec/batch\n",
      "Epoch: 48/500...  Training Step: 10651...  Training loss: 1.3819...  Val loss: 1.4728...  0.3635 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10676...  Training loss: 1.3630...  Val loss: 1.4697...  0.3626 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10701...  Training loss: 1.3488...  Val loss: 1.4769...  0.3630 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10726...  Training loss: 1.3675...  Val loss: 1.4692...  0.3645 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10751...  Training loss: 1.3833...  Val loss: 1.4762...  0.3632 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10776...  Training loss: 1.3947...  Val loss: 1.4775...  0.3641 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10801...  Training loss: 1.3909...  Val loss: 1.4767...  0.3637 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10826...  Training loss: 1.3941...  Val loss: 1.4651...  0.3632 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10851...  Training loss: 1.3622...  Val loss: 1.4711...  0.3630 sec/batch\n",
      "Epoch: 49/500...  Training Step: 10876...  Training loss: 1.3779...  Val loss: 1.4649...  0.3643 sec/batch\n",
      "Epoch: 50/500...  Training Step: 10901...  Training loss: 1.3880...  Val loss: 1.4712...  0.3632 sec/batch\n",
      "Epoch: 50/500...  Training Step: 10926...  Training loss: 1.3434...  Val loss: 1.4738...  0.3630 sec/batch\n",
      "Epoch: 50/500...  Training Step: 10951...  Training loss: 1.3652...  Val loss: 1.4714...  0.3636 sec/batch\n",
      "Epoch: 50/500...  Training Step: 10976...  Training loss: 1.3543...  Val loss: 1.4776...  0.3630 sec/batch\n",
      "Epoch: 50/500...  Training Step: 11001...  Training loss: 1.3822...  Val loss: 1.4766...  0.3637 sec/batch\n",
      "Epoch: 50/500...  Training Step: 11026...  Training loss: 1.3887...  Val loss: 1.4752...  0.3632 sec/batch\n",
      "Epoch: 50/500...  Training Step: 11051...  Training loss: 1.3978...  Val loss: 1.4715...  0.3638 sec/batch\n",
      "Epoch: 50/500...  Training Step: 11076...  Training loss: 1.3831...  Val loss: 1.4700...  0.3630 sec/batch\n",
      "Epoch 50/500 time:81.91180300712585...  finished at 2017-10-30 10:46:28\n",
      "Epoch: 51/500...  Training Step: 11101...  Training loss: 1.4883...  Val loss: 1.4680...  0.3640 sec/batch\n",
      "Epoch: 51/500...  Training Step: 11126...  Training loss: 1.3735...  Val loss: 1.4722...  0.3634 sec/batch\n",
      "Epoch: 51/500...  Training Step: 11151...  Training loss: 1.3654...  Val loss: 1.4745...  0.3639 sec/batch\n",
      "Epoch: 51/500...  Training Step: 11176...  Training loss: 1.3429...  Val loss: 1.4677...  0.3627 sec/batch\n",
      "Epoch: 51/500...  Training Step: 11201...  Training loss: 1.3846...  Val loss: 1.4737...  0.3627 sec/batch\n",
      "Epoch: 51/500...  Training Step: 11226...  Training loss: 1.3669...  Val loss: 1.4694...  0.3633 sec/batch\n",
      "Epoch: 51/500...  Training Step: 11251...  Training loss: 1.3707...  Val loss: 1.4768...  0.3634 sec/batch\n",
      "Epoch: 51/500...  Training Step: 11276...  Training loss: 1.3357...  Val loss: 1.4744...  0.3630 sec/batch\n",
      "Epoch: 51/500...  Training Step: 11301...  Training loss: 1.3868...  Val loss: 1.4674...  0.3634 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11326...  Training loss: 1.3874...  Val loss: 1.4692...  0.3633 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11351...  Training loss: 1.3445...  Val loss: 1.4724...  0.3633 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11376...  Training loss: 1.3335...  Val loss: 1.4622...  0.3635 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11401...  Training loss: 1.3211...  Val loss: 1.4651...  0.3636 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11426...  Training loss: 1.3446...  Val loss: 1.4642...  0.3635 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11451...  Training loss: 1.3652...  Val loss: 1.4663...  0.3634 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11476...  Training loss: 1.3646...  Val loss: 1.4733...  0.3632 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11501...  Training loss: 1.3389...  Val loss: 1.4738...  0.3638 sec/batch\n",
      "Epoch: 52/500...  Training Step: 11526...  Training loss: 1.3411...  Val loss: 1.4643...  0.3631 sec/batch\n",
      "Epoch: 53/500...  Training Step: 11551...  Training loss: 1.3627...  Val loss: 1.4600...  0.3629 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/500...  Training Step: 11576...  Training loss: 1.3654...  Val loss: 1.4640...  0.3631 sec/batch\n",
      "Epoch: 53/500...  Training Step: 11601...  Training loss: 1.3300...  Val loss: 1.4670...  0.3638 sec/batch\n",
      "Epoch: 53/500...  Training Step: 11626...  Training loss: 1.3505...  Val loss: 1.4666...  0.3644 sec/batch\n",
      "Epoch: 53/500...  Training Step: 11651...  Training loss: 1.3260...  Val loss: 1.4668...  0.3634 sec/batch\n",
      "Epoch: 53/500...  Training Step: 11676...  Training loss: 1.3590...  Val loss: 1.4671...  0.3636 sec/batch\n",
      "Epoch: 53/500...  Training Step: 11701...  Training loss: 1.3704...  Val loss: 1.4707...  0.3630 sec/batch\n",
      "Epoch: 53/500...  Training Step: 11726...  Training loss: 1.3452...  Val loss: 1.4651...  0.3637 sec/batch\n",
      "Epoch: 53/500...  Training Step: 11751...  Training loss: 1.3531...  Val loss: 1.4711...  0.3638 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11776...  Training loss: 1.3387...  Val loss: 1.4607...  0.3638 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11801...  Training loss: 1.3606...  Val loss: 1.4645...  0.3637 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11826...  Training loss: 1.3461...  Val loss: 1.4683...  0.3630 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11851...  Training loss: 1.3759...  Val loss: 1.4672...  0.3635 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11876...  Training loss: 1.3512...  Val loss: 1.4661...  0.3637 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11901...  Training loss: 1.3480...  Val loss: 1.4631...  0.3630 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11926...  Training loss: 1.3668...  Val loss: 1.4596...  0.3634 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11951...  Training loss: 1.3335...  Val loss: 1.4650...  0.3636 sec/batch\n",
      "Epoch: 54/500...  Training Step: 11976...  Training loss: 1.3237...  Val loss: 1.4608...  0.3633 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12001...  Training loss: 1.3664...  Val loss: 1.4594...  0.3625 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12026...  Training loss: 1.3453...  Val loss: 1.4679...  0.3634 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12051...  Training loss: 1.3474...  Val loss: 1.4587...  0.3631 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12076...  Training loss: 1.3346...  Val loss: 1.4648...  0.3635 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12101...  Training loss: 1.3460...  Val loss: 1.4664...  0.3636 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12126...  Training loss: 1.3411...  Val loss: 1.4657...  0.3630 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12151...  Training loss: 1.3362...  Val loss: 1.4656...  0.3637 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12176...  Training loss: 1.3592...  Val loss: 1.4597...  0.3637 sec/batch\n",
      "Epoch: 55/500...  Training Step: 12201...  Training loss: 1.3805...  Val loss: 1.4618...  0.3633 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12226...  Training loss: 1.3161...  Val loss: 1.4622...  0.3636 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12251...  Training loss: 1.3307...  Val loss: 1.4540...  0.3634 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12276...  Training loss: 1.3268...  Val loss: 1.4630...  0.3660 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12301...  Training loss: 1.3283...  Val loss: 1.4585...  0.3631 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12326...  Training loss: 1.3426...  Val loss: 1.4569...  0.3647 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12351...  Training loss: 1.3207...  Val loss: 1.4645...  0.3637 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12376...  Training loss: 1.3362...  Val loss: 1.4551...  0.3642 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12401...  Training loss: 1.3182...  Val loss: 1.4620...  0.3630 sec/batch\n",
      "Epoch: 56/500...  Training Step: 12426...  Training loss: 1.3493...  Val loss: 1.4589...  0.3631 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12451...  Training loss: 1.3351...  Val loss: 1.4546...  0.3629 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12476...  Training loss: 1.3340...  Val loss: 1.4560...  0.3634 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12501...  Training loss: 1.3332...  Val loss: 1.4583...  0.3631 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12526...  Training loss: 1.3189...  Val loss: 1.4597...  0.3634 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12551...  Training loss: 1.3394...  Val loss: 1.4600...  0.3642 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12576...  Training loss: 1.3413...  Val loss: 1.4690...  0.3630 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12601...  Training loss: 1.3559...  Val loss: 1.4558...  0.3634 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12626...  Training loss: 1.3405...  Val loss: 1.4642...  0.3631 sec/batch\n",
      "Epoch: 57/500...  Training Step: 12651...  Training loss: 1.3495...  Val loss: 1.4553...  0.3638 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12676...  Training loss: 1.3186...  Val loss: 1.4486...  0.3638 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12701...  Training loss: 1.3206...  Val loss: 1.4513...  0.3629 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12726...  Training loss: 1.3115...  Val loss: 1.4564...  0.3636 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12751...  Training loss: 1.2846...  Val loss: 1.4573...  0.3633 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12776...  Training loss: 1.3501...  Val loss: 1.4625...  0.3634 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12801...  Training loss: 1.3625...  Val loss: 1.4496...  0.3640 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12826...  Training loss: 1.3500...  Val loss: 1.4501...  0.3635 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12851...  Training loss: 1.3354...  Val loss: 1.4584...  0.3638 sec/batch\n",
      "Epoch: 58/500...  Training Step: 12876...  Training loss: 1.3336...  Val loss: 1.4464...  0.3640 sec/batch\n",
      "Epoch: 59/500...  Training Step: 12901...  Training loss: 1.2959...  Val loss: 1.4500...  0.3634 sec/batch\n",
      "Epoch: 59/500...  Training Step: 12926...  Training loss: 1.3534...  Val loss: 1.4612...  0.3634 sec/batch\n",
      "Epoch: 59/500...  Training Step: 12951...  Training loss: 1.3450...  Val loss: 1.4580...  0.3632 sec/batch\n",
      "Epoch: 59/500...  Training Step: 12976...  Training loss: 1.3169...  Val loss: 1.4571...  0.3636 sec/batch\n",
      "Epoch: 59/500...  Training Step: 13001...  Training loss: 1.3340...  Val loss: 1.4654...  0.3636 sec/batch\n",
      "Epoch: 59/500...  Training Step: 13026...  Training loss: 1.3526...  Val loss: 1.4531...  0.3640 sec/batch\n",
      "Epoch: 59/500...  Training Step: 13051...  Training loss: 1.3295...  Val loss: 1.4537...  0.3630 sec/batch\n",
      "Epoch: 59/500...  Training Step: 13076...  Training loss: 1.3145...  Val loss: 1.4575...  0.3635 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13101...  Training loss: 1.3418...  Val loss: 1.4411...  0.3634 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13126...  Training loss: 1.3053...  Val loss: 1.4505...  0.3637 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13151...  Training loss: 1.2841...  Val loss: 1.4534...  0.3632 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13176...  Training loss: 1.2969...  Val loss: 1.4605...  0.3636 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13201...  Training loss: 1.2986...  Val loss: 1.4572...  0.3629 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13226...  Training loss: 1.3294...  Val loss: 1.4658...  0.3631 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13251...  Training loss: 1.3186...  Val loss: 1.4600...  0.3632 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13276...  Training loss: 1.3193...  Val loss: 1.4628...  0.3632 sec/batch\n",
      "Epoch: 60/500...  Training Step: 13301...  Training loss: 1.3285...  Val loss: 1.4536...  0.3637 sec/batch\n",
      "Epoch 60/500 time:81.62973523139954...  finished at 2017-10-30 11:00:06\n",
      "Epoch: 61/500...  Training Step: 13326...  Training loss: 1.3425...  Val loss: 1.4539...  0.3641 sec/batch\n",
      "Epoch: 61/500...  Training Step: 13351...  Training loss: 1.2882...  Val loss: 1.4540...  0.3627 sec/batch\n",
      "Epoch: 61/500...  Training Step: 13376...  Training loss: 1.3282...  Val loss: 1.4506...  0.3630 sec/batch\n",
      "Epoch: 61/500...  Training Step: 13401...  Training loss: 1.2901...  Val loss: 1.4468...  0.3625 sec/batch\n",
      "Epoch: 61/500...  Training Step: 13426...  Training loss: 1.3086...  Val loss: 1.4495...  0.3624 sec/batch\n",
      "Epoch: 61/500...  Training Step: 13451...  Training loss: 1.3088...  Val loss: 1.4517...  0.3632 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/500...  Training Step: 13476...  Training loss: 1.3250...  Val loss: 1.4606...  0.3637 sec/batch\n",
      "Epoch: 61/500...  Training Step: 13501...  Training loss: 1.3001...  Val loss: 1.4606...  0.3654 sec/batch\n",
      "Epoch: 61/500...  Training Step: 13526...  Training loss: 1.2944...  Val loss: 1.4611...  0.3640 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13551...  Training loss: 1.3170...  Val loss: 1.4500...  0.3635 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13576...  Training loss: 1.2951...  Val loss: 1.4512...  0.3660 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13601...  Training loss: 1.3168...  Val loss: 1.4637...  0.3635 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13626...  Training loss: 1.3239...  Val loss: 1.4534...  0.3633 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13651...  Training loss: 1.3072...  Val loss: 1.4476...  0.3632 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13676...  Training loss: 1.3151...  Val loss: 1.4466...  0.3634 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13701...  Training loss: 1.3142...  Val loss: 1.4559...  0.3671 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13726...  Training loss: 1.3116...  Val loss: 1.4527...  0.3634 sec/batch\n",
      "Epoch: 62/500...  Training Step: 13751...  Training loss: 1.2889...  Val loss: 1.4478...  0.3626 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13776...  Training loss: 1.3039...  Val loss: 1.4475...  0.3631 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13801...  Training loss: 1.3071...  Val loss: 1.4434...  0.3633 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13826...  Training loss: 1.3190...  Val loss: 1.4587...  0.3745 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13851...  Training loss: 1.3097...  Val loss: 1.4533...  0.3662 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13876...  Training loss: 1.2946...  Val loss: 1.4494...  0.3634 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13901...  Training loss: 1.3062...  Val loss: 1.4602...  0.3644 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13926...  Training loss: 1.3224...  Val loss: 1.4456...  0.3634 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13951...  Training loss: 1.3178...  Val loss: 1.4487...  0.3635 sec/batch\n",
      "Epoch: 63/500...  Training Step: 13976...  Training loss: 1.2925...  Val loss: 1.4458...  0.3660 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14001...  Training loss: 1.3193...  Val loss: 1.4454...  0.3638 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14026...  Training loss: 1.2869...  Val loss: 1.4518...  0.3640 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14051...  Training loss: 1.3260...  Val loss: 1.4536...  0.3647 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14076...  Training loss: 1.2759...  Val loss: 1.4587...  0.3635 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14101...  Training loss: 1.3118...  Val loss: 1.4552...  0.3639 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14126...  Training loss: 1.2998...  Val loss: 1.4529...  0.3632 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14151...  Training loss: 1.3093...  Val loss: 1.4489...  0.3634 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14176...  Training loss: 1.3125...  Val loss: 1.4500...  0.3643 sec/batch\n",
      "Epoch: 64/500...  Training Step: 14201...  Training loss: 1.3152...  Val loss: 1.4475...  0.3637 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14226...  Training loss: 1.3001...  Val loss: 1.4472...  0.3724 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14251...  Training loss: 1.2907...  Val loss: 1.4501...  0.3643 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14276...  Training loss: 1.2978...  Val loss: 1.4487...  0.3639 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14301...  Training loss: 1.2738...  Val loss: 1.4441...  0.3682 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14326...  Training loss: 1.3040...  Val loss: 1.4517...  0.3632 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14351...  Training loss: 1.2990...  Val loss: 1.4584...  0.3638 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14376...  Training loss: 1.2951...  Val loss: 1.4465...  0.3631 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14401...  Training loss: 1.2958...  Val loss: 1.4616...  0.3634 sec/batch\n",
      "Epoch: 65/500...  Training Step: 14426...  Training loss: 1.3106...  Val loss: 1.4494...  0.3655 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14451...  Training loss: 1.3045...  Val loss: 1.4498...  0.3637 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14476...  Training loss: 1.2794...  Val loss: 1.4513...  0.3640 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14501...  Training loss: 1.3091...  Val loss: 1.4496...  0.3641 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14526...  Training loss: 1.2626...  Val loss: 1.4461...  0.3636 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14551...  Training loss: 1.3378...  Val loss: 1.4466...  0.3637 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14576...  Training loss: 1.3264...  Val loss: 1.4608...  0.3635 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14601...  Training loss: 1.3187...  Val loss: 1.4492...  0.3630 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14626...  Training loss: 1.2987...  Val loss: 1.4432...  0.3672 sec/batch\n",
      "Epoch: 66/500...  Training Step: 14651...  Training loss: 1.3058...  Val loss: 1.4596...  0.3634 sec/batch\n",
      "Epoch: 67/500...  Training Step: 14676...  Training loss: 1.2931...  Val loss: 1.4398...  0.3637 sec/batch\n",
      "Epoch: 67/500...  Training Step: 14701...  Training loss: 1.2792...  Val loss: 1.4495...  0.3640 sec/batch\n",
      "Epoch: 67/500...  Training Step: 14726...  Training loss: 1.2655...  Val loss: 1.4514...  0.3629 sec/batch\n",
      "Epoch: 67/500...  Training Step: 14751...  Training loss: 1.2788...  Val loss: 1.4548...  0.3675 sec/batch\n",
      "Epoch: 67/500...  Training Step: 14776...  Training loss: 1.2965...  Val loss: 1.4547...  0.3638 sec/batch\n",
      "Epoch: 67/500...  Training Step: 14801...  Training loss: 1.3054...  Val loss: 1.4498...  0.3639 sec/batch\n",
      "Epoch: 67/500...  Training Step: 14826...  Training loss: 1.3137...  Val loss: 1.4444...  0.3622 sec/batch\n",
      "Epoch: 67/500...  Training Step: 14851...  Training loss: 1.2944...  Val loss: 1.4523...  0.3629 sec/batch\n",
      "Epoch: 68/500...  Training Step: 14876...  Training loss: 1.2839...  Val loss: 1.4434...  0.3662 sec/batch\n",
      "Epoch: 68/500...  Training Step: 14901...  Training loss: 1.2765...  Val loss: 1.4310...  0.3633 sec/batch\n",
      "Epoch: 68/500...  Training Step: 14926...  Training loss: 1.2854...  Val loss: 1.4439...  0.3631 sec/batch\n",
      "Epoch: 68/500...  Training Step: 14951...  Training loss: 1.2608...  Val loss: 1.4527...  0.3746 sec/batch\n",
      "Epoch: 68/500...  Training Step: 14976...  Training loss: 1.3009...  Val loss: 1.4535...  0.3632 sec/batch\n",
      "Epoch: 68/500...  Training Step: 15001...  Training loss: 1.2882...  Val loss: 1.4591...  0.3636 sec/batch\n",
      "Epoch: 68/500...  Training Step: 15026...  Training loss: 1.2849...  Val loss: 1.4521...  0.3636 sec/batch\n",
      "Epoch: 68/500...  Training Step: 15051...  Training loss: 1.2749...  Val loss: 1.4555...  0.3762 sec/batch\n",
      "Epoch: 68/500...  Training Step: 15076...  Training loss: 1.2383...  Val loss: 1.4424...  0.3638 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15101...  Training loss: 1.3023...  Val loss: 1.4442...  0.3634 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15126...  Training loss: 1.2609...  Val loss: 1.4460...  0.3634 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15151...  Training loss: 1.2657...  Val loss: 1.4513...  0.3667 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15176...  Training loss: 1.2862...  Val loss: 1.4466...  0.3632 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15201...  Training loss: 1.2916...  Val loss: 1.4411...  0.3688 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15226...  Training loss: 1.2846...  Val loss: 1.4522...  0.3640 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15251...  Training loss: 1.2822...  Val loss: 1.4519...  0.3638 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15276...  Training loss: 1.2859...  Val loss: 1.4578...  0.3652 sec/batch\n",
      "Epoch: 69/500...  Training Step: 15301...  Training loss: 1.2681...  Val loss: 1.4431...  0.3705 sec/batch\n",
      "Epoch: 70/500...  Training Step: 15326...  Training loss: 1.2801...  Val loss: 1.4380...  0.3638 sec/batch\n",
      "Epoch: 70/500...  Training Step: 15351...  Training loss: 1.2821...  Val loss: 1.4467...  0.3633 sec/batch\n",
      "Epoch: 70/500...  Training Step: 15376...  Training loss: 1.2852...  Val loss: 1.4563...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70/500...  Training Step: 15401...  Training loss: 1.2600...  Val loss: 1.4473...  0.3637 sec/batch\n",
      "Epoch: 70/500...  Training Step: 15426...  Training loss: 1.2757...  Val loss: 1.4483...  0.3636 sec/batch\n",
      "Epoch: 70/500...  Training Step: 15451...  Training loss: 1.2648...  Val loss: 1.4529...  0.3662 sec/batch\n",
      "Epoch: 70/500...  Training Step: 15476...  Training loss: 1.2919...  Val loss: 1.4459...  0.3634 sec/batch\n",
      "Epoch: 70/500...  Training Step: 15501...  Training loss: 1.2812...  Val loss: 1.4544...  0.3645 sec/batch\n",
      "Epoch: 70/500...  Training Step: 15526...  Training loss: 1.2657...  Val loss: 1.4532...  0.3735 sec/batch\n",
      "Epoch 70/500 time:82.28320384025574...  finished at 2017-10-30 11:13:46\n",
      "Epoch: 71/500...  Training Step: 15551...  Training loss: 1.2521...  Val loss: 1.4491...  0.3633 sec/batch\n",
      "Epoch: 71/500...  Training Step: 15576...  Training loss: 1.2580...  Val loss: 1.4471...  0.3636 sec/batch\n",
      "Epoch: 71/500...  Training Step: 15601...  Training loss: 1.2886...  Val loss: 1.4513...  0.3633 sec/batch\n",
      "Epoch: 71/500...  Training Step: 15626...  Training loss: 1.2723...  Val loss: 1.4482...  0.3638 sec/batch\n",
      "Epoch: 71/500...  Training Step: 15651...  Training loss: 1.3009...  Val loss: 1.4510...  0.3636 sec/batch\n",
      "Epoch: 71/500...  Training Step: 15676...  Training loss: 1.2647...  Val loss: 1.4467...  0.3660 sec/batch\n",
      "Epoch: 71/500...  Training Step: 15701...  Training loss: 1.3003...  Val loss: 1.4492...  0.3632 sec/batch\n",
      "Epoch: 71/500...  Training Step: 15726...  Training loss: 1.2996...  Val loss: 1.4542...  0.3635 sec/batch\n",
      "Epoch: 71/500...  Training Step: 15751...  Training loss: 1.2891...  Val loss: 1.4492...  0.3634 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15776...  Training loss: 1.2589...  Val loss: 1.4437...  0.3814 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15801...  Training loss: 1.2646...  Val loss: 1.4535...  0.3639 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15826...  Training loss: 1.2812...  Val loss: 1.4412...  0.3637 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15851...  Training loss: 1.2613...  Val loss: 1.4485...  0.3640 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15876...  Training loss: 1.2683...  Val loss: 1.4489...  0.3641 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15901...  Training loss: 1.2690...  Val loss: 1.4513...  0.3636 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15926...  Training loss: 1.2695...  Val loss: 1.4574...  0.3632 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15951...  Training loss: 1.2662...  Val loss: 1.4465...  0.3626 sec/batch\n",
      "Epoch: 72/500...  Training Step: 15976...  Training loss: 1.2701...  Val loss: 1.4466...  0.3636 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16001...  Training loss: 1.2777...  Val loss: 1.4360...  0.3634 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16026...  Training loss: 1.2723...  Val loss: 1.4460...  0.3766 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16051...  Training loss: 1.2419...  Val loss: 1.4514...  0.3715 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16076...  Training loss: 1.2452...  Val loss: 1.4482...  0.3653 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16101...  Training loss: 1.2789...  Val loss: 1.4549...  0.3638 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16126...  Training loss: 1.2714...  Val loss: 1.4466...  0.3640 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16151...  Training loss: 1.2568...  Val loss: 1.4566...  0.3630 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16176...  Training loss: 1.2880...  Val loss: 1.4523...  0.3631 sec/batch\n",
      "Epoch: 73/500...  Training Step: 16201...  Training loss: 1.2642...  Val loss: 1.4445...  0.3655 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16226...  Training loss: 1.2504...  Val loss: 1.4479...  0.3635 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16251...  Training loss: 1.2399...  Val loss: 1.4456...  0.3636 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16276...  Training loss: 1.2593...  Val loss: 1.4395...  0.3632 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16301...  Training loss: 1.2773...  Val loss: 1.4395...  0.3644 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16326...  Training loss: 1.2770...  Val loss: 1.4440...  0.3672 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16351...  Training loss: 1.2762...  Val loss: 1.4524...  0.3636 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16376...  Training loss: 1.2740...  Val loss: 1.4386...  0.3634 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16401...  Training loss: 1.2535...  Val loss: 1.4413...  0.3634 sec/batch\n",
      "Epoch: 74/500...  Training Step: 16426...  Training loss: 1.2529...  Val loss: 1.4464...  0.3642 sec/batch\n",
      "Epoch: 75/500...  Training Step: 16451...  Training loss: 1.2743...  Val loss: 1.4477...  0.3656 sec/batch\n",
      "Epoch: 75/500...  Training Step: 16476...  Training loss: 1.2417...  Val loss: 1.4424...  0.3627 sec/batch\n",
      "Epoch: 75/500...  Training Step: 16501...  Training loss: 1.2522...  Val loss: 1.4433...  0.3704 sec/batch\n",
      "Epoch: 75/500...  Training Step: 16526...  Training loss: 1.2464...  Val loss: 1.4424...  0.3646 sec/batch\n",
      "Epoch: 75/500...  Training Step: 16551...  Training loss: 1.2619...  Val loss: 1.4506...  0.3631 sec/batch\n",
      "Epoch: 75/500...  Training Step: 16576...  Training loss: 1.2730...  Val loss: 1.4491...  0.3631 sec/batch\n",
      "Epoch: 75/500...  Training Step: 16601...  Training loss: 1.2745...  Val loss: 1.4327...  0.3640 sec/batch\n",
      "Epoch: 75/500...  Training Step: 16626...  Training loss: 1.2643...  Val loss: 1.4432...  0.3640 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16651...  Training loss: 1.3647...  Val loss: 1.4371...  0.3643 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16676...  Training loss: 1.2671...  Val loss: 1.4364...  0.3631 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16701...  Training loss: 1.2482...  Val loss: 1.4442...  0.3636 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16726...  Training loss: 1.2333...  Val loss: 1.4532...  0.3659 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16751...  Training loss: 1.2710...  Val loss: 1.4477...  0.3637 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16776...  Training loss: 1.2616...  Val loss: 1.4532...  0.3635 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16801...  Training loss: 1.2645...  Val loss: 1.4539...  0.3634 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16826...  Training loss: 1.2326...  Val loss: 1.4372...  0.3630 sec/batch\n",
      "Epoch: 76/500...  Training Step: 16851...  Training loss: 1.2658...  Val loss: 1.4387...  0.3634 sec/batch\n",
      "Epoch: 77/500...  Training Step: 16876...  Training loss: 1.2620...  Val loss: 1.4387...  0.3631 sec/batch\n",
      "Epoch: 77/500...  Training Step: 16901...  Training loss: 1.2381...  Val loss: 1.4412...  0.3637 sec/batch\n",
      "Epoch: 77/500...  Training Step: 16926...  Training loss: 1.2322...  Val loss: 1.4422...  0.3634 sec/batch\n",
      "Epoch: 77/500...  Training Step: 16951...  Training loss: 1.2227...  Val loss: 1.4502...  0.3630 sec/batch\n",
      "Epoch: 77/500...  Training Step: 16976...  Training loss: 1.2447...  Val loss: 1.4440...  0.3629 sec/batch\n",
      "Epoch: 77/500...  Training Step: 17001...  Training loss: 1.2536...  Val loss: 1.4549...  0.3628 sec/batch\n",
      "Epoch: 77/500...  Training Step: 17026...  Training loss: 1.2487...  Val loss: 1.4507...  0.3636 sec/batch\n",
      "Epoch: 77/500...  Training Step: 17051...  Training loss: 1.2347...  Val loss: 1.4524...  0.3634 sec/batch\n",
      "Epoch: 77/500...  Training Step: 17076...  Training loss: 1.2399...  Val loss: 1.4452...  0.3638 sec/batch\n",
      "Epoch: 78/500...  Training Step: 17101...  Training loss: 1.2602...  Val loss: 1.4485...  0.3637 sec/batch\n",
      "Epoch: 78/500...  Training Step: 17126...  Training loss: 1.2545...  Val loss: 1.4465...  0.3639 sec/batch\n",
      "Epoch: 78/500...  Training Step: 17151...  Training loss: 1.2299...  Val loss: 1.4490...  0.3627 sec/batch\n",
      "Epoch: 78/500...  Training Step: 17176...  Training loss: 1.2529...  Val loss: 1.4493...  0.3638 sec/batch\n",
      "Epoch: 78/500...  Training Step: 17201...  Training loss: 1.2178...  Val loss: 1.4448...  0.3632 sec/batch\n",
      "Epoch: 78/500...  Training Step: 17226...  Training loss: 1.2453...  Val loss: 1.4491...  0.3632 sec/batch\n",
      "Epoch: 78/500...  Training Step: 17251...  Training loss: 1.2620...  Val loss: 1.4516...  0.3632 sec/batch\n",
      "Epoch: 78/500...  Training Step: 17276...  Training loss: 1.2517...  Val loss: 1.4517...  0.3637 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/500...  Training Step: 17301...  Training loss: 1.2428...  Val loss: 1.4551...  0.3633 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17326...  Training loss: 1.2302...  Val loss: 1.4449...  0.3632 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17351...  Training loss: 1.2573...  Val loss: 1.4481...  0.3634 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17376...  Training loss: 1.2441...  Val loss: 1.4493...  0.3633 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17401...  Training loss: 1.2599...  Val loss: 1.4462...  0.3639 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17426...  Training loss: 1.2465...  Val loss: 1.4465...  0.3630 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17451...  Training loss: 1.2424...  Val loss: 1.4507...  0.3632 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17476...  Training loss: 1.2596...  Val loss: 1.4432...  0.3632 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17501...  Training loss: 1.2409...  Val loss: 1.4618...  0.3634 sec/batch\n",
      "Epoch: 79/500...  Training Step: 17526...  Training loss: 1.2354...  Val loss: 1.4489...  0.3630 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17551...  Training loss: 1.2522...  Val loss: 1.4456...  0.3635 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17576...  Training loss: 1.2313...  Val loss: 1.4567...  0.3638 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17601...  Training loss: 1.2536...  Val loss: 1.4456...  0.3631 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17626...  Training loss: 1.2303...  Val loss: 1.4515...  0.3632 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17651...  Training loss: 1.2400...  Val loss: 1.4547...  0.3629 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17676...  Training loss: 1.2382...  Val loss: 1.4515...  0.3637 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17701...  Training loss: 1.2427...  Val loss: 1.4493...  0.3635 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17726...  Training loss: 1.2467...  Val loss: 1.4533...  0.3631 sec/batch\n",
      "Epoch: 80/500...  Training Step: 17751...  Training loss: 1.2813...  Val loss: 1.4542...  0.3632 sec/batch\n",
      "Epoch 80/500 time:81.59379577636719...  finished at 2017-10-30 11:27:24\n",
      "Epoch: 81/500...  Training Step: 17776...  Training loss: 1.2163...  Val loss: 1.4475...  0.3629 sec/batch\n",
      "Epoch: 81/500...  Training Step: 17801...  Training loss: 1.2252...  Val loss: 1.4476...  0.3639 sec/batch\n",
      "Epoch: 81/500...  Training Step: 17826...  Training loss: 1.2242...  Val loss: 1.4537...  0.3634 sec/batch\n",
      "Epoch: 81/500...  Training Step: 17851...  Training loss: 1.2277...  Val loss: 1.4548...  0.3635 sec/batch\n",
      "Epoch: 81/500...  Training Step: 17876...  Training loss: 1.2388...  Val loss: 1.4511...  0.3630 sec/batch\n",
      "Epoch: 81/500...  Training Step: 17901...  Training loss: 1.2167...  Val loss: 1.4513...  0.3631 sec/batch\n",
      "Epoch: 81/500...  Training Step: 17926...  Training loss: 1.2477...  Val loss: 1.4475...  0.3633 sec/batch\n",
      "Epoch: 81/500...  Training Step: 17951...  Training loss: 1.2265...  Val loss: 1.4598...  0.3637 sec/batch\n",
      "Epoch: 81/500...  Training Step: 17976...  Training loss: 1.2540...  Val loss: 1.4462...  0.3629 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18001...  Training loss: 1.2375...  Val loss: 1.4459...  0.3635 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18026...  Training loss: 1.2361...  Val loss: 1.4501...  0.3636 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18051...  Training loss: 1.2324...  Val loss: 1.4479...  0.3631 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18076...  Training loss: 1.2299...  Val loss: 1.4545...  0.3636 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18101...  Training loss: 1.2503...  Val loss: 1.4581...  0.3636 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18126...  Training loss: 1.2435...  Val loss: 1.4568...  0.3633 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18151...  Training loss: 1.2517...  Val loss: 1.4513...  0.3638 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18176...  Training loss: 1.2371...  Val loss: 1.4574...  0.3631 sec/batch\n",
      "Epoch: 82/500...  Training Step: 18201...  Training loss: 1.2435...  Val loss: 1.4495...  0.3640 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18226...  Training loss: 1.2157...  Val loss: 1.4577...  0.3638 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18251...  Training loss: 1.2246...  Val loss: 1.4426...  0.3637 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18276...  Training loss: 1.2125...  Val loss: 1.4502...  0.3637 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18301...  Training loss: 1.1928...  Val loss: 1.4523...  0.3632 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18326...  Training loss: 1.2502...  Val loss: 1.4480...  0.3637 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18351...  Training loss: 1.2453...  Val loss: 1.4460...  0.3639 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18376...  Training loss: 1.2503...  Val loss: 1.4430...  0.3640 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18401...  Training loss: 1.2412...  Val loss: 1.4557...  0.3638 sec/batch\n",
      "Epoch: 83/500...  Training Step: 18426...  Training loss: 1.2351...  Val loss: 1.4439...  0.3638 sec/batch\n",
      "Epoch: 84/500...  Training Step: 18451...  Training loss: 1.2034...  Val loss: 1.4534...  0.3622 sec/batch\n",
      "Epoch: 84/500...  Training Step: 18476...  Training loss: 1.2554...  Val loss: 1.4489...  0.3631 sec/batch\n",
      "Epoch: 84/500...  Training Step: 18501...  Training loss: 1.2434...  Val loss: 1.4503...  0.3632 sec/batch\n",
      "Epoch: 84/500...  Training Step: 18526...  Training loss: 1.2243...  Val loss: 1.4486...  0.3626 sec/batch\n",
      "Epoch: 84/500...  Training Step: 18551...  Training loss: 1.2336...  Val loss: 1.4559...  0.3635 sec/batch\n",
      "Epoch: 84/500...  Training Step: 18576...  Training loss: 1.2495...  Val loss: 1.4559...  0.3637 sec/batch\n",
      "Epoch: 84/500...  Training Step: 18601...  Training loss: 1.2270...  Val loss: 1.4379...  0.3631 sec/batch\n",
      "Epoch: 84/500...  Training Step: 18626...  Training loss: 1.2310...  Val loss: 1.4481...  0.3630 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18651...  Training loss: 1.2411...  Val loss: 1.4422...  0.3636 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18676...  Training loss: 1.2121...  Val loss: 1.4458...  0.3636 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18701...  Training loss: 1.2047...  Val loss: 1.4443...  0.3640 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18726...  Training loss: 1.2060...  Val loss: 1.4614...  0.3636 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18751...  Training loss: 1.2059...  Val loss: 1.4476...  0.3637 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18776...  Training loss: 1.2412...  Val loss: 1.4578...  0.3634 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18801...  Training loss: 1.2217...  Val loss: 1.4559...  0.3629 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18826...  Training loss: 1.2248...  Val loss: 1.4490...  0.3634 sec/batch\n",
      "Epoch: 85/500...  Training Step: 18851...  Training loss: 1.2415...  Val loss: 1.4450...  0.3635 sec/batch\n",
      "Epoch: 86/500...  Training Step: 18876...  Training loss: 1.2399...  Val loss: 1.4383...  0.3633 sec/batch\n",
      "Epoch: 86/500...  Training Step: 18901...  Training loss: 1.1981...  Val loss: 1.4452...  0.3642 sec/batch\n",
      "Epoch: 86/500...  Training Step: 18926...  Training loss: 1.2356...  Val loss: 1.4507...  0.3629 sec/batch\n",
      "Epoch: 86/500...  Training Step: 18951...  Training loss: 1.2052...  Val loss: 1.4558...  0.3631 sec/batch\n",
      "Epoch: 86/500...  Training Step: 18976...  Training loss: 1.2126...  Val loss: 1.4532...  0.3632 sec/batch\n",
      "Epoch: 86/500...  Training Step: 19001...  Training loss: 1.2169...  Val loss: 1.4540...  0.3640 sec/batch\n",
      "Epoch: 86/500...  Training Step: 19026...  Training loss: 1.2277...  Val loss: 1.4526...  0.3632 sec/batch\n",
      "Epoch: 86/500...  Training Step: 19051...  Training loss: 1.2075...  Val loss: 1.4502...  0.3633 sec/batch\n",
      "Epoch: 86/500...  Training Step: 19076...  Training loss: 1.2006...  Val loss: 1.4535...  0.3636 sec/batch\n",
      "Epoch: 87/500...  Training Step: 19101...  Training loss: 1.2211...  Val loss: 1.4508...  0.3633 sec/batch\n",
      "Epoch: 87/500...  Training Step: 19126...  Training loss: 1.2063...  Val loss: 1.4469...  0.3629 sec/batch\n",
      "Epoch: 87/500...  Training Step: 19151...  Training loss: 1.2246...  Val loss: 1.4666...  0.3639 sec/batch\n",
      "Epoch: 87/500...  Training Step: 19176...  Training loss: 1.2251...  Val loss: 1.4593...  0.3639 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/500...  Training Step: 19201...  Training loss: 1.2218...  Val loss: 1.4452...  0.3634 sec/batch\n",
      "Epoch: 87/500...  Training Step: 19226...  Training loss: 1.2219...  Val loss: 1.4575...  0.3635 sec/batch\n",
      "Epoch: 87/500...  Training Step: 19251...  Training loss: 1.2252...  Val loss: 1.4600...  0.3637 sec/batch\n",
      "Epoch: 87/500...  Training Step: 19276...  Training loss: 1.2278...  Val loss: 1.4550...  0.3633 sec/batch\n",
      "Epoch: 87/500...  Training Step: 19301...  Training loss: 1.1983...  Val loss: 1.4561...  0.3634 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19326...  Training loss: 1.2083...  Val loss: 1.4506...  0.3637 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19351...  Training loss: 1.2155...  Val loss: 1.4457...  0.3634 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19376...  Training loss: 1.2272...  Val loss: 1.4587...  0.3641 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19401...  Training loss: 1.2146...  Val loss: 1.4543...  0.3637 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19426...  Training loss: 1.1972...  Val loss: 1.4499...  0.3635 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19451...  Training loss: 1.2044...  Val loss: 1.4613...  0.3632 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19476...  Training loss: 1.2393...  Val loss: 1.4517...  0.3638 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19501...  Training loss: 1.2221...  Val loss: 1.4558...  0.3639 sec/batch\n",
      "Epoch: 88/500...  Training Step: 19526...  Training loss: 1.1983...  Val loss: 1.4568...  0.3634 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19551...  Training loss: 1.2258...  Val loss: 1.4470...  0.3636 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19576...  Training loss: 1.1998...  Val loss: 1.4569...  0.3642 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19601...  Training loss: 1.2229...  Val loss: 1.4620...  0.3640 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19626...  Training loss: 1.1901...  Val loss: 1.4704...  0.3635 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19651...  Training loss: 1.2158...  Val loss: 1.4614...  0.3634 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19676...  Training loss: 1.1996...  Val loss: 1.4503...  0.3637 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19701...  Training loss: 1.2154...  Val loss: 1.4576...  0.3636 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19726...  Training loss: 1.2200...  Val loss: 1.4557...  0.3639 sec/batch\n",
      "Epoch: 89/500...  Training Step: 19751...  Training loss: 1.2293...  Val loss: 1.4661...  0.3637 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19776...  Training loss: 1.2054...  Val loss: 1.4433...  0.3639 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19801...  Training loss: 1.1961...  Val loss: 1.4650...  0.3629 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19826...  Training loss: 1.2185...  Val loss: 1.4591...  0.3637 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19851...  Training loss: 1.1920...  Val loss: 1.4519...  0.3621 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19876...  Training loss: 1.2129...  Val loss: 1.4649...  0.3635 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19901...  Training loss: 1.2063...  Val loss: 1.4569...  0.3638 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19926...  Training loss: 1.2097...  Val loss: 1.4711...  0.3637 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19951...  Training loss: 1.2043...  Val loss: 1.4743...  0.3629 sec/batch\n",
      "Epoch: 90/500...  Training Step: 19976...  Training loss: 1.2231...  Val loss: 1.4562...  0.3632 sec/batch\n",
      "Epoch 90/500 time:81.61986136436462...  finished at 2017-10-30 11:41:02\n",
      "Epoch: 91/500...  Training Step: 20001...  Training loss: 1.2138...  Val loss: 1.4579...  0.3633 sec/batch\n",
      "Epoch: 91/500...  Training Step: 20026...  Training loss: 1.1862...  Val loss: 1.4632...  0.3631 sec/batch\n",
      "Epoch: 91/500...  Training Step: 20051...  Training loss: 1.2175...  Val loss: 1.4515...  0.3630 sec/batch\n",
      "Epoch: 91/500...  Training Step: 20076...  Training loss: 1.1740...  Val loss: 1.4575...  0.3637 sec/batch\n",
      "Epoch: 91/500...  Training Step: 20101...  Training loss: 1.2401...  Val loss: 1.4559...  0.3631 sec/batch\n",
      "Epoch: 91/500...  Training Step: 20126...  Training loss: 1.2331...  Val loss: 1.4729...  0.3626 sec/batch\n",
      "Epoch: 91/500...  Training Step: 20151...  Training loss: 1.2383...  Val loss: 1.4638...  0.3633 sec/batch\n",
      "Epoch: 91/500...  Training Step: 20176...  Training loss: 1.2150...  Val loss: 1.4606...  0.3630 sec/batch\n",
      "Epoch: 91/500...  Training Step: 20201...  Training loss: 1.2129...  Val loss: 1.4692...  0.3637 sec/batch\n",
      "Epoch: 92/500...  Training Step: 20226...  Training loss: 1.2026...  Val loss: 1.4693...  0.3638 sec/batch\n",
      "Epoch: 92/500...  Training Step: 20251...  Training loss: 1.1984...  Val loss: 1.4630...  0.3639 sec/batch\n",
      "Epoch: 92/500...  Training Step: 20276...  Training loss: 1.1873...  Val loss: 1.4554...  0.3624 sec/batch\n",
      "Epoch: 92/500...  Training Step: 20301...  Training loss: 1.1952...  Val loss: 1.4642...  0.3633 sec/batch\n",
      "Epoch: 92/500...  Training Step: 20326...  Training loss: 1.1913...  Val loss: 1.4631...  0.3630 sec/batch\n",
      "Epoch: 92/500...  Training Step: 20351...  Training loss: 1.2203...  Val loss: 1.4608...  0.3633 sec/batch\n",
      "Epoch: 92/500...  Training Step: 20376...  Training loss: 1.2188...  Val loss: 1.4549...  0.3639 sec/batch\n",
      "Epoch: 92/500...  Training Step: 20401...  Training loss: 1.1956...  Val loss: 1.4634...  0.3644 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20426...  Training loss: 1.1901...  Val loss: 1.4559...  0.3633 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20451...  Training loss: 1.1870...  Val loss: 1.4594...  0.3628 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20476...  Training loss: 1.2010...  Val loss: 1.4557...  0.3635 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20501...  Training loss: 1.1861...  Val loss: 1.4546...  0.3629 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20526...  Training loss: 1.2158...  Val loss: 1.4568...  0.3634 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20551...  Training loss: 1.1951...  Val loss: 1.4625...  0.3638 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20576...  Training loss: 1.1980...  Val loss: 1.4696...  0.3633 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20601...  Training loss: 1.1877...  Val loss: 1.4572...  0.3635 sec/batch\n",
      "Epoch: 93/500...  Training Step: 20626...  Training loss: 1.1546...  Val loss: 1.4524...  0.3646 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20651...  Training loss: 1.2163...  Val loss: 1.4564...  0.3632 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20676...  Training loss: 1.1712...  Val loss: 1.4600...  0.3649 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20701...  Training loss: 1.1785...  Val loss: 1.4674...  0.3633 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20726...  Training loss: 1.1984...  Val loss: 1.4573...  0.3631 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20751...  Training loss: 1.2016...  Val loss: 1.4531...  0.3636 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20776...  Training loss: 1.1993...  Val loss: 1.4680...  0.3635 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20801...  Training loss: 1.1869...  Val loss: 1.4724...  0.3636 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20826...  Training loss: 1.1960...  Val loss: 1.4638...  0.3633 sec/batch\n",
      "Epoch: 94/500...  Training Step: 20851...  Training loss: 1.1856...  Val loss: 1.4564...  0.3634 sec/batch\n",
      "Epoch: 95/500...  Training Step: 20876...  Training loss: 1.1981...  Val loss: 1.4476...  0.3634 sec/batch\n",
      "Epoch: 95/500...  Training Step: 20901...  Training loss: 1.1939...  Val loss: 1.4582...  0.3634 sec/batch\n",
      "Epoch: 95/500...  Training Step: 20926...  Training loss: 1.2038...  Val loss: 1.4737...  0.3646 sec/batch\n",
      "Epoch: 95/500...  Training Step: 20951...  Training loss: 1.1795...  Val loss: 1.4737...  0.3652 sec/batch\n",
      "Epoch: 95/500...  Training Step: 20976...  Training loss: 1.1854...  Val loss: 1.4629...  0.3633 sec/batch\n",
      "Epoch: 95/500...  Training Step: 21001...  Training loss: 1.1907...  Val loss: 1.4662...  0.3641 sec/batch\n",
      "Epoch: 95/500...  Training Step: 21026...  Training loss: 1.2107...  Val loss: 1.4633...  0.3634 sec/batch\n",
      "Epoch: 95/500...  Training Step: 21051...  Training loss: 1.2011...  Val loss: 1.4550...  0.3632 sec/batch\n",
      "Epoch: 95/500...  Training Step: 21076...  Training loss: 1.1789...  Val loss: 1.4652...  0.3634 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96/500...  Training Step: 21101...  Training loss: 1.1688...  Val loss: 1.4681...  0.3640 sec/batch\n",
      "Epoch: 96/500...  Training Step: 21126...  Training loss: 1.1710...  Val loss: 1.4581...  0.3642 sec/batch\n",
      "Epoch: 96/500...  Training Step: 21151...  Training loss: 1.1971...  Val loss: 1.4745...  0.3632 sec/batch\n",
      "Epoch: 96/500...  Training Step: 21176...  Training loss: 1.1844...  Val loss: 1.4662...  0.3627 sec/batch\n",
      "Epoch: 96/500...  Training Step: 21201...  Training loss: 1.2157...  Val loss: 1.4649...  0.3631 sec/batch\n",
      "Epoch: 96/500...  Training Step: 21226...  Training loss: 1.1771...  Val loss: 1.4728...  0.3637 sec/batch\n",
      "Epoch: 96/500...  Training Step: 21251...  Training loss: 1.2197...  Val loss: 1.4606...  0.3642 sec/batch\n",
      "Epoch: 96/500...  Training Step: 21276...  Training loss: 1.2110...  Val loss: 1.4699...  0.3640 sec/batch\n",
      "Epoch: 96/500...  Training Step: 21301...  Training loss: 1.1933...  Val loss: 1.4684...  0.3631 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21326...  Training loss: 1.1779...  Val loss: 1.4641...  0.3643 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21351...  Training loss: 1.1795...  Val loss: 1.4752...  0.3633 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21376...  Training loss: 1.1914...  Val loss: 1.4645...  0.3637 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21401...  Training loss: 1.1799...  Val loss: 1.4766...  0.3633 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21426...  Training loss: 1.1829...  Val loss: 1.4656...  0.3636 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21451...  Training loss: 1.1933...  Val loss: 1.4745...  0.3633 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21476...  Training loss: 1.1868...  Val loss: 1.4646...  0.3628 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21501...  Training loss: 1.1835...  Val loss: 1.4582...  0.3634 sec/batch\n",
      "Epoch: 97/500...  Training Step: 21526...  Training loss: 1.1997...  Val loss: 1.4629...  0.3633 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21551...  Training loss: 1.1952...  Val loss: 1.4552...  0.3640 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21576...  Training loss: 1.1904...  Val loss: 1.4601...  0.3636 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21601...  Training loss: 1.1695...  Val loss: 1.4703...  0.3636 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21626...  Training loss: 1.1709...  Val loss: 1.4687...  0.3638 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21651...  Training loss: 1.1988...  Val loss: 1.4723...  0.3633 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21676...  Training loss: 1.1745...  Val loss: 1.4668...  0.3638 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21701...  Training loss: 1.1716...  Val loss: 1.4686...  0.3631 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21726...  Training loss: 1.2032...  Val loss: 1.4654...  0.3633 sec/batch\n",
      "Epoch: 98/500...  Training Step: 21751...  Training loss: 1.1802...  Val loss: 1.4706...  0.3631 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21776...  Training loss: 1.1706...  Val loss: 1.4615...  0.3637 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21801...  Training loss: 1.1654...  Val loss: 1.4751...  0.3638 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21826...  Training loss: 1.1825...  Val loss: 1.4575...  0.3625 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21851...  Training loss: 1.1792...  Val loss: 1.4641...  0.3633 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21876...  Training loss: 1.2006...  Val loss: 1.4735...  0.3637 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21901...  Training loss: 1.1812...  Val loss: 1.4654...  0.3636 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21926...  Training loss: 1.1914...  Val loss: 1.4657...  0.3635 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21951...  Training loss: 1.1690...  Val loss: 1.4702...  0.3630 sec/batch\n",
      "Epoch: 99/500...  Training Step: 21976...  Training loss: 1.1721...  Val loss: 1.4666...  0.3635 sec/batch\n",
      "Epoch: 100/500...  Training Step: 22001...  Training loss: 1.1838...  Val loss: 1.4727...  0.3632 sec/batch\n",
      "Epoch: 100/500...  Training Step: 22026...  Training loss: 1.1580...  Val loss: 1.4683...  0.3639 sec/batch\n",
      "Epoch: 100/500...  Training Step: 22051...  Training loss: 1.1715...  Val loss: 1.4621...  0.3631 sec/batch\n",
      "Epoch: 100/500...  Training Step: 22076...  Training loss: 1.1683...  Val loss: 1.4697...  0.3636 sec/batch\n",
      "Epoch: 100/500...  Training Step: 22101...  Training loss: 1.1847...  Val loss: 1.4776...  0.3638 sec/batch\n",
      "Epoch: 100/500...  Training Step: 22126...  Training loss: 1.1843...  Val loss: 1.4726...  0.3635 sec/batch\n",
      "Epoch: 100/500...  Training Step: 22151...  Training loss: 1.1934...  Val loss: 1.4696...  0.3639 sec/batch\n",
      "Epoch: 100/500...  Training Step: 22176...  Training loss: 1.1851...  Val loss: 1.4774...  0.3631 sec/batch\n",
      "Epoch 100/500 time:81.89489817619324...  finished at 2017-10-30 11:54:40\n",
      "Epoch: 101/500...  Training Step: 22201...  Training loss: 1.2856...  Val loss: 1.4600...  0.3637 sec/batch\n",
      "Epoch: 101/500...  Training Step: 22226...  Training loss: 1.1809...  Val loss: 1.4675...  0.3645 sec/batch\n",
      "Epoch: 101/500...  Training Step: 22251...  Training loss: 1.1803...  Val loss: 1.4700...  0.3634 sec/batch\n",
      "Epoch: 101/500...  Training Step: 22276...  Training loss: 1.1592...  Val loss: 1.4661...  0.3641 sec/batch\n",
      "Epoch: 101/500...  Training Step: 22301...  Training loss: 1.1854...  Val loss: 1.4790...  0.3633 sec/batch\n",
      "Epoch: 101/500...  Training Step: 22326...  Training loss: 1.1768...  Val loss: 1.4709...  0.3633 sec/batch\n",
      "Epoch: 101/500...  Training Step: 22351...  Training loss: 1.1817...  Val loss: 1.4785...  0.3630 sec/batch\n",
      "Epoch: 101/500...  Training Step: 22376...  Training loss: 1.1632...  Val loss: 1.4729...  0.3635 sec/batch\n",
      "Epoch: 101/500...  Training Step: 22401...  Training loss: 1.1813...  Val loss: 1.4754...  0.3639 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22426...  Training loss: 1.1848...  Val loss: 1.4668...  0.3635 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22451...  Training loss: 1.1627...  Val loss: 1.4730...  0.3644 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22476...  Training loss: 1.1486...  Val loss: 1.4656...  0.3680 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22501...  Training loss: 1.1462...  Val loss: 1.4683...  0.3640 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22526...  Training loss: 1.1615...  Val loss: 1.4745...  0.3643 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22551...  Training loss: 1.1739...  Val loss: 1.4742...  0.3674 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22576...  Training loss: 1.1705...  Val loss: 1.4824...  0.3643 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22601...  Training loss: 1.1654...  Val loss: 1.4734...  0.3631 sec/batch\n",
      "Epoch: 102/500...  Training Step: 22626...  Training loss: 1.1643...  Val loss: 1.4777...  0.3630 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22651...  Training loss: 1.1840...  Val loss: 1.4754...  0.3633 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22676...  Training loss: 1.1778...  Val loss: 1.4753...  0.3658 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22701...  Training loss: 1.1540...  Val loss: 1.4749...  0.3636 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22726...  Training loss: 1.1567...  Val loss: 1.4656...  0.3630 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22751...  Training loss: 1.1469...  Val loss: 1.4731...  0.3636 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22776...  Training loss: 1.1702...  Val loss: 1.4757...  0.3637 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22801...  Training loss: 1.1822...  Val loss: 1.4824...  0.3651 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22826...  Training loss: 1.1787...  Val loss: 1.4696...  0.3641 sec/batch\n",
      "Epoch: 103/500...  Training Step: 22851...  Training loss: 1.1670...  Val loss: 1.4851...  0.3632 sec/batch\n",
      "Epoch: 104/500...  Training Step: 22876...  Training loss: 1.1530...  Val loss: 1.4702...  0.3644 sec/batch\n",
      "Epoch: 104/500...  Training Step: 22901...  Training loss: 1.1771...  Val loss: 1.4735...  0.3640 sec/batch\n",
      "Epoch: 104/500...  Training Step: 22926...  Training loss: 1.1705...  Val loss: 1.4731...  0.3671 sec/batch\n",
      "Epoch: 104/500...  Training Step: 22951...  Training loss: 1.1849...  Val loss: 1.4741...  0.3637 sec/batch\n",
      "Epoch: 104/500...  Training Step: 22976...  Training loss: 1.1745...  Val loss: 1.4722...  0.3646 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104/500...  Training Step: 23001...  Training loss: 1.1659...  Val loss: 1.4854...  0.3636 sec/batch\n",
      "Epoch: 104/500...  Training Step: 23026...  Training loss: 1.1777...  Val loss: 1.4676...  0.3634 sec/batch\n",
      "Epoch: 104/500...  Training Step: 23051...  Training loss: 1.1587...  Val loss: 1.4877...  0.3636 sec/batch\n",
      "Epoch: 104/500...  Training Step: 23076...  Training loss: 1.1484...  Val loss: 1.4745...  0.3630 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23101...  Training loss: 1.1762...  Val loss: 1.4761...  0.3633 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23126...  Training loss: 1.1545...  Val loss: 1.4806...  0.3675 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23151...  Training loss: 1.1793...  Val loss: 1.4804...  0.3632 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23176...  Training loss: 1.1516...  Val loss: 1.4828...  0.3633 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23201...  Training loss: 1.1570...  Val loss: 1.4741...  0.3637 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23226...  Training loss: 1.1627...  Val loss: 1.4836...  0.3633 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23251...  Training loss: 1.1633...  Val loss: 1.4623...  0.3641 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23276...  Training loss: 1.1594...  Val loss: 1.4738...  0.3639 sec/batch\n",
      "Epoch: 105/500...  Training Step: 23301...  Training loss: 1.1981...  Val loss: 1.4840...  0.3634 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23326...  Training loss: 1.1436...  Val loss: 1.4807...  0.3633 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23351...  Training loss: 1.1453...  Val loss: 1.4789...  0.3639 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23376...  Training loss: 1.1486...  Val loss: 1.4824...  0.3640 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23401...  Training loss: 1.1556...  Val loss: 1.4933...  0.3632 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23426...  Training loss: 1.1598...  Val loss: 1.4702...  0.3639 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23451...  Training loss: 1.1389...  Val loss: 1.4873...  0.3639 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23476...  Training loss: 1.1702...  Val loss: 1.4686...  0.3633 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23501...  Training loss: 1.1495...  Val loss: 1.4818...  0.3635 sec/batch\n",
      "Epoch: 106/500...  Training Step: 23526...  Training loss: 1.1774...  Val loss: 1.4792...  0.3630 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23551...  Training loss: 1.1629...  Val loss: 1.4724...  0.3631 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23576...  Training loss: 1.1524...  Val loss: 1.4827...  0.3645 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23601...  Training loss: 1.1560...  Val loss: 1.4829...  0.3638 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23626...  Training loss: 1.1555...  Val loss: 1.4936...  0.3639 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23651...  Training loss: 1.1640...  Val loss: 1.4871...  0.3636 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23676...  Training loss: 1.1648...  Val loss: 1.4916...  0.3633 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23701...  Training loss: 1.1628...  Val loss: 1.4974...  0.3633 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23726...  Training loss: 1.1652...  Val loss: 1.4824...  0.3637 sec/batch\n",
      "Epoch: 107/500...  Training Step: 23751...  Training loss: 1.1697...  Val loss: 1.4865...  0.3634 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23776...  Training loss: 1.1445...  Val loss: 1.4846...  0.3639 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23801...  Training loss: 1.1480...  Val loss: 1.4867...  0.3635 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23826...  Training loss: 1.1493...  Val loss: 1.4887...  0.3637 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23851...  Training loss: 1.1306...  Val loss: 1.4835...  0.3633 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23876...  Training loss: 1.1743...  Val loss: 1.4840...  0.3633 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23901...  Training loss: 1.1660...  Val loss: 1.4770...  0.3633 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23926...  Training loss: 1.1734...  Val loss: 1.4916...  0.3638 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23951...  Training loss: 1.1640...  Val loss: 1.4881...  0.3638 sec/batch\n",
      "Epoch: 108/500...  Training Step: 23976...  Training loss: 1.1652...  Val loss: 1.4768...  0.3641 sec/batch\n",
      "Epoch: 109/500...  Training Step: 24001...  Training loss: 1.1342...  Val loss: 1.4842...  0.3637 sec/batch\n",
      "Epoch: 109/500...  Training Step: 24026...  Training loss: 1.1766...  Val loss: 1.4903...  0.3623 sec/batch\n",
      "Epoch: 109/500...  Training Step: 24051...  Training loss: 1.1715...  Val loss: 1.4868...  0.3636 sec/batch\n",
      "Epoch: 109/500...  Training Step: 24076...  Training loss: 1.1552...  Val loss: 1.4822...  0.3624 sec/batch\n",
      "Epoch: 109/500...  Training Step: 24101...  Training loss: 1.1503...  Val loss: 1.4925...  0.3637 sec/batch\n",
      "Epoch: 109/500...  Training Step: 24126...  Training loss: 1.1657...  Val loss: 1.4900...  0.3636 sec/batch\n",
      "Epoch: 109/500...  Training Step: 24151...  Training loss: 1.1500...  Val loss: 1.4858...  0.3637 sec/batch\n",
      "Epoch: 109/500...  Training Step: 24176...  Training loss: 1.1477...  Val loss: 1.4877...  0.3635 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24201...  Training loss: 1.1663...  Val loss: 1.4793...  0.3632 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24226...  Training loss: 1.1403...  Val loss: 1.4855...  0.3638 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24251...  Training loss: 1.1221...  Val loss: 1.4798...  0.3634 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24276...  Training loss: 1.1377...  Val loss: 1.4923...  0.3636 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24301...  Training loss: 1.1326...  Val loss: 1.4828...  0.3640 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24326...  Training loss: 1.1531...  Val loss: 1.4923...  0.3632 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24351...  Training loss: 1.1375...  Val loss: 1.4904...  0.3625 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24376...  Training loss: 1.1409...  Val loss: 1.4956...  0.3632 sec/batch\n",
      "Epoch: 110/500...  Training Step: 24401...  Training loss: 1.1688...  Val loss: 1.4912...  0.3627 sec/batch\n",
      "Epoch 110/500 time:81.61398839950562...  finished at 2017-10-30 12:08:18\n",
      "Epoch: 111/500...  Training Step: 24426...  Training loss: 1.1648...  Val loss: 1.4682...  0.3634 sec/batch\n",
      "Epoch: 111/500...  Training Step: 24451...  Training loss: 1.1155...  Val loss: 1.4887...  0.3639 sec/batch\n",
      "Epoch: 111/500...  Training Step: 24476...  Training loss: 1.1662...  Val loss: 1.4821...  0.3635 sec/batch\n",
      "Epoch: 111/500...  Training Step: 24501...  Training loss: 1.1300...  Val loss: 1.4803...  0.3635 sec/batch\n",
      "Epoch: 111/500...  Training Step: 24526...  Training loss: 1.1372...  Val loss: 1.4929...  0.3636 sec/batch\n",
      "Epoch: 111/500...  Training Step: 24551...  Training loss: 1.1409...  Val loss: 1.4814...  0.3634 sec/batch\n",
      "Epoch: 111/500...  Training Step: 24576...  Training loss: 1.1514...  Val loss: 1.4999...  0.3630 sec/batch\n",
      "Epoch: 111/500...  Training Step: 24601...  Training loss: 1.1361...  Val loss: 1.4892...  0.3633 sec/batch\n",
      "Epoch: 111/500...  Training Step: 24626...  Training loss: 1.1314...  Val loss: 1.5014...  0.3634 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24651...  Training loss: 1.1486...  Val loss: 1.4865...  0.3636 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24676...  Training loss: 1.1404...  Val loss: 1.4889...  0.3633 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24701...  Training loss: 1.1571...  Val loss: 1.5051...  0.3634 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24726...  Training loss: 1.1537...  Val loss: 1.4840...  0.3632 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24751...  Training loss: 1.1419...  Val loss: 1.4892...  0.3630 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24776...  Training loss: 1.1470...  Val loss: 1.4896...  0.3629 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24801...  Training loss: 1.1501...  Val loss: 1.5037...  0.3640 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24826...  Training loss: 1.1510...  Val loss: 1.4908...  0.3636 sec/batch\n",
      "Epoch: 112/500...  Training Step: 24851...  Training loss: 1.1294...  Val loss: 1.4961...  0.3637 sec/batch\n",
      "Epoch: 113/500...  Training Step: 24876...  Training loss: 1.1469...  Val loss: 1.4970...  0.3628 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113/500...  Training Step: 24901...  Training loss: 1.1504...  Val loss: 1.4816...  0.3768 sec/batch\n",
      "Epoch: 113/500...  Training Step: 24926...  Training loss: 1.1588...  Val loss: 1.4927...  0.3635 sec/batch\n",
      "Epoch: 113/500...  Training Step: 24951...  Training loss: 1.1454...  Val loss: 1.4837...  0.3679 sec/batch\n",
      "Epoch: 113/500...  Training Step: 24976...  Training loss: 1.1296...  Val loss: 1.4876...  0.3657 sec/batch\n",
      "Epoch: 113/500...  Training Step: 25001...  Training loss: 1.1235...  Val loss: 1.4991...  0.3632 sec/batch\n",
      "Epoch: 113/500...  Training Step: 25026...  Training loss: 1.1573...  Val loss: 1.4889...  0.3641 sec/batch\n",
      "Epoch: 113/500...  Training Step: 25051...  Training loss: 1.1490...  Val loss: 1.4921...  0.3664 sec/batch\n",
      "Epoch: 113/500...  Training Step: 25076...  Training loss: 1.1355...  Val loss: 1.4977...  0.3629 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25101...  Training loss: 1.1429...  Val loss: 1.4791...  0.3646 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25126...  Training loss: 1.1241...  Val loss: 1.4914...  0.3641 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25151...  Training loss: 1.1620...  Val loss: 1.5014...  0.3635 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25176...  Training loss: 1.1110...  Val loss: 1.5085...  0.3650 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25201...  Training loss: 1.1455...  Val loss: 1.4980...  0.3636 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25226...  Training loss: 1.1260...  Val loss: 1.4885...  0.3642 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25251...  Training loss: 1.1316...  Val loss: 1.4857...  0.3632 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25276...  Training loss: 1.1590...  Val loss: 1.5000...  0.3633 sec/batch\n",
      "Epoch: 114/500...  Training Step: 25301...  Training loss: 1.1584...  Val loss: 1.5085...  0.3655 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25326...  Training loss: 1.1350...  Val loss: 1.4810...  0.3636 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25351...  Training loss: 1.1202...  Val loss: 1.5035...  0.3648 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25376...  Training loss: 1.1348...  Val loss: 1.5008...  0.3631 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25401...  Training loss: 1.1159...  Val loss: 1.5001...  0.3630 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25426...  Training loss: 1.1440...  Val loss: 1.4953...  0.3664 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25451...  Training loss: 1.1302...  Val loss: 1.4981...  0.3625 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25476...  Training loss: 1.1333...  Val loss: 1.4937...  0.3643 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25501...  Training loss: 1.1373...  Val loss: 1.5117...  0.3636 sec/batch\n",
      "Epoch: 115/500...  Training Step: 25526...  Training loss: 1.1499...  Val loss: 1.4953...  0.3638 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25551...  Training loss: 1.1399...  Val loss: 1.4901...  0.3642 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25576...  Training loss: 1.1162...  Val loss: 1.5103...  0.3635 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25601...  Training loss: 1.1494...  Val loss: 1.4983...  0.3634 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25626...  Training loss: 1.1067...  Val loss: 1.5013...  0.3664 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25651...  Training loss: 1.1519...  Val loss: 1.4940...  0.3635 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25676...  Training loss: 1.1570...  Val loss: 1.5126...  0.3643 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25701...  Training loss: 1.1605...  Val loss: 1.5069...  0.3635 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25726...  Training loss: 1.1263...  Val loss: 1.4961...  0.3629 sec/batch\n",
      "Epoch: 116/500...  Training Step: 25751...  Training loss: 1.1424...  Val loss: 1.5111...  0.3653 sec/batch\n",
      "Epoch: 117/500...  Training Step: 25776...  Training loss: 1.1272...  Val loss: 1.5044...  0.3630 sec/batch\n",
      "Epoch: 117/500...  Training Step: 25801...  Training loss: 1.1271...  Val loss: 1.5093...  0.3635 sec/batch\n",
      "Epoch: 117/500...  Training Step: 25826...  Training loss: 1.1259...  Val loss: 1.5041...  0.3638 sec/batch\n",
      "Epoch: 117/500...  Training Step: 25851...  Training loss: 1.1219...  Val loss: 1.5173...  0.3628 sec/batch\n",
      "Epoch: 117/500...  Training Step: 25876...  Training loss: 1.1199...  Val loss: 1.5018...  0.3666 sec/batch\n",
      "Epoch: 117/500...  Training Step: 25901...  Training loss: 1.1285...  Val loss: 1.5033...  0.3635 sec/batch\n",
      "Epoch: 117/500...  Training Step: 25926...  Training loss: 1.1450...  Val loss: 1.4908...  0.3643 sec/batch\n",
      "Epoch: 117/500...  Training Step: 25951...  Training loss: 1.1279...  Val loss: 1.5020...  0.3639 sec/batch\n",
      "Epoch: 118/500...  Training Step: 25976...  Training loss: 1.1250...  Val loss: 1.4932...  0.3635 sec/batch\n",
      "Epoch: 118/500...  Training Step: 26001...  Training loss: 1.1259...  Val loss: 1.5010...  0.3664 sec/batch\n",
      "Epoch: 118/500...  Training Step: 26026...  Training loss: 1.1405...  Val loss: 1.4998...  0.3636 sec/batch\n",
      "Epoch: 118/500...  Training Step: 26051...  Training loss: 1.1155...  Val loss: 1.5083...  0.3634 sec/batch\n",
      "Epoch: 118/500...  Training Step: 26076...  Training loss: 1.1440...  Val loss: 1.4947...  0.3672 sec/batch\n",
      "Epoch: 118/500...  Training Step: 26101...  Training loss: 1.1235...  Val loss: 1.5131...  0.3640 sec/batch\n",
      "Epoch: 118/500...  Training Step: 26126...  Training loss: 1.1130...  Val loss: 1.5082...  0.3635 sec/batch\n",
      "Epoch: 118/500...  Training Step: 26151...  Training loss: 1.1061...  Val loss: 1.5076...  0.3633 sec/batch\n",
      "Epoch: 118/500...  Training Step: 26176...  Training loss: 1.0945...  Val loss: 1.4937...  0.3630 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26201...  Training loss: 1.1375...  Val loss: 1.4994...  0.3667 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26226...  Training loss: 1.0991...  Val loss: 1.5042...  0.3633 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26251...  Training loss: 1.1013...  Val loss: 1.5053...  0.3636 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26276...  Training loss: 1.1327...  Val loss: 1.5026...  0.3635 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26301...  Training loss: 1.1341...  Val loss: 1.4930...  0.3640 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26326...  Training loss: 1.1185...  Val loss: 1.5115...  0.3672 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26351...  Training loss: 1.1134...  Val loss: 1.5167...  0.3638 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26376...  Training loss: 1.1292...  Val loss: 1.5112...  0.3634 sec/batch\n",
      "Epoch: 119/500...  Training Step: 26401...  Training loss: 1.1195...  Val loss: 1.4963...  0.3633 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26426...  Training loss: 1.1305...  Val loss: 1.4847...  0.3634 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26451...  Training loss: 1.1361...  Val loss: 1.5100...  0.3669 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26476...  Training loss: 1.1264...  Val loss: 1.5138...  0.3639 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26501...  Training loss: 1.1063...  Val loss: 1.5072...  0.3636 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26526...  Training loss: 1.1148...  Val loss: 1.5043...  0.3678 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26551...  Training loss: 1.1204...  Val loss: 1.5107...  0.3638 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26576...  Training loss: 1.1338...  Val loss: 1.5163...  0.3647 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26601...  Training loss: 1.1260...  Val loss: 1.4999...  0.3632 sec/batch\n",
      "Epoch: 120/500...  Training Step: 26626...  Training loss: 1.1103...  Val loss: 1.5052...  0.3632 sec/batch\n",
      "Epoch 120/500 time:82.1206305027008...  finished at 2017-10-30 12:21:57\n",
      "Epoch: 121/500...  Training Step: 26651...  Training loss: 1.0934...  Val loss: 1.5031...  0.3656 sec/batch\n",
      "Epoch: 121/500...  Training Step: 26676...  Training loss: 1.1096...  Val loss: 1.5062...  0.3638 sec/batch\n",
      "Epoch: 121/500...  Training Step: 26701...  Training loss: 1.1276...  Val loss: 1.5114...  0.3630 sec/batch\n",
      "Epoch: 121/500...  Training Step: 26726...  Training loss: 1.1255...  Val loss: 1.4990...  0.3642 sec/batch\n",
      "Epoch: 121/500...  Training Step: 26751...  Training loss: 1.1352...  Val loss: 1.5092...  0.3637 sec/batch\n",
      "Epoch: 121/500...  Training Step: 26776...  Training loss: 1.1132...  Val loss: 1.5122...  0.3681 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121/500...  Training Step: 26801...  Training loss: 1.1366...  Val loss: 1.5103...  0.3636 sec/batch\n",
      "Epoch: 121/500...  Training Step: 26826...  Training loss: 1.1442...  Val loss: 1.5156...  0.3630 sec/batch\n",
      "Epoch: 121/500...  Training Step: 26851...  Training loss: 1.1277...  Val loss: 1.5135...  0.3638 sec/batch\n",
      "Epoch: 122/500...  Training Step: 26876...  Training loss: 1.1155...  Val loss: 1.5073...  0.3640 sec/batch\n",
      "Epoch: 122/500...  Training Step: 26901...  Training loss: 1.1158...  Val loss: 1.5176...  0.3663 sec/batch\n",
      "Epoch: 122/500...  Training Step: 26926...  Training loss: 1.1299...  Val loss: 1.5013...  0.3634 sec/batch\n",
      "Epoch: 122/500...  Training Step: 26951...  Training loss: 1.1226...  Val loss: 1.5076...  0.3631 sec/batch\n",
      "Epoch: 122/500...  Training Step: 26976...  Training loss: 1.1144...  Val loss: 1.5180...  0.3637 sec/batch\n",
      "Epoch: 122/500...  Training Step: 27001...  Training loss: 1.1264...  Val loss: 1.5161...  0.3634 sec/batch\n",
      "Epoch: 122/500...  Training Step: 27026...  Training loss: 1.1275...  Val loss: 1.5153...  0.3642 sec/batch\n",
      "Epoch: 122/500...  Training Step: 27051...  Training loss: 1.1152...  Val loss: 1.5077...  0.3632 sec/batch\n",
      "Epoch: 122/500...  Training Step: 27076...  Training loss: 1.1217...  Val loss: 1.5189...  0.3629 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27101...  Training loss: 1.1332...  Val loss: 1.4983...  0.3662 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27126...  Training loss: 1.1178...  Val loss: 1.5003...  0.3635 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27151...  Training loss: 1.1062...  Val loss: 1.5062...  0.3631 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27176...  Training loss: 1.0983...  Val loss: 1.4974...  0.3637 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27201...  Training loss: 1.1172...  Val loss: 1.5224...  0.3637 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27226...  Training loss: 1.1095...  Val loss: 1.5177...  0.3666 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27251...  Training loss: 1.1056...  Val loss: 1.5114...  0.3631 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27276...  Training loss: 1.1237...  Val loss: 1.5193...  0.3637 sec/batch\n",
      "Epoch: 123/500...  Training Step: 27301...  Training loss: 1.1136...  Val loss: 1.5206...  0.3632 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27326...  Training loss: 1.1055...  Val loss: 1.4914...  0.3637 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27351...  Training loss: 1.0940...  Val loss: 1.5143...  0.3656 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27376...  Training loss: 1.1175...  Val loss: 1.5044...  0.3642 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27401...  Training loss: 1.1067...  Val loss: 1.5144...  0.3637 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27426...  Training loss: 1.1149...  Val loss: 1.5194...  0.3646 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27451...  Training loss: 1.1107...  Val loss: 1.5151...  0.3639 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27476...  Training loss: 1.1114...  Val loss: 1.5009...  0.3647 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27501...  Training loss: 1.1027...  Val loss: 1.5141...  0.3633 sec/batch\n",
      "Epoch: 124/500...  Training Step: 27526...  Training loss: 1.0944...  Val loss: 1.5210...  0.3635 sec/batch\n",
      "Epoch: 125/500...  Training Step: 27551...  Training loss: 1.1329...  Val loss: 1.5123...  0.3648 sec/batch\n",
      "Epoch: 125/500...  Training Step: 27576...  Training loss: 1.0926...  Val loss: 1.5152...  0.3646 sec/batch\n",
      "Epoch: 125/500...  Training Step: 27601...  Training loss: 1.0987...  Val loss: 1.5113...  0.3636 sec/batch\n",
      "Epoch: 125/500...  Training Step: 27626...  Training loss: 1.1091...  Val loss: 1.5193...  0.3666 sec/batch\n",
      "Epoch: 125/500...  Training Step: 27651...  Training loss: 1.1085...  Val loss: 1.5258...  0.3636 sec/batch\n",
      "Epoch: 125/500...  Training Step: 27676...  Training loss: 1.1137...  Val loss: 1.5226...  0.3650 sec/batch\n",
      "Epoch: 125/500...  Training Step: 27701...  Training loss: 1.1211...  Val loss: 1.5138...  0.3635 sec/batch\n",
      "Epoch: 125/500...  Training Step: 27726...  Training loss: 1.1047...  Val loss: 1.5143...  0.3631 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27751...  Training loss: 1.2144...  Val loss: 1.5121...  0.3659 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27776...  Training loss: 1.1193...  Val loss: 1.5134...  0.3633 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27801...  Training loss: 1.0983...  Val loss: 1.5194...  0.3633 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27826...  Training loss: 1.0953...  Val loss: 1.5208...  0.3639 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27851...  Training loss: 1.1275...  Val loss: 1.5283...  0.3638 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27876...  Training loss: 1.1120...  Val loss: 1.5176...  0.3636 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27901...  Training loss: 1.1029...  Val loss: 1.5292...  0.3632 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27926...  Training loss: 1.0772...  Val loss: 1.5209...  0.3628 sec/batch\n",
      "Epoch: 126/500...  Training Step: 27951...  Training loss: 1.1132...  Val loss: 1.5213...  0.3628 sec/batch\n",
      "Epoch: 127/500...  Training Step: 27976...  Training loss: 1.1079...  Val loss: 1.5107...  0.3637 sec/batch\n",
      "Epoch: 127/500...  Training Step: 28001...  Training loss: 1.0904...  Val loss: 1.5264...  0.3631 sec/batch\n",
      "Epoch: 127/500...  Training Step: 28026...  Training loss: 1.0869...  Val loss: 1.5211...  0.3634 sec/batch\n",
      "Epoch: 127/500...  Training Step: 28051...  Training loss: 1.0860...  Val loss: 1.5326...  0.3638 sec/batch\n",
      "Epoch: 127/500...  Training Step: 28076...  Training loss: 1.1035...  Val loss: 1.5259...  0.3638 sec/batch\n",
      "Epoch: 127/500...  Training Step: 28101...  Training loss: 1.1045...  Val loss: 1.5285...  0.3637 sec/batch\n",
      "Epoch: 127/500...  Training Step: 28126...  Training loss: 1.1032...  Val loss: 1.5337...  0.3634 sec/batch\n",
      "Epoch: 127/500...  Training Step: 28151...  Training loss: 1.0957...  Val loss: 1.5279...  0.3627 sec/batch\n",
      "Epoch: 127/500...  Training Step: 28176...  Training loss: 1.0955...  Val loss: 1.5251...  0.3635 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28201...  Training loss: 1.1160...  Val loss: 1.5230...  0.3641 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28226...  Training loss: 1.1187...  Val loss: 1.5308...  0.3640 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28251...  Training loss: 1.0863...  Val loss: 1.5216...  0.3634 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28276...  Training loss: 1.0930...  Val loss: 1.5236...  0.3629 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28301...  Training loss: 1.0833...  Val loss: 1.5226...  0.3631 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28326...  Training loss: 1.1079...  Val loss: 1.5269...  0.3633 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28351...  Training loss: 1.1053...  Val loss: 1.5393...  0.3632 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28376...  Training loss: 1.1058...  Val loss: 1.5206...  0.3634 sec/batch\n",
      "Epoch: 128/500...  Training Step: 28401...  Training loss: 1.0978...  Val loss: 1.5306...  0.3805 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28426...  Training loss: 1.0806...  Val loss: 1.5158...  0.3659 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28451...  Training loss: 1.1132...  Val loss: 1.5289...  0.3639 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28476...  Training loss: 1.1068...  Val loss: 1.5225...  0.3641 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28501...  Training loss: 1.1315...  Val loss: 1.5278...  0.3635 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28526...  Training loss: 1.1065...  Val loss: 1.5202...  0.3634 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28551...  Training loss: 1.1021...  Val loss: 1.5306...  0.3640 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28576...  Training loss: 1.1071...  Val loss: 1.5302...  0.3639 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28601...  Training loss: 1.0906...  Val loss: 1.5361...  0.3633 sec/batch\n",
      "Epoch: 129/500...  Training Step: 28626...  Training loss: 1.0816...  Val loss: 1.5263...  0.3658 sec/batch\n",
      "Epoch: 130/500...  Training Step: 28651...  Training loss: 1.1046...  Val loss: 1.5216...  0.3633 sec/batch\n",
      "Epoch: 130/500...  Training Step: 28676...  Training loss: 1.0833...  Val loss: 1.5301...  0.3637 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130/500...  Training Step: 28701...  Training loss: 1.1002...  Val loss: 1.5310...  0.3635 sec/batch\n",
      "Epoch: 130/500...  Training Step: 28726...  Training loss: 1.1013...  Val loss: 1.5341...  0.3638 sec/batch\n",
      "Epoch: 130/500...  Training Step: 28751...  Training loss: 1.0884...  Val loss: 1.5233...  0.3673 sec/batch\n",
      "Epoch: 130/500...  Training Step: 28776...  Training loss: 1.0964...  Val loss: 1.5309...  0.3636 sec/batch\n",
      "Epoch: 130/500...  Training Step: 28801...  Training loss: 1.0954...  Val loss: 1.5280...  0.3637 sec/batch\n",
      "Epoch: 130/500...  Training Step: 28826...  Training loss: 1.0946...  Val loss: 1.5241...  0.3630 sec/batch\n",
      "Epoch: 130/500...  Training Step: 28851...  Training loss: 1.1244...  Val loss: 1.5329...  0.3636 sec/batch\n",
      "Epoch 130/500 time:81.79773592948914...  finished at 2017-10-30 12:35:36\n",
      "Epoch: 131/500...  Training Step: 28876...  Training loss: 1.0810...  Val loss: 1.5315...  0.3667 sec/batch\n",
      "Epoch: 131/500...  Training Step: 28901...  Training loss: 1.0806...  Val loss: 1.5271...  0.3637 sec/batch\n",
      "Epoch: 131/500...  Training Step: 28926...  Training loss: 1.0929...  Val loss: 1.5357...  0.3631 sec/batch\n",
      "Epoch: 131/500...  Training Step: 28951...  Training loss: 1.0903...  Val loss: 1.5259...  0.3638 sec/batch\n",
      "Epoch: 131/500...  Training Step: 28976...  Training loss: 1.0975...  Val loss: 1.5267...  0.3632 sec/batch\n",
      "Epoch: 131/500...  Training Step: 29001...  Training loss: 1.0682...  Val loss: 1.5313...  0.3652 sec/batch\n",
      "Epoch: 131/500...  Training Step: 29026...  Training loss: 1.0996...  Val loss: 1.5207...  0.3637 sec/batch\n",
      "Epoch: 131/500...  Training Step: 29051...  Training loss: 1.0927...  Val loss: 1.5387...  0.3637 sec/batch\n",
      "Epoch: 131/500...  Training Step: 29076...  Training loss: 1.1032...  Val loss: 1.5347...  0.3655 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29101...  Training loss: 1.1151...  Val loss: 1.5112...  0.3638 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29126...  Training loss: 1.0909...  Val loss: 1.5273...  0.3638 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29151...  Training loss: 1.0894...  Val loss: 1.5281...  0.3629 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29176...  Training loss: 1.0868...  Val loss: 1.5287...  0.3636 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29201...  Training loss: 1.0928...  Val loss: 1.5393...  0.3676 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29226...  Training loss: 1.1014...  Val loss: 1.5377...  0.3632 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29251...  Training loss: 1.0950...  Val loss: 1.5383...  0.3629 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29276...  Training loss: 1.1014...  Val loss: 1.5390...  0.3648 sec/batch\n",
      "Epoch: 132/500...  Training Step: 29301...  Training loss: 1.1085...  Val loss: 1.5463...  0.3635 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29326...  Training loss: 1.0782...  Val loss: 1.5191...  0.3635 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29351...  Training loss: 1.0927...  Val loss: 1.5325...  0.3631 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29376...  Training loss: 1.0862...  Val loss: 1.5384...  0.3637 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29401...  Training loss: 1.0717...  Val loss: 1.5297...  0.3632 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29426...  Training loss: 1.0990...  Val loss: 1.5358...  0.3641 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29451...  Training loss: 1.1029...  Val loss: 1.5254...  0.3639 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29476...  Training loss: 1.1035...  Val loss: 1.5395...  0.3638 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29501...  Training loss: 1.0936...  Val loss: 1.5375...  0.3632 sec/batch\n",
      "Epoch: 133/500...  Training Step: 29526...  Training loss: 1.0967...  Val loss: 1.5346...  0.3637 sec/batch\n",
      "Epoch: 134/500...  Training Step: 29551...  Training loss: 1.0722...  Val loss: 1.5265...  0.3633 sec/batch\n",
      "Epoch: 134/500...  Training Step: 29576...  Training loss: 1.1078...  Val loss: 1.5406...  0.3633 sec/batch\n",
      "Epoch: 134/500...  Training Step: 29601...  Training loss: 1.1090...  Val loss: 1.5411...  0.3638 sec/batch\n",
      "Epoch: 134/500...  Training Step: 29626...  Training loss: 1.0829...  Val loss: 1.5409...  0.3635 sec/batch\n",
      "Epoch: 134/500...  Training Step: 29651...  Training loss: 1.0858...  Val loss: 1.5539...  0.3629 sec/batch\n",
      "Epoch: 134/500...  Training Step: 29676...  Training loss: 1.0993...  Val loss: 1.5512...  0.3637 sec/batch\n",
      "Epoch: 134/500...  Training Step: 29701...  Training loss: 1.0878...  Val loss: 1.5326...  0.3630 sec/batch\n",
      "Epoch: 134/500...  Training Step: 29726...  Training loss: 1.0791...  Val loss: 1.5442...  0.3633 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29751...  Training loss: 1.0951...  Val loss: 1.5320...  0.3633 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29776...  Training loss: 1.0731...  Val loss: 1.5356...  0.3641 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29801...  Training loss: 1.0522...  Val loss: 1.5339...  0.3644 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29826...  Training loss: 1.0658...  Val loss: 1.5489...  0.3642 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29851...  Training loss: 1.0695...  Val loss: 1.5473...  0.3643 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29876...  Training loss: 1.0866...  Val loss: 1.5441...  0.3636 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29901...  Training loss: 1.0701...  Val loss: 1.5527...  0.3645 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29926...  Training loss: 1.0700...  Val loss: 1.5512...  0.3642 sec/batch\n",
      "Epoch: 135/500...  Training Step: 29951...  Training loss: 1.0937...  Val loss: 1.5414...  0.3645 sec/batch\n",
      "Epoch: 136/500...  Training Step: 29976...  Training loss: 1.0952...  Val loss: 1.5253...  0.3639 sec/batch\n",
      "Epoch: 136/500...  Training Step: 30001...  Training loss: 1.0494...  Val loss: 1.5476...  0.3640 sec/batch\n",
      "Epoch: 136/500...  Training Step: 30026...  Training loss: 1.1001...  Val loss: 1.5345...  0.3640 sec/batch\n",
      "Epoch: 136/500...  Training Step: 30051...  Training loss: 1.0645...  Val loss: 1.5408...  0.3643 sec/batch\n",
      "Epoch: 136/500...  Training Step: 30076...  Training loss: 1.0729...  Val loss: 1.5446...  0.3642 sec/batch\n",
      "Epoch: 136/500...  Training Step: 30101...  Training loss: 1.0827...  Val loss: 1.5354...  0.3642 sec/batch\n",
      "Epoch: 136/500...  Training Step: 30126...  Training loss: 1.0682...  Val loss: 1.5542...  0.3636 sec/batch\n",
      "Epoch: 136/500...  Training Step: 30151...  Training loss: 1.0612...  Val loss: 1.5455...  0.3645 sec/batch\n",
      "Epoch: 136/500...  Training Step: 30176...  Training loss: 1.0796...  Val loss: 1.5485...  0.3638 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30201...  Training loss: 1.0846...  Val loss: 1.5486...  0.3637 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30226...  Training loss: 1.0681...  Val loss: 1.5523...  0.3646 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30251...  Training loss: 1.0896...  Val loss: 1.5682...  0.3638 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30276...  Training loss: 1.0877...  Val loss: 1.5448...  0.3637 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30301...  Training loss: 1.0840...  Val loss: 1.5389...  0.3643 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30326...  Training loss: 1.0827...  Val loss: 1.5434...  0.3640 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30351...  Training loss: 1.0877...  Val loss: 1.5610...  0.3641 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30376...  Training loss: 1.0836...  Val loss: 1.5504...  0.3641 sec/batch\n",
      "Epoch: 137/500...  Training Step: 30401...  Training loss: 1.0623...  Val loss: 1.5485...  0.3646 sec/batch\n",
      "Epoch: 138/500...  Training Step: 30426...  Training loss: 1.0746...  Val loss: 1.5515...  0.3644 sec/batch\n",
      "Epoch: 138/500...  Training Step: 30451...  Training loss: 1.0819...  Val loss: 1.5435...  0.3642 sec/batch\n",
      "Epoch: 138/500...  Training Step: 30476...  Training loss: 1.0878...  Val loss: 1.5405...  0.3641 sec/batch\n",
      "Epoch: 138/500...  Training Step: 30501...  Training loss: 1.0847...  Val loss: 1.5501...  0.3638 sec/batch\n",
      "Epoch: 138/500...  Training Step: 30526...  Training loss: 1.0708...  Val loss: 1.5439...  0.3636 sec/batch\n",
      "Epoch: 138/500...  Training Step: 30551...  Training loss: 1.0611...  Val loss: 1.5514...  0.3644 sec/batch\n",
      "Epoch: 138/500...  Training Step: 30576...  Training loss: 1.0885...  Val loss: 1.5478...  0.3639 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138/500...  Training Step: 30601...  Training loss: 1.0806...  Val loss: 1.5460...  0.3642 sec/batch\n",
      "Epoch: 138/500...  Training Step: 30626...  Training loss: 1.0660...  Val loss: 1.5428...  0.3637 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30651...  Training loss: 1.0798...  Val loss: 1.5390...  0.3642 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30676...  Training loss: 1.0637...  Val loss: 1.5492...  0.3639 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30701...  Training loss: 1.0960...  Val loss: 1.5561...  0.3640 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30726...  Training loss: 1.0483...  Val loss: 1.5597...  0.3638 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30751...  Training loss: 1.0796...  Val loss: 1.5563...  0.3641 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30776...  Training loss: 1.0653...  Val loss: 1.5419...  0.3636 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30801...  Training loss: 1.0703...  Val loss: 1.5454...  0.3636 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30826...  Training loss: 1.0880...  Val loss: 1.5542...  0.3639 sec/batch\n",
      "Epoch: 139/500...  Training Step: 30851...  Training loss: 1.0810...  Val loss: 1.5599...  0.3640 sec/batch\n",
      "Epoch: 140/500...  Training Step: 30876...  Training loss: 1.0707...  Val loss: 1.5323...  0.3646 sec/batch\n",
      "Epoch: 140/500...  Training Step: 30901...  Training loss: 1.0573...  Val loss: 1.5657...  0.3640 sec/batch\n",
      "Epoch: 140/500...  Training Step: 30926...  Training loss: 1.0947...  Val loss: 1.5510...  0.3638 sec/batch\n",
      "Epoch: 140/500...  Training Step: 30951...  Training loss: 1.0622...  Val loss: 1.5413...  0.3644 sec/batch\n",
      "Epoch: 140/500...  Training Step: 30976...  Training loss: 1.0832...  Val loss: 1.5549...  0.3640 sec/batch\n",
      "Epoch: 140/500...  Training Step: 31001...  Training loss: 1.0765...  Val loss: 1.5525...  0.3637 sec/batch\n",
      "Epoch: 140/500...  Training Step: 31026...  Training loss: 1.0742...  Val loss: 1.5533...  0.3638 sec/batch\n",
      "Epoch: 140/500...  Training Step: 31051...  Training loss: 1.0617...  Val loss: 1.5668...  0.3636 sec/batch\n",
      "Epoch: 140/500...  Training Step: 31076...  Training loss: 1.0873...  Val loss: 1.5550...  0.3637 sec/batch\n",
      "Epoch 140/500 time:82.06361198425293...  finished at 2017-10-30 12:49:15\n",
      "Epoch: 141/500...  Training Step: 31101...  Training loss: 1.0846...  Val loss: 1.5475...  0.3635 sec/batch\n",
      "Epoch: 141/500...  Training Step: 31126...  Training loss: 1.0616...  Val loss: 1.5659...  0.3639 sec/batch\n",
      "Epoch: 141/500...  Training Step: 31151...  Training loss: 1.0813...  Val loss: 1.5477...  0.3636 sec/batch\n",
      "Epoch: 141/500...  Training Step: 31176...  Training loss: 1.0578...  Val loss: 1.5472...  0.3630 sec/batch\n",
      "Epoch: 141/500...  Training Step: 31201...  Training loss: 1.0826...  Val loss: 1.5461...  0.3635 sec/batch\n",
      "Epoch: 141/500...  Training Step: 31226...  Training loss: 1.0947...  Val loss: 1.5632...  0.3639 sec/batch\n",
      "Epoch: 141/500...  Training Step: 31251...  Training loss: 1.0993...  Val loss: 1.5590...  0.3640 sec/batch\n",
      "Epoch: 141/500...  Training Step: 31276...  Training loss: 1.0793...  Val loss: 1.5517...  0.3645 sec/batch\n",
      "Epoch: 141/500...  Training Step: 31301...  Training loss: 1.0736...  Val loss: 1.5695...  0.3642 sec/batch\n",
      "Epoch: 142/500...  Training Step: 31326...  Training loss: 1.0777...  Val loss: 1.5507...  0.3635 sec/batch\n",
      "Epoch: 142/500...  Training Step: 31351...  Training loss: 1.0662...  Val loss: 1.5589...  0.3636 sec/batch\n",
      "Epoch: 142/500...  Training Step: 31376...  Training loss: 1.0638...  Val loss: 1.5562...  0.3639 sec/batch\n",
      "Epoch: 142/500...  Training Step: 31401...  Training loss: 1.0611...  Val loss: 1.5623...  0.3642 sec/batch\n",
      "Epoch: 142/500...  Training Step: 31426...  Training loss: 1.0552...  Val loss: 1.5567...  0.3641 sec/batch\n",
      "Epoch: 142/500...  Training Step: 31451...  Training loss: 1.0767...  Val loss: 1.5485...  0.3642 sec/batch\n",
      "Epoch: 142/500...  Training Step: 31476...  Training loss: 1.0769...  Val loss: 1.5380...  0.3639 sec/batch\n",
      "Epoch: 142/500...  Training Step: 31501...  Training loss: 1.0703...  Val loss: 1.5508...  0.3643 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31526...  Training loss: 1.0559...  Val loss: 1.5561...  0.3637 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31551...  Training loss: 1.0599...  Val loss: 1.5483...  0.3642 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31576...  Training loss: 1.0801...  Val loss: 1.5425...  0.3639 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31601...  Training loss: 1.0656...  Val loss: 1.5557...  0.3644 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31626...  Training loss: 1.0758...  Val loss: 1.5554...  0.3639 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31651...  Training loss: 1.0557...  Val loss: 1.5688...  0.3636 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31676...  Training loss: 1.0546...  Val loss: 1.5656...  0.3634 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31701...  Training loss: 1.0487...  Val loss: 1.5545...  0.3635 sec/batch\n",
      "Epoch: 143/500...  Training Step: 31726...  Training loss: 1.0429...  Val loss: 1.5520...  0.3638 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31751...  Training loss: 1.0808...  Val loss: 1.5612...  0.3634 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31776...  Training loss: 1.0418...  Val loss: 1.5508...  0.3642 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31801...  Training loss: 1.0391...  Val loss: 1.5491...  0.3638 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31826...  Training loss: 1.0784...  Val loss: 1.5516...  0.3636 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31851...  Training loss: 1.0672...  Val loss: 1.5510...  0.3644 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31876...  Training loss: 1.0588...  Val loss: 1.5673...  0.3641 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31901...  Training loss: 1.0427...  Val loss: 1.5805...  0.3638 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31926...  Training loss: 1.0578...  Val loss: 1.5651...  0.3636 sec/batch\n",
      "Epoch: 144/500...  Training Step: 31951...  Training loss: 1.0564...  Val loss: 1.5527...  0.3630 sec/batch\n",
      "Epoch: 145/500...  Training Step: 31976...  Training loss: 1.0749...  Val loss: 1.5438...  0.3633 sec/batch\n",
      "Epoch: 145/500...  Training Step: 32001...  Training loss: 1.0651...  Val loss: 1.5642...  0.3640 sec/batch\n",
      "Epoch: 145/500...  Training Step: 32026...  Training loss: 1.0751...  Val loss: 1.5595...  0.3634 sec/batch\n",
      "Epoch: 145/500...  Training Step: 32051...  Training loss: 1.0515...  Val loss: 1.5684...  0.3641 sec/batch\n",
      "Epoch: 145/500...  Training Step: 32076...  Training loss: 1.0515...  Val loss: 1.5691...  0.3643 sec/batch\n",
      "Epoch: 145/500...  Training Step: 32101...  Training loss: 1.0571...  Val loss: 1.5649...  0.3641 sec/batch\n",
      "Epoch: 145/500...  Training Step: 32126...  Training loss: 1.0808...  Val loss: 1.5725...  0.3638 sec/batch\n",
      "Epoch: 145/500...  Training Step: 32151...  Training loss: 1.0668...  Val loss: 1.5590...  0.3638 sec/batch\n",
      "Epoch: 145/500...  Training Step: 32176...  Training loss: 1.0648...  Val loss: 1.5570...  0.3642 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32201...  Training loss: 1.0440...  Val loss: 1.5654...  0.3641 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32226...  Training loss: 1.0492...  Val loss: 1.5693...  0.3642 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32251...  Training loss: 1.0711...  Val loss: 1.5610...  0.3635 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32276...  Training loss: 1.0586...  Val loss: 1.5586...  0.3637 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32301...  Training loss: 1.0771...  Val loss: 1.5645...  0.3640 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32326...  Training loss: 1.0521...  Val loss: 1.5641...  0.3639 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32351...  Training loss: 1.0773...  Val loss: 1.5783...  0.3646 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32376...  Training loss: 1.0697...  Val loss: 1.5657...  0.3631 sec/batch\n",
      "Epoch: 146/500...  Training Step: 32401...  Training loss: 1.0670...  Val loss: 1.5559...  0.3632 sec/batch\n",
      "Epoch: 147/500...  Training Step: 32426...  Training loss: 1.0450...  Val loss: 1.5595...  0.3640 sec/batch\n",
      "Epoch: 147/500...  Training Step: 32451...  Training loss: 1.0507...  Val loss: 1.5778...  0.3641 sec/batch\n",
      "Epoch: 147/500...  Training Step: 32476...  Training loss: 1.0689...  Val loss: 1.5522...  0.3641 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147/500...  Training Step: 32501...  Training loss: 1.0601...  Val loss: 1.5698...  0.3634 sec/batch\n",
      "Epoch: 147/500...  Training Step: 32526...  Training loss: 1.0418...  Val loss: 1.5723...  0.3647 sec/batch\n",
      "Epoch: 147/500...  Training Step: 32551...  Training loss: 1.0700...  Val loss: 1.5727...  0.3643 sec/batch\n",
      "Epoch: 147/500...  Training Step: 32576...  Training loss: 1.0560...  Val loss: 1.5686...  0.3633 sec/batch\n",
      "Epoch: 147/500...  Training Step: 32601...  Training loss: 1.0470...  Val loss: 1.5599...  0.3637 sec/batch\n",
      "Epoch: 147/500...  Training Step: 32626...  Training loss: 1.0544...  Val loss: 1.5664...  0.3644 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32651...  Training loss: 1.0697...  Val loss: 1.5526...  0.3639 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32676...  Training loss: 1.0609...  Val loss: 1.5656...  0.3639 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32701...  Training loss: 1.0482...  Val loss: 1.5670...  0.3641 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32726...  Training loss: 1.0541...  Val loss: 1.5577...  0.3638 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32751...  Training loss: 1.0605...  Val loss: 1.5760...  0.3639 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32776...  Training loss: 1.0549...  Val loss: 1.5683...  0.3642 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32801...  Training loss: 1.0477...  Val loss: 1.5731...  0.3640 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32826...  Training loss: 1.0699...  Val loss: 1.5780...  0.3639 sec/batch\n",
      "Epoch: 148/500...  Training Step: 32851...  Training loss: 1.0562...  Val loss: 1.5655...  0.3637 sec/batch\n",
      "Epoch: 149/500...  Training Step: 32876...  Training loss: 1.0470...  Val loss: 1.5436...  0.3637 sec/batch\n",
      "Epoch: 149/500...  Training Step: 32901...  Training loss: 1.0288...  Val loss: 1.5721...  0.3639 sec/batch\n",
      "Epoch: 149/500...  Training Step: 32926...  Training loss: 1.0628...  Val loss: 1.5644...  0.3648 sec/batch\n",
      "Epoch: 149/500...  Training Step: 32951...  Training loss: 1.0505...  Val loss: 1.5623...  0.3642 sec/batch\n",
      "Epoch: 149/500...  Training Step: 32976...  Training loss: 1.0593...  Val loss: 1.5800...  0.3639 sec/batch\n",
      "Epoch: 149/500...  Training Step: 33001...  Training loss: 1.0501...  Val loss: 1.5605...  0.3641 sec/batch\n",
      "Epoch: 149/500...  Training Step: 33026...  Training loss: 1.0383...  Val loss: 1.5554...  0.3637 sec/batch\n",
      "Epoch: 149/500...  Training Step: 33051...  Training loss: 1.0473...  Val loss: 1.5710...  0.3639 sec/batch\n",
      "Epoch: 149/500...  Training Step: 33076...  Training loss: 1.0440...  Val loss: 1.5741...  0.3638 sec/batch\n",
      "Epoch: 150/500...  Training Step: 33101...  Training loss: 1.0577...  Val loss: 1.5629...  0.3644 sec/batch\n",
      "Epoch: 150/500...  Training Step: 33126...  Training loss: 1.0431...  Val loss: 1.5799...  0.3636 sec/batch\n",
      "Epoch: 150/500...  Training Step: 33151...  Training loss: 1.0462...  Val loss: 1.5636...  0.3638 sec/batch\n",
      "Epoch: 150/500...  Training Step: 33176...  Training loss: 1.0397...  Val loss: 1.5651...  0.3639 sec/batch\n",
      "Epoch: 150/500...  Training Step: 33201...  Training loss: 1.0477...  Val loss: 1.5807...  0.3635 sec/batch\n",
      "Epoch: 150/500...  Training Step: 33226...  Training loss: 1.0616...  Val loss: 1.5720...  0.3637 sec/batch\n",
      "Epoch: 150/500...  Training Step: 33251...  Training loss: 1.0611...  Val loss: 1.5734...  0.3638 sec/batch\n",
      "Epoch: 150/500...  Training Step: 33276...  Training loss: 1.0497...  Val loss: 1.5831...  0.3638 sec/batch\n",
      "Epoch 150/500 time:81.59853410720825...  finished at 2017-10-30 13:02:53\n",
      "Epoch: 151/500...  Training Step: 33301...  Training loss: 1.1521...  Val loss: 1.5676...  0.3645 sec/batch\n",
      "Epoch: 151/500...  Training Step: 33326...  Training loss: 1.0644...  Val loss: 1.5604...  0.3639 sec/batch\n",
      "Epoch: 151/500...  Training Step: 33351...  Training loss: 1.0353...  Val loss: 1.5847...  0.3641 sec/batch\n",
      "Epoch: 151/500...  Training Step: 33376...  Training loss: 1.0454...  Val loss: 1.5754...  0.3643 sec/batch\n",
      "Epoch: 151/500...  Training Step: 33401...  Training loss: 1.0578...  Val loss: 1.5779...  0.3638 sec/batch\n",
      "Epoch: 151/500...  Training Step: 33426...  Training loss: 1.0469...  Val loss: 1.5794...  0.3634 sec/batch\n",
      "Epoch: 151/500...  Training Step: 33451...  Training loss: 1.0408...  Val loss: 1.5799...  0.3636 sec/batch\n",
      "Epoch: 151/500...  Training Step: 33476...  Training loss: 1.0348...  Val loss: 1.5802...  0.3632 sec/batch\n",
      "Epoch: 151/500...  Training Step: 33501...  Training loss: 1.0470...  Val loss: 1.5846...  0.3640 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33526...  Training loss: 1.0647...  Val loss: 1.5772...  0.3643 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33551...  Training loss: 1.0310...  Val loss: 1.5652...  0.3637 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33576...  Training loss: 1.0323...  Val loss: 1.5690...  0.3641 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33601...  Training loss: 1.0267...  Val loss: 1.5795...  0.3632 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33626...  Training loss: 1.0316...  Val loss: 1.5789...  0.3636 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33651...  Training loss: 1.0391...  Val loss: 1.5773...  0.3640 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33676...  Training loss: 1.0391...  Val loss: 1.6006...  0.3645 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33701...  Training loss: 1.0403...  Val loss: 1.5828...  0.3643 sec/batch\n",
      "Epoch: 152/500...  Training Step: 33726...  Training loss: 1.0371...  Val loss: 1.5903...  0.3635 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33751...  Training loss: 1.0601...  Val loss: 1.5878...  0.3637 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33776...  Training loss: 1.0482...  Val loss: 1.5755...  0.3641 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33801...  Training loss: 1.0333...  Val loss: 1.5768...  0.3638 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33826...  Training loss: 1.0330...  Val loss: 1.5716...  0.3635 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33851...  Training loss: 1.0275...  Val loss: 1.5804...  0.3638 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33876...  Training loss: 1.0563...  Val loss: 1.5823...  0.3637 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33901...  Training loss: 1.0558...  Val loss: 1.5967...  0.3641 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33926...  Training loss: 1.0460...  Val loss: 1.5742...  0.3638 sec/batch\n",
      "Epoch: 153/500...  Training Step: 33951...  Training loss: 1.0525...  Val loss: 1.5856...  0.3638 sec/batch\n",
      "Epoch: 154/500...  Training Step: 33976...  Training loss: 1.0196...  Val loss: 1.5805...  0.3641 sec/batch\n",
      "Epoch: 154/500...  Training Step: 34001...  Training loss: 1.0488...  Val loss: 1.5891...  0.3636 sec/batch\n",
      "Epoch: 154/500...  Training Step: 34026...  Training loss: 1.0444...  Val loss: 1.5716...  0.3642 sec/batch\n",
      "Epoch: 154/500...  Training Step: 34051...  Training loss: 1.0699...  Val loss: 1.5778...  0.3637 sec/batch\n",
      "Epoch: 154/500...  Training Step: 34076...  Training loss: 1.0606...  Val loss: 1.5792...  0.3664 sec/batch\n",
      "Epoch: 154/500...  Training Step: 34101...  Training loss: 1.0338...  Val loss: 1.5947...  0.3638 sec/batch\n",
      "Epoch: 154/500...  Training Step: 34126...  Training loss: 1.0492...  Val loss: 1.5873...  0.3638 sec/batch\n",
      "Epoch: 154/500...  Training Step: 34151...  Training loss: 1.0288...  Val loss: 1.5864...  0.3637 sec/batch\n",
      "Epoch: 154/500...  Training Step: 34176...  Training loss: 1.0333...  Val loss: 1.5739...  0.3637 sec/batch\n",
      "Epoch: 155/500...  Training Step: 34201...  Training loss: 1.0457...  Val loss: 1.5825...  0.3634 sec/batch\n",
      "Epoch: 155/500...  Training Step: 34226...  Training loss: 1.0335...  Val loss: 1.5887...  0.3642 sec/batch\n",
      "Epoch: 155/500...  Training Step: 34251...  Training loss: 1.0475...  Val loss: 1.5824...  0.3638 sec/batch\n",
      "Epoch: 155/500...  Training Step: 34276...  Training loss: 1.0526...  Val loss: 1.5893...  0.3641 sec/batch\n",
      "Epoch: 155/500...  Training Step: 34301...  Training loss: 1.0329...  Val loss: 1.5904...  0.3635 sec/batch\n",
      "Epoch: 155/500...  Training Step: 34326...  Training loss: 1.0426...  Val loss: 1.5883...  0.3639 sec/batch\n",
      "Epoch: 155/500...  Training Step: 34351...  Training loss: 1.0467...  Val loss: 1.5862...  0.3637 sec/batch\n",
      "Epoch: 155/500...  Training Step: 34376...  Training loss: 1.0332...  Val loss: 1.5831...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155/500...  Training Step: 34401...  Training loss: 1.0721...  Val loss: 1.5732...  0.3640 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34426...  Training loss: 1.0262...  Val loss: 1.6017...  0.3640 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34451...  Training loss: 1.0140...  Val loss: 1.5905...  0.3640 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34476...  Training loss: 1.0450...  Val loss: 1.5911...  0.3632 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34501...  Training loss: 1.0324...  Val loss: 1.5848...  0.3648 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34526...  Training loss: 1.0325...  Val loss: 1.5817...  0.3637 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34551...  Training loss: 1.0179...  Val loss: 1.5913...  0.3637 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34576...  Training loss: 1.0415...  Val loss: 1.5792...  0.3645 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34601...  Training loss: 1.0355...  Val loss: 1.5918...  0.3635 sec/batch\n",
      "Epoch: 156/500...  Training Step: 34626...  Training loss: 1.0494...  Val loss: 1.5753...  0.3635 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34651...  Training loss: 1.0470...  Val loss: 1.5689...  0.3638 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34676...  Training loss: 1.0338...  Val loss: 1.5932...  0.3637 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34701...  Training loss: 1.0376...  Val loss: 1.5859...  0.3640 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34726...  Training loss: 1.0379...  Val loss: 1.5826...  0.3634 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34751...  Training loss: 1.0479...  Val loss: 1.5937...  0.3640 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34776...  Training loss: 1.0488...  Val loss: 1.5924...  0.3634 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34801...  Training loss: 1.0398...  Val loss: 1.5972...  0.3639 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34826...  Training loss: 1.0393...  Val loss: 1.5991...  0.3637 sec/batch\n",
      "Epoch: 157/500...  Training Step: 34851...  Training loss: 1.0561...  Val loss: 1.5944...  0.3640 sec/batch\n",
      "Epoch: 158/500...  Training Step: 34876...  Training loss: 1.0231...  Val loss: 1.5788...  0.3640 sec/batch\n",
      "Epoch: 158/500...  Training Step: 34901...  Training loss: 1.0245...  Val loss: 1.6077...  0.3634 sec/batch\n",
      "Epoch: 158/500...  Training Step: 34926...  Training loss: 1.0312...  Val loss: 1.6017...  0.3636 sec/batch\n",
      "Epoch: 158/500...  Training Step: 34951...  Training loss: 1.0181...  Val loss: 1.5850...  0.3631 sec/batch\n",
      "Epoch: 158/500...  Training Step: 34976...  Training loss: 1.0483...  Val loss: 1.5866...  0.3637 sec/batch\n",
      "Epoch: 158/500...  Training Step: 35001...  Training loss: 1.0497...  Val loss: 1.5755...  0.3638 sec/batch\n",
      "Epoch: 158/500...  Training Step: 35026...  Training loss: 1.0420...  Val loss: 1.5950...  0.3638 sec/batch\n",
      "Epoch: 158/500...  Training Step: 35051...  Training loss: 1.0350...  Val loss: 1.5992...  0.3635 sec/batch\n",
      "Epoch: 158/500...  Training Step: 35076...  Training loss: 1.0306...  Val loss: 1.5909...  0.3632 sec/batch\n",
      "Epoch: 159/500...  Training Step: 35101...  Training loss: 1.0174...  Val loss: 1.5796...  0.3634 sec/batch\n",
      "Epoch: 159/500...  Training Step: 35126...  Training loss: 1.0526...  Val loss: 1.6142...  0.3643 sec/batch\n",
      "Epoch: 159/500...  Training Step: 35151...  Training loss: 1.0520...  Val loss: 1.5942...  0.3643 sec/batch\n",
      "Epoch: 159/500...  Training Step: 35176...  Training loss: 1.0415...  Val loss: 1.5903...  0.3642 sec/batch\n",
      "Epoch: 159/500...  Training Step: 35201...  Training loss: 1.0176...  Val loss: 1.6011...  0.3637 sec/batch\n",
      "Epoch: 159/500...  Training Step: 35226...  Training loss: 1.0385...  Val loss: 1.5999...  0.3639 sec/batch\n",
      "Epoch: 159/500...  Training Step: 35251...  Training loss: 1.0288...  Val loss: 1.5861...  0.3639 sec/batch\n",
      "Epoch: 159/500...  Training Step: 35276...  Training loss: 1.0362...  Val loss: 1.6057...  0.3641 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35301...  Training loss: 1.0455...  Val loss: 1.5937...  0.3638 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35326...  Training loss: 1.0256...  Val loss: 1.5790...  0.3639 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35351...  Training loss: 0.9977...  Val loss: 1.5995...  0.3638 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35376...  Training loss: 1.0192...  Val loss: 1.5998...  0.3648 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35401...  Training loss: 1.0124...  Val loss: 1.5913...  0.3638 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35426...  Training loss: 1.0194...  Val loss: 1.6016...  0.3644 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35451...  Training loss: 1.0224...  Val loss: 1.5904...  0.3638 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35476...  Training loss: 1.0225...  Val loss: 1.6007...  0.3635 sec/batch\n",
      "Epoch: 160/500...  Training Step: 35501...  Training loss: 1.0403...  Val loss: 1.6070...  0.3641 sec/batch\n",
      "Epoch 160/500 time:82.06166195869446...  finished at 2017-10-30 13:16:32\n",
      "Epoch: 161/500...  Training Step: 35526...  Training loss: 1.0454...  Val loss: 1.5906...  0.3644 sec/batch\n",
      "Epoch: 161/500...  Training Step: 35551...  Training loss: 0.9984...  Val loss: 1.5910...  0.3633 sec/batch\n",
      "Epoch: 161/500...  Training Step: 35576...  Training loss: 1.0356...  Val loss: 1.5990...  0.3641 sec/batch\n",
      "Epoch: 161/500...  Training Step: 35601...  Training loss: 1.0131...  Val loss: 1.5990...  0.3639 sec/batch\n",
      "Epoch: 161/500...  Training Step: 35626...  Training loss: 1.0128...  Val loss: 1.6014...  0.3639 sec/batch\n",
      "Epoch: 161/500...  Training Step: 35651...  Training loss: 1.0274...  Val loss: 1.5899...  0.3638 sec/batch\n",
      "Epoch: 161/500...  Training Step: 35676...  Training loss: 1.0212...  Val loss: 1.6057...  0.3643 sec/batch\n",
      "Epoch: 161/500...  Training Step: 35701...  Training loss: 1.0126...  Val loss: 1.6069...  0.3643 sec/batch\n",
      "Epoch: 161/500...  Training Step: 35726...  Training loss: 1.0210...  Val loss: 1.6197...  0.3639 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35751...  Training loss: 1.0326...  Val loss: 1.6073...  0.3644 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35776...  Training loss: 1.0210...  Val loss: 1.5957...  0.3660 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35801...  Training loss: 1.0284...  Val loss: 1.6176...  0.3632 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35826...  Training loss: 1.0413...  Val loss: 1.6056...  0.3636 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35851...  Training loss: 1.0254...  Val loss: 1.6044...  0.3639 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35876...  Training loss: 1.0160...  Val loss: 1.5980...  0.3641 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35901...  Training loss: 1.0284...  Val loss: 1.6223...  0.3638 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35926...  Training loss: 1.0384...  Val loss: 1.6104...  0.3639 sec/batch\n",
      "Epoch: 162/500...  Training Step: 35951...  Training loss: 1.0175...  Val loss: 1.6104...  0.3638 sec/batch\n",
      "Epoch: 163/500...  Training Step: 35976...  Training loss: 1.0296...  Val loss: 1.6132...  0.3643 sec/batch\n",
      "Epoch: 163/500...  Training Step: 36001...  Training loss: 1.0287...  Val loss: 1.5939...  0.3639 sec/batch\n",
      "Epoch: 163/500...  Training Step: 36026...  Training loss: 1.0404...  Val loss: 1.5914...  0.3635 sec/batch\n",
      "Epoch: 163/500...  Training Step: 36051...  Training loss: 1.0391...  Val loss: 1.6053...  0.3641 sec/batch\n",
      "Epoch: 163/500...  Training Step: 36076...  Training loss: 1.0089...  Val loss: 1.6085...  0.3639 sec/batch\n",
      "Epoch: 163/500...  Training Step: 36101...  Training loss: 1.0099...  Val loss: 1.6147...  0.3636 sec/batch\n",
      "Epoch: 163/500...  Training Step: 36126...  Training loss: 1.0254...  Val loss: 1.6193...  0.3634 sec/batch\n",
      "Epoch: 163/500...  Training Step: 36151...  Training loss: 1.0151...  Val loss: 1.6025...  0.3640 sec/batch\n",
      "Epoch: 163/500...  Training Step: 36176...  Training loss: 1.0177...  Val loss: 1.6032...  0.3638 sec/batch\n",
      "Epoch: 164/500...  Training Step: 36201...  Training loss: 1.0322...  Val loss: 1.6011...  0.3636 sec/batch\n",
      "Epoch: 164/500...  Training Step: 36226...  Training loss: 1.0088...  Val loss: 1.6040...  0.3641 sec/batch\n",
      "Epoch: 164/500...  Training Step: 36251...  Training loss: 1.0393...  Val loss: 1.6039...  0.3640 sec/batch\n",
      "Epoch: 164/500...  Training Step: 36276...  Training loss: 0.9999...  Val loss: 1.6245...  0.3644 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164/500...  Training Step: 36301...  Training loss: 1.0258...  Val loss: 1.6093...  0.3639 sec/batch\n",
      "Epoch: 164/500...  Training Step: 36326...  Training loss: 1.0150...  Val loss: 1.6049...  0.3639 sec/batch\n",
      "Epoch: 164/500...  Training Step: 36351...  Training loss: 1.0145...  Val loss: 1.5990...  0.3640 sec/batch\n",
      "Epoch: 164/500...  Training Step: 36376...  Training loss: 1.0362...  Val loss: 1.6021...  0.3636 sec/batch\n",
      "Epoch: 164/500...  Training Step: 36401...  Training loss: 1.0368...  Val loss: 1.6072...  0.3630 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36426...  Training loss: 1.0178...  Val loss: 1.5967...  0.3642 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36451...  Training loss: 1.0017...  Val loss: 1.6166...  0.3638 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36476...  Training loss: 1.0426...  Val loss: 1.6060...  0.3641 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36501...  Training loss: 1.0090...  Val loss: 1.6008...  0.3643 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36526...  Training loss: 1.0176...  Val loss: 1.6094...  0.3640 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36551...  Training loss: 1.0230...  Val loss: 1.6164...  0.3645 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36576...  Training loss: 1.0190...  Val loss: 1.6156...  0.3639 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36601...  Training loss: 1.0043...  Val loss: 1.6168...  0.3642 sec/batch\n",
      "Epoch: 165/500...  Training Step: 36626...  Training loss: 1.0331...  Val loss: 1.5943...  0.3660 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36651...  Training loss: 1.0290...  Val loss: 1.6095...  0.3640 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36676...  Training loss: 1.0102...  Val loss: 1.6201...  0.3639 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36701...  Training loss: 1.0351...  Val loss: 1.6114...  0.3644 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36726...  Training loss: 1.0148...  Val loss: 1.6011...  0.3634 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36751...  Training loss: 1.0271...  Val loss: 1.6007...  0.3637 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36776...  Training loss: 1.0352...  Val loss: 1.6249...  0.3640 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36801...  Training loss: 1.0482...  Val loss: 1.6179...  0.3635 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36826...  Training loss: 1.0218...  Val loss: 1.5958...  0.3638 sec/batch\n",
      "Epoch: 166/500...  Training Step: 36851...  Training loss: 1.0299...  Val loss: 1.6177...  0.3639 sec/batch\n",
      "Epoch: 167/500...  Training Step: 36876...  Training loss: 1.0272...  Val loss: 1.6008...  0.3640 sec/batch\n",
      "Epoch: 167/500...  Training Step: 36901...  Training loss: 1.0153...  Val loss: 1.6355...  0.3637 sec/batch\n",
      "Epoch: 167/500...  Training Step: 36926...  Training loss: 1.0086...  Val loss: 1.6234...  0.3641 sec/batch\n",
      "Epoch: 167/500...  Training Step: 36951...  Training loss: 1.0095...  Val loss: 1.6075...  0.3641 sec/batch\n",
      "Epoch: 167/500...  Training Step: 36976...  Training loss: 1.0035...  Val loss: 1.6133...  0.3633 sec/batch\n",
      "Epoch: 167/500...  Training Step: 37001...  Training loss: 1.0226...  Val loss: 1.6086...  0.3638 sec/batch\n",
      "Epoch: 167/500...  Training Step: 37026...  Training loss: 1.0191...  Val loss: 1.6050...  0.3646 sec/batch\n",
      "Epoch: 167/500...  Training Step: 37051...  Training loss: 1.0182...  Val loss: 1.6172...  0.3628 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37076...  Training loss: 1.0053...  Val loss: 1.6088...  0.3641 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37101...  Training loss: 1.0125...  Val loss: 1.5983...  0.3640 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37126...  Training loss: 1.0172...  Val loss: 1.6218...  0.3639 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37151...  Training loss: 1.0071...  Val loss: 1.6230...  0.3639 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37176...  Training loss: 1.0205...  Val loss: 1.6035...  0.3644 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37201...  Training loss: 1.0078...  Val loss: 1.6213...  0.3638 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37226...  Training loss: 1.0027...  Val loss: 1.6190...  0.3642 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37251...  Training loss: 0.9975...  Val loss: 1.6129...  0.3635 sec/batch\n",
      "Epoch: 168/500...  Training Step: 37276...  Training loss: 0.9829...  Val loss: 1.6195...  0.3633 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37301...  Training loss: 1.0126...  Val loss: 1.6129...  0.3637 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37326...  Training loss: 0.9956...  Val loss: 1.6071...  0.3627 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37351...  Training loss: 0.9830...  Val loss: 1.6322...  0.3637 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37376...  Training loss: 1.0201...  Val loss: 1.6094...  0.3640 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37401...  Training loss: 1.0084...  Val loss: 1.5983...  0.3638 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37426...  Training loss: 1.0009...  Val loss: 1.6197...  0.3633 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37451...  Training loss: 0.9940...  Val loss: 1.6215...  0.3633 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37476...  Training loss: 1.0060...  Val loss: 1.6166...  0.3639 sec/batch\n",
      "Epoch: 169/500...  Training Step: 37501...  Training loss: 1.0000...  Val loss: 1.6252...  0.3638 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37526...  Training loss: 1.0278...  Val loss: 1.5995...  0.3635 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37551...  Training loss: 1.0158...  Val loss: 1.6135...  0.3637 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37576...  Training loss: 1.0195...  Val loss: 1.6236...  0.3638 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37601...  Training loss: 0.9969...  Val loss: 1.6285...  0.3640 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37626...  Training loss: 0.9974...  Val loss: 1.6276...  0.3639 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37651...  Training loss: 1.0041...  Val loss: 1.6188...  0.3638 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37676...  Training loss: 1.0165...  Val loss: 1.6279...  0.3639 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37701...  Training loss: 1.0103...  Val loss: 1.6105...  0.3641 sec/batch\n",
      "Epoch: 170/500...  Training Step: 37726...  Training loss: 1.0077...  Val loss: 1.6322...  0.3638 sec/batch\n",
      "Epoch 170/500 time:81.69383502006531...  finished at 2017-10-30 13:30:10\n",
      "Epoch: 171/500...  Training Step: 37751...  Training loss: 0.9998...  Val loss: 1.6277...  0.3636 sec/batch\n",
      "Epoch: 171/500...  Training Step: 37776...  Training loss: 1.0047...  Val loss: 1.6121...  0.3640 sec/batch\n",
      "Epoch: 171/500...  Training Step: 37801...  Training loss: 1.0216...  Val loss: 1.6087...  0.3642 sec/batch\n",
      "Epoch: 171/500...  Training Step: 37826...  Training loss: 1.0131...  Val loss: 1.6147...  0.3640 sec/batch\n",
      "Epoch: 171/500...  Training Step: 37851...  Training loss: 1.0273...  Val loss: 1.6255...  0.3636 sec/batch\n",
      "Epoch: 171/500...  Training Step: 37876...  Training loss: 1.0016...  Val loss: 1.6213...  0.3640 sec/batch\n",
      "Epoch: 171/500...  Training Step: 37901...  Training loss: 1.0163...  Val loss: 1.6255...  0.3640 sec/batch\n",
      "Epoch: 171/500...  Training Step: 37926...  Training loss: 1.0203...  Val loss: 1.6243...  0.3636 sec/batch\n",
      "Epoch: 171/500...  Training Step: 37951...  Training loss: 1.0179...  Val loss: 1.6310...  0.3639 sec/batch\n",
      "Epoch: 172/500...  Training Step: 37976...  Training loss: 0.9947...  Val loss: 1.6291...  0.3637 sec/batch\n",
      "Epoch: 172/500...  Training Step: 38001...  Training loss: 1.0023...  Val loss: 1.6223...  0.3639 sec/batch\n",
      "Epoch: 172/500...  Training Step: 38026...  Training loss: 1.0099...  Val loss: 1.6039...  0.3638 sec/batch\n",
      "Epoch: 172/500...  Training Step: 38051...  Training loss: 1.0093...  Val loss: 1.6276...  0.3633 sec/batch\n",
      "Epoch: 172/500...  Training Step: 38076...  Training loss: 1.0036...  Val loss: 1.6422...  0.3644 sec/batch\n",
      "Epoch: 172/500...  Training Step: 38101...  Training loss: 1.0241...  Val loss: 1.6376...  0.3641 sec/batch\n",
      "Epoch: 172/500...  Training Step: 38126...  Training loss: 1.0070...  Val loss: 1.6399...  0.3638 sec/batch\n",
      "Epoch: 172/500...  Training Step: 38151...  Training loss: 1.0021...  Val loss: 1.6211...  0.3636 sec/batch\n",
      "Epoch: 172/500...  Training Step: 38176...  Training loss: 1.0141...  Val loss: 1.6305...  0.3626 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173/500...  Training Step: 38201...  Training loss: 1.0139...  Val loss: 1.6267...  0.3628 sec/batch\n",
      "Epoch: 173/500...  Training Step: 38226...  Training loss: 1.0101...  Val loss: 1.6167...  0.3638 sec/batch\n",
      "Epoch: 173/500...  Training Step: 38251...  Training loss: 0.9978...  Val loss: 1.6261...  0.3640 sec/batch\n",
      "Epoch: 173/500...  Training Step: 38276...  Training loss: 1.0009...  Val loss: 1.6213...  0.3643 sec/batch\n",
      "Epoch: 173/500...  Training Step: 38301...  Training loss: 1.0058...  Val loss: 1.6459...  0.3639 sec/batch\n",
      "Epoch: 173/500...  Training Step: 38326...  Training loss: 1.0006...  Val loss: 1.6301...  0.3638 sec/batch\n",
      "Epoch: 173/500...  Training Step: 38351...  Training loss: 0.9937...  Val loss: 1.6295...  0.3646 sec/batch\n",
      "Epoch: 173/500...  Training Step: 38376...  Training loss: 1.0124...  Val loss: 1.6347...  0.3637 sec/batch\n",
      "Epoch: 173/500...  Training Step: 38401...  Training loss: 1.0017...  Val loss: 1.6171...  0.3644 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38426...  Training loss: 0.9891...  Val loss: 1.6123...  0.3633 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38451...  Training loss: 0.9853...  Val loss: 1.6407...  0.3637 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38476...  Training loss: 1.0115...  Val loss: 1.6192...  0.3636 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38501...  Training loss: 1.0037...  Val loss: 1.6285...  0.3643 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38526...  Training loss: 1.0025...  Val loss: 1.6304...  0.3639 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38551...  Training loss: 1.0041...  Val loss: 1.6273...  0.3640 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38576...  Training loss: 0.9946...  Val loss: 1.6147...  0.3640 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38601...  Training loss: 0.9937...  Val loss: 1.6282...  0.3636 sec/batch\n",
      "Epoch: 174/500...  Training Step: 38626...  Training loss: 0.9861...  Val loss: 1.6276...  0.3638 sec/batch\n",
      "Epoch: 175/500...  Training Step: 38651...  Training loss: 1.0148...  Val loss: 1.6213...  0.3641 sec/batch\n",
      "Epoch: 175/500...  Training Step: 38676...  Training loss: 0.9847...  Val loss: 1.6400...  0.3638 sec/batch\n",
      "Epoch: 175/500...  Training Step: 38701...  Training loss: 0.9915...  Val loss: 1.6294...  0.3637 sec/batch\n",
      "Epoch: 175/500...  Training Step: 38726...  Training loss: 0.9979...  Val loss: 1.6271...  0.3639 sec/batch\n",
      "Epoch: 175/500...  Training Step: 38751...  Training loss: 0.9989...  Val loss: 1.6365...  0.3632 sec/batch\n",
      "Epoch: 175/500...  Training Step: 38776...  Training loss: 1.0063...  Val loss: 1.6396...  0.3636 sec/batch\n",
      "Epoch: 175/500...  Training Step: 38801...  Training loss: 1.0077...  Val loss: 1.6394...  0.3638 sec/batch\n",
      "Epoch: 175/500...  Training Step: 38826...  Training loss: 0.9954...  Val loss: 1.6247...  0.3644 sec/batch\n",
      "Epoch: 176/500...  Training Step: 38851...  Training loss: 1.1072...  Val loss: 1.6114...  0.3645 sec/batch\n",
      "Epoch: 176/500...  Training Step: 38876...  Training loss: 1.0178...  Val loss: 1.6235...  0.3640 sec/batch\n",
      "Epoch: 176/500...  Training Step: 38901...  Training loss: 0.9827...  Val loss: 1.6598...  0.3637 sec/batch\n",
      "Epoch: 176/500...  Training Step: 38926...  Training loss: 0.9947...  Val loss: 1.6426...  0.3632 sec/batch\n",
      "Epoch: 176/500...  Training Step: 38951...  Training loss: 1.0072...  Val loss: 1.6246...  0.3643 sec/batch\n",
      "Epoch: 176/500...  Training Step: 38976...  Training loss: 0.9883...  Val loss: 1.6307...  0.3635 sec/batch\n",
      "Epoch: 176/500...  Training Step: 39001...  Training loss: 0.9905...  Val loss: 1.6422...  0.3645 sec/batch\n",
      "Epoch: 176/500...  Training Step: 39026...  Training loss: 0.9771...  Val loss: 1.6341...  0.3641 sec/batch\n",
      "Epoch: 176/500...  Training Step: 39051...  Training loss: 1.0005...  Val loss: 1.6450...  0.3635 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39076...  Training loss: 1.0068...  Val loss: 1.6314...  0.3635 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39101...  Training loss: 0.9979...  Val loss: 1.6185...  0.3628 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39126...  Training loss: 0.9800...  Val loss: 1.6507...  0.3640 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39151...  Training loss: 0.9685...  Val loss: 1.6517...  0.3634 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39176...  Training loss: 0.9820...  Val loss: 1.6241...  0.3644 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39201...  Training loss: 0.9938...  Val loss: 1.6320...  0.3638 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39226...  Training loss: 0.9899...  Val loss: 1.6505...  0.3634 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39251...  Training loss: 0.9922...  Val loss: 1.6480...  0.3632 sec/batch\n",
      "Epoch: 177/500...  Training Step: 39276...  Training loss: 0.9917...  Val loss: 1.6482...  0.3634 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39301...  Training loss: 1.0043...  Val loss: 1.6416...  0.3634 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39326...  Training loss: 1.0084...  Val loss: 1.6328...  0.3631 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39351...  Training loss: 0.9806...  Val loss: 1.6539...  0.3638 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39376...  Training loss: 0.9912...  Val loss: 1.6339...  0.3633 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39401...  Training loss: 0.9750...  Val loss: 1.6312...  0.3637 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39426...  Training loss: 1.0047...  Val loss: 1.6390...  0.3636 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39451...  Training loss: 1.0001...  Val loss: 1.6511...  0.3635 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39476...  Training loss: 0.9940...  Val loss: 1.6431...  0.3638 sec/batch\n",
      "Epoch: 178/500...  Training Step: 39501...  Training loss: 0.9990...  Val loss: 1.6496...  0.3635 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39526...  Training loss: 0.9736...  Val loss: 1.6390...  0.3624 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39551...  Training loss: 1.0058...  Val loss: 1.6316...  0.3618 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39576...  Training loss: 0.9945...  Val loss: 1.6343...  0.3629 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39601...  Training loss: 1.0072...  Val loss: 1.6279...  0.3635 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39626...  Training loss: 1.0060...  Val loss: 1.6370...  0.3636 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39651...  Training loss: 0.9930...  Val loss: 1.6523...  0.3634 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39676...  Training loss: 0.9904...  Val loss: 1.6419...  0.3632 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39701...  Training loss: 0.9823...  Val loss: 1.6534...  0.3629 sec/batch\n",
      "Epoch: 179/500...  Training Step: 39726...  Training loss: 0.9730...  Val loss: 1.6407...  0.3631 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39751...  Training loss: 0.9944...  Val loss: 1.6487...  0.3637 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39776...  Training loss: 0.9874...  Val loss: 1.6359...  0.3634 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39801...  Training loss: 1.0050...  Val loss: 1.6268...  0.3635 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39826...  Training loss: 0.9918...  Val loss: 1.6450...  0.3634 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39851...  Training loss: 0.9715...  Val loss: 1.6352...  0.3635 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39876...  Training loss: 0.9812...  Val loss: 1.6399...  0.3630 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39901...  Training loss: 0.9965...  Val loss: 1.6468...  0.3633 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39926...  Training loss: 0.9864...  Val loss: 1.6422...  0.3631 sec/batch\n",
      "Epoch: 180/500...  Training Step: 39951...  Training loss: 1.0192...  Val loss: 1.6575...  0.3633 sec/batch\n",
      "Epoch 180/500 time:81.574547290802...  finished at 2017-10-30 13:43:48\n",
      "Epoch: 181/500...  Training Step: 39976...  Training loss: 0.9701...  Val loss: 1.6665...  0.3627 sec/batch\n",
      "Epoch: 181/500...  Training Step: 40001...  Training loss: 0.9715...  Val loss: 1.6373...  0.3635 sec/batch\n",
      "Epoch: 181/500...  Training Step: 40026...  Training loss: 0.9807...  Val loss: 1.6335...  0.3634 sec/batch\n",
      "Epoch: 181/500...  Training Step: 40051...  Training loss: 0.9848...  Val loss: 1.6471...  0.3635 sec/batch\n",
      "Epoch: 181/500...  Training Step: 40076...  Training loss: 0.9709...  Val loss: 1.6570...  0.3635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181/500...  Training Step: 40101...  Training loss: 0.9787...  Val loss: 1.6495...  0.3634 sec/batch\n",
      "Epoch: 181/500...  Training Step: 40126...  Training loss: 0.9930...  Val loss: 1.6432...  0.3629 sec/batch\n",
      "Epoch: 181/500...  Training Step: 40151...  Training loss: 0.9858...  Val loss: 1.6445...  0.3637 sec/batch\n",
      "Epoch: 181/500...  Training Step: 40176...  Training loss: 1.0019...  Val loss: 1.6478...  0.3633 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40201...  Training loss: 1.0060...  Val loss: 1.6421...  0.3633 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40226...  Training loss: 0.9907...  Val loss: 1.6463...  0.3635 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40251...  Training loss: 0.9874...  Val loss: 1.6384...  0.3635 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40276...  Training loss: 0.9795...  Val loss: 1.6499...  0.3636 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40301...  Training loss: 0.9894...  Val loss: 1.6623...  0.3633 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40326...  Training loss: 1.0042...  Val loss: 1.6635...  0.3634 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40351...  Training loss: 0.9827...  Val loss: 1.6723...  0.3629 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40376...  Training loss: 0.9901...  Val loss: 1.6390...  0.3633 sec/batch\n",
      "Epoch: 182/500...  Training Step: 40401...  Training loss: 1.0060...  Val loss: 1.6468...  0.3636 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40426...  Training loss: 0.9798...  Val loss: 1.6421...  0.3636 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40451...  Training loss: 0.9784...  Val loss: 1.6562...  0.3617 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40476...  Training loss: 0.9680...  Val loss: 1.6557...  0.3634 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40501...  Training loss: 0.9769...  Val loss: 1.6498...  0.3630 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40526...  Training loss: 0.9932...  Val loss: 1.6476...  0.3635 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40551...  Training loss: 0.9936...  Val loss: 1.6425...  0.3621 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40576...  Training loss: 0.9905...  Val loss: 1.6549...  0.3628 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40601...  Training loss: 0.9858...  Val loss: 1.6454...  0.3633 sec/batch\n",
      "Epoch: 183/500...  Training Step: 40626...  Training loss: 0.9864...  Val loss: 1.6448...  0.3633 sec/batch\n",
      "Epoch: 184/500...  Training Step: 40651...  Training loss: 0.9688...  Val loss: 1.6382...  0.3631 sec/batch\n",
      "Epoch: 184/500...  Training Step: 40676...  Training loss: 0.9995...  Val loss: 1.6777...  0.3620 sec/batch\n",
      "Epoch: 184/500...  Training Step: 40701...  Training loss: 1.0078...  Val loss: 1.6553...  0.3633 sec/batch\n",
      "Epoch: 184/500...  Training Step: 40726...  Training loss: 0.9887...  Val loss: 1.6515...  0.3636 sec/batch\n",
      "Epoch: 184/500...  Training Step: 40751...  Training loss: 0.9713...  Val loss: 1.6607...  0.3634 sec/batch\n",
      "Epoch: 184/500...  Training Step: 40776...  Training loss: 0.9922...  Val loss: 1.6669...  0.3636 sec/batch\n",
      "Epoch: 184/500...  Training Step: 40801...  Training loss: 0.9824...  Val loss: 1.6437...  0.3636 sec/batch\n",
      "Epoch: 184/500...  Training Step: 40826...  Training loss: 0.9853...  Val loss: 1.6518...  0.3631 sec/batch\n",
      "Epoch: 185/500...  Training Step: 40851...  Training loss: 0.9986...  Val loss: 1.6437...  0.3635 sec/batch\n",
      "Epoch: 185/500...  Training Step: 40876...  Training loss: 0.9761...  Val loss: 1.6394...  0.3630 sec/batch\n",
      "Epoch: 185/500...  Training Step: 40901...  Training loss: 0.9594...  Val loss: 1.6625...  0.3641 sec/batch\n",
      "Epoch: 185/500...  Training Step: 40926...  Training loss: 0.9718...  Val loss: 1.6646...  0.3633 sec/batch\n",
      "Epoch: 185/500...  Training Step: 40951...  Training loss: 0.9626...  Val loss: 1.6438...  0.3631 sec/batch\n",
      "Epoch: 185/500...  Training Step: 40976...  Training loss: 0.9685...  Val loss: 1.6619...  0.3629 sec/batch\n",
      "Epoch: 185/500...  Training Step: 41001...  Training loss: 0.9670...  Val loss: 1.6552...  0.3638 sec/batch\n",
      "Epoch: 185/500...  Training Step: 41026...  Training loss: 0.9697...  Val loss: 1.6682...  0.3636 sec/batch\n",
      "Epoch: 185/500...  Training Step: 41051...  Training loss: 0.9929...  Val loss: 1.6688...  0.3634 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41076...  Training loss: 1.0036...  Val loss: 1.6320...  0.3630 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41101...  Training loss: 0.9701...  Val loss: 1.6428...  0.3634 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41126...  Training loss: 0.9843...  Val loss: 1.6736...  0.3633 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41151...  Training loss: 0.9610...  Val loss: 1.6631...  0.3631 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41176...  Training loss: 0.9758...  Val loss: 1.6529...  0.3637 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41201...  Training loss: 0.9696...  Val loss: 1.6403...  0.3635 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41226...  Training loss: 0.9912...  Val loss: 1.6671...  0.3634 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41251...  Training loss: 0.9603...  Val loss: 1.6707...  0.3633 sec/batch\n",
      "Epoch: 186/500...  Training Step: 41276...  Training loss: 0.9712...  Val loss: 1.6841...  0.3629 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41301...  Training loss: 0.9778...  Val loss: 1.6572...  0.3636 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41326...  Training loss: 0.9801...  Val loss: 1.6552...  0.3630 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41351...  Training loss: 0.9863...  Val loss: 1.6795...  0.3633 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41376...  Training loss: 0.9889...  Val loss: 1.6582...  0.3630 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41401...  Training loss: 0.9692...  Val loss: 1.6508...  0.3628 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41426...  Training loss: 0.9659...  Val loss: 1.6591...  0.3631 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41451...  Training loss: 0.9821...  Val loss: 1.6816...  0.3632 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41476...  Training loss: 0.9908...  Val loss: 1.6680...  0.3634 sec/batch\n",
      "Epoch: 187/500...  Training Step: 41501...  Training loss: 0.9788...  Val loss: 1.6732...  0.3631 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41526...  Training loss: 0.9749...  Val loss: 1.6707...  0.3632 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41551...  Training loss: 0.9819...  Val loss: 1.6440...  0.3633 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41576...  Training loss: 0.9977...  Val loss: 1.6579...  0.3632 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41601...  Training loss: 0.9845...  Val loss: 1.6570...  0.3634 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41626...  Training loss: 0.9654...  Val loss: 1.6610...  0.3634 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41651...  Training loss: 0.9577...  Val loss: 1.6658...  0.3636 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41676...  Training loss: 0.9785...  Val loss: 1.6738...  0.3633 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41701...  Training loss: 0.9710...  Val loss: 1.6732...  0.3637 sec/batch\n",
      "Epoch: 188/500...  Training Step: 41726...  Training loss: 0.9606...  Val loss: 1.6705...  0.3631 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41751...  Training loss: 0.9771...  Val loss: 1.6660...  0.3620 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41776...  Training loss: 0.9620...  Val loss: 1.6523...  0.3635 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41801...  Training loss: 0.9939...  Val loss: 1.6619...  0.3630 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41826...  Training loss: 0.9549...  Val loss: 1.6815...  0.3634 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41851...  Training loss: 0.9888...  Val loss: 1.6832...  0.3641 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41876...  Training loss: 0.9594...  Val loss: 1.6630...  0.3635 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41901...  Training loss: 0.9659...  Val loss: 1.6593...  0.3637 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41926...  Training loss: 0.9801...  Val loss: 1.6695...  0.3630 sec/batch\n",
      "Epoch: 189/500...  Training Step: 41951...  Training loss: 0.9949...  Val loss: 1.6860...  0.3632 sec/batch\n",
      "Epoch: 190/500...  Training Step: 41976...  Training loss: 0.9719...  Val loss: 1.6703...  0.3620 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190/500...  Training Step: 42001...  Training loss: 0.9503...  Val loss: 1.6693...  0.3631 sec/batch\n",
      "Epoch: 190/500...  Training Step: 42026...  Training loss: 0.9909...  Val loss: 1.6533...  0.3633 sec/batch\n",
      "Epoch: 190/500...  Training Step: 42051...  Training loss: 0.9685...  Val loss: 1.6649...  0.3636 sec/batch\n",
      "Epoch: 190/500...  Training Step: 42076...  Training loss: 0.9715...  Val loss: 1.6883...  0.3633 sec/batch\n",
      "Epoch: 190/500...  Training Step: 42101...  Training loss: 0.9705...  Val loss: 1.6754...  0.3635 sec/batch\n",
      "Epoch: 190/500...  Training Step: 42126...  Training loss: 0.9659...  Val loss: 1.6775...  0.3635 sec/batch\n",
      "Epoch: 190/500...  Training Step: 42151...  Training loss: 0.9700...  Val loss: 1.6775...  0.3637 sec/batch\n",
      "Epoch: 190/500...  Training Step: 42176...  Training loss: 0.9935...  Val loss: 1.6683...  0.3634 sec/batch\n",
      "Epoch 190/500 time:81.9319338798523...  finished at 2017-10-30 13:57:26\n",
      "Epoch: 191/500...  Training Step: 42201...  Training loss: 0.9848...  Val loss: 1.6747...  0.3635 sec/batch\n",
      "Epoch: 191/500...  Training Step: 42226...  Training loss: 0.9642...  Val loss: 1.6768...  0.3632 sec/batch\n",
      "Epoch: 191/500...  Training Step: 42251...  Training loss: 0.9829...  Val loss: 1.6575...  0.3633 sec/batch\n",
      "Epoch: 191/500...  Training Step: 42276...  Training loss: 0.9611...  Val loss: 1.6666...  0.3630 sec/batch\n",
      "Epoch: 191/500...  Training Step: 42301...  Training loss: 0.9702...  Val loss: 1.6612...  0.3633 sec/batch\n",
      "Epoch: 191/500...  Training Step: 42326...  Training loss: 0.9893...  Val loss: 1.6773...  0.3634 sec/batch\n",
      "Epoch: 191/500...  Training Step: 42351...  Training loss: 1.0044...  Val loss: 1.6815...  0.3636 sec/batch\n",
      "Epoch: 191/500...  Training Step: 42376...  Training loss: 0.9619...  Val loss: 1.6559...  0.3629 sec/batch\n",
      "Epoch: 191/500...  Training Step: 42401...  Training loss: 0.9750...  Val loss: 1.6812...  0.3630 sec/batch\n",
      "Epoch: 192/500...  Training Step: 42426...  Training loss: 0.9750...  Val loss: 1.6777...  0.3630 sec/batch\n",
      "Epoch: 192/500...  Training Step: 42451...  Training loss: 0.9750...  Val loss: 1.6918...  0.3637 sec/batch\n",
      "Epoch: 192/500...  Training Step: 42476...  Training loss: 0.9703...  Val loss: 1.6730...  0.3634 sec/batch\n",
      "Epoch: 192/500...  Training Step: 42501...  Training loss: 0.9740...  Val loss: 1.6658...  0.3634 sec/batch\n",
      "Epoch: 192/500...  Training Step: 42526...  Training loss: 0.9514...  Val loss: 1.6790...  0.3634 sec/batch\n",
      "Epoch: 192/500...  Training Step: 42551...  Training loss: 0.9752...  Val loss: 1.6676...  0.3640 sec/batch\n",
      "Epoch: 192/500...  Training Step: 42576...  Training loss: 0.9776...  Val loss: 1.6571...  0.3631 sec/batch\n",
      "Epoch: 192/500...  Training Step: 42601...  Training loss: 0.9703...  Val loss: 1.6723...  0.3633 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42626...  Training loss: 0.9639...  Val loss: 1.6695...  0.3634 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42651...  Training loss: 0.9619...  Val loss: 1.6574...  0.3632 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42676...  Training loss: 0.9687...  Val loss: 1.6756...  0.3629 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42701...  Training loss: 0.9609...  Val loss: 1.6825...  0.3633 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42726...  Training loss: 0.9898...  Val loss: 1.6556...  0.3634 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42751...  Training loss: 0.9632...  Val loss: 1.6857...  0.3632 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42776...  Training loss: 0.9413...  Val loss: 1.6809...  0.3637 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42801...  Training loss: 0.9510...  Val loss: 1.6853...  0.3632 sec/batch\n",
      "Epoch: 193/500...  Training Step: 42826...  Training loss: 0.9429...  Val loss: 1.6747...  0.3628 sec/batch\n",
      "Epoch: 194/500...  Training Step: 42851...  Training loss: 0.9783...  Val loss: 1.6662...  0.3634 sec/batch\n",
      "Epoch: 194/500...  Training Step: 42876...  Training loss: 0.9452...  Val loss: 1.6698...  0.3629 sec/batch\n",
      "Epoch: 194/500...  Training Step: 42901...  Training loss: 0.9410...  Val loss: 1.6921...  0.3632 sec/batch\n",
      "Epoch: 194/500...  Training Step: 42926...  Training loss: 0.9691...  Val loss: 1.6859...  0.3622 sec/batch\n",
      "Epoch: 194/500...  Training Step: 42951...  Training loss: 0.9740...  Val loss: 1.6489...  0.3632 sec/batch\n",
      "Epoch: 194/500...  Training Step: 42976...  Training loss: 0.9586...  Val loss: 1.6784...  0.3637 sec/batch\n",
      "Epoch: 194/500...  Training Step: 43001...  Training loss: 0.9478...  Val loss: 1.6927...  0.3635 sec/batch\n",
      "Epoch: 194/500...  Training Step: 43026...  Training loss: 0.9656...  Val loss: 1.6839...  0.3631 sec/batch\n",
      "Epoch: 194/500...  Training Step: 43051...  Training loss: 0.9588...  Val loss: 1.6763...  0.3632 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43076...  Training loss: 0.9678...  Val loss: 1.6498...  0.3634 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43101...  Training loss: 0.9711...  Val loss: 1.6736...  0.3632 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43126...  Training loss: 0.9659...  Val loss: 1.6987...  0.3638 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43151...  Training loss: 0.9558...  Val loss: 1.6817...  0.3634 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43176...  Training loss: 0.9468...  Val loss: 1.6757...  0.3631 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43201...  Training loss: 0.9679...  Val loss: 1.6744...  0.3637 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43226...  Training loss: 0.9693...  Val loss: 1.6852...  0.3632 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43251...  Training loss: 0.9600...  Val loss: 1.6715...  0.3628 sec/batch\n",
      "Epoch: 195/500...  Training Step: 43276...  Training loss: 0.9637...  Val loss: 1.6926...  0.3633 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43301...  Training loss: 0.9510...  Val loss: 1.6827...  0.3632 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43326...  Training loss: 0.9575...  Val loss: 1.6669...  0.3625 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43351...  Training loss: 0.9653...  Val loss: 1.6814...  0.3633 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43376...  Training loss: 0.9670...  Val loss: 1.6755...  0.3636 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43401...  Training loss: 0.9746...  Val loss: 1.6728...  0.3636 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43426...  Training loss: 0.9558...  Val loss: 1.6790...  0.3631 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43451...  Training loss: 0.9679...  Val loss: 1.6679...  0.3634 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43476...  Training loss: 0.9720...  Val loss: 1.6969...  0.3631 sec/batch\n",
      "Epoch: 196/500...  Training Step: 43501...  Training loss: 0.9652...  Val loss: 1.6868...  0.3630 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43526...  Training loss: 0.9507...  Val loss: 1.6847...  0.3632 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43551...  Training loss: 0.9521...  Val loss: 1.6862...  0.3633 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43576...  Training loss: 0.9724...  Val loss: 1.6685...  0.3640 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43601...  Training loss: 0.9610...  Val loss: 1.6863...  0.3637 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43626...  Training loss: 0.9633...  Val loss: 1.7038...  0.3637 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43651...  Training loss: 0.9744...  Val loss: 1.6830...  0.3634 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43676...  Training loss: 0.9617...  Val loss: 1.6960...  0.3637 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43701...  Training loss: 0.9578...  Val loss: 1.6805...  0.3635 sec/batch\n",
      "Epoch: 197/500...  Training Step: 43726...  Training loss: 0.9679...  Val loss: 1.6878...  0.3633 sec/batch\n",
      "Epoch: 198/500...  Training Step: 43751...  Training loss: 0.9650...  Val loss: 1.6882...  0.3635 sec/batch\n",
      "Epoch: 198/500...  Training Step: 43776...  Training loss: 0.9614...  Val loss: 1.6694...  0.3633 sec/batch\n",
      "Epoch: 198/500...  Training Step: 43801...  Training loss: 0.9603...  Val loss: 1.6757...  0.3634 sec/batch\n",
      "Epoch: 198/500...  Training Step: 43826...  Training loss: 0.9581...  Val loss: 1.6818...  0.3635 sec/batch\n",
      "Epoch: 198/500...  Training Step: 43851...  Training loss: 0.9607...  Val loss: 1.6934...  0.3630 sec/batch\n",
      "Epoch: 198/500...  Training Step: 43876...  Training loss: 0.9509...  Val loss: 1.6831...  0.3635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198/500...  Training Step: 43901...  Training loss: 0.9622...  Val loss: 1.6814...  0.3628 sec/batch\n",
      "Epoch: 198/500...  Training Step: 43926...  Training loss: 0.9657...  Val loss: 1.6922...  0.3636 sec/batch\n",
      "Epoch: 198/500...  Training Step: 43951...  Training loss: 0.9449...  Val loss: 1.6888...  0.3631 sec/batch\n",
      "Epoch: 199/500...  Training Step: 43976...  Training loss: 0.9463...  Val loss: 1.6720...  0.3639 sec/batch\n",
      "Epoch: 199/500...  Training Step: 44001...  Training loss: 0.9397...  Val loss: 1.6831...  0.3633 sec/batch\n",
      "Epoch: 199/500...  Training Step: 44026...  Training loss: 0.9718...  Val loss: 1.6632...  0.3638 sec/batch\n",
      "Epoch: 199/500...  Training Step: 44051...  Training loss: 0.9561...  Val loss: 1.6902...  0.3620 sec/batch\n",
      "Epoch: 199/500...  Training Step: 44076...  Training loss: 0.9459...  Val loss: 1.6972...  0.3632 sec/batch\n",
      "Epoch: 199/500...  Training Step: 44101...  Training loss: 0.9626...  Val loss: 1.6826...  0.3624 sec/batch\n",
      "Epoch: 199/500...  Training Step: 44126...  Training loss: 0.9457...  Val loss: 1.6809...  0.3636 sec/batch\n",
      "Epoch: 199/500...  Training Step: 44151...  Training loss: 0.9435...  Val loss: 1.6900...  0.3631 sec/batch\n",
      "Epoch: 199/500...  Training Step: 44176...  Training loss: 0.9532...  Val loss: 1.7003...  0.3627 sec/batch\n",
      "Epoch: 200/500...  Training Step: 44201...  Training loss: 0.9553...  Val loss: 1.6963...  0.3640 sec/batch\n",
      "Epoch: 200/500...  Training Step: 44226...  Training loss: 0.9534...  Val loss: 1.6848...  0.3632 sec/batch\n",
      "Epoch: 200/500...  Training Step: 44251...  Training loss: 0.9447...  Val loss: 1.6686...  0.3629 sec/batch\n",
      "Epoch: 200/500...  Training Step: 44276...  Training loss: 0.9524...  Val loss: 1.6835...  0.3625 sec/batch\n",
      "Epoch: 200/500...  Training Step: 44301...  Training loss: 0.9545...  Val loss: 1.6926...  0.3632 sec/batch\n",
      "Epoch: 200/500...  Training Step: 44326...  Training loss: 0.9605...  Val loss: 1.6864...  0.3632 sec/batch\n",
      "Epoch: 200/500...  Training Step: 44351...  Training loss: 0.9697...  Val loss: 1.6915...  0.3628 sec/batch\n",
      "Epoch: 200/500...  Training Step: 44376...  Training loss: 0.9541...  Val loss: 1.6712...  0.3632 sec/batch\n",
      "Epoch 200/500 time:81.48988962173462...  finished at 2017-10-30 14:11:03\n",
      "Epoch: 201/500...  Training Step: 44401...  Training loss: 1.0659...  Val loss: 1.6719...  0.3633 sec/batch\n",
      "Epoch: 201/500...  Training Step: 44426...  Training loss: 0.9668...  Val loss: 1.6885...  0.3696 sec/batch\n",
      "Epoch: 201/500...  Training Step: 44451...  Training loss: 0.9368...  Val loss: 1.7065...  0.3635 sec/batch\n",
      "Epoch: 201/500...  Training Step: 44476...  Training loss: 0.9533...  Val loss: 1.6871...  0.3621 sec/batch\n",
      "Epoch: 201/500...  Training Step: 44501...  Training loss: 0.9573...  Val loss: 1.6802...  0.3637 sec/batch\n",
      "Epoch: 201/500...  Training Step: 44526...  Training loss: 0.9421...  Val loss: 1.6976...  0.3634 sec/batch\n",
      "Epoch: 201/500...  Training Step: 44551...  Training loss: 0.9490...  Val loss: 1.6987...  0.3635 sec/batch\n",
      "Epoch: 201/500...  Training Step: 44576...  Training loss: 0.9394...  Val loss: 1.6895...  0.3638 sec/batch\n",
      "Epoch: 201/500...  Training Step: 44601...  Training loss: 0.9564...  Val loss: 1.6881...  0.3631 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44626...  Training loss: 0.9606...  Val loss: 1.6816...  0.3632 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44651...  Training loss: 0.9480...  Val loss: 1.6824...  0.3635 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44676...  Training loss: 0.9451...  Val loss: 1.7035...  0.3636 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44701...  Training loss: 0.9370...  Val loss: 1.7041...  0.3637 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44726...  Training loss: 0.9409...  Val loss: 1.6779...  0.3634 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44751...  Training loss: 0.9416...  Val loss: 1.6917...  0.3631 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44776...  Training loss: 0.9349...  Val loss: 1.7167...  0.3633 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44801...  Training loss: 0.9479...  Val loss: 1.7093...  0.3638 sec/batch\n",
      "Epoch: 202/500...  Training Step: 44826...  Training loss: 0.9517...  Val loss: 1.6985...  0.3626 sec/batch\n",
      "Epoch: 203/500...  Training Step: 44851...  Training loss: 0.9719...  Val loss: 1.6920...  0.3638 sec/batch\n",
      "Epoch: 203/500...  Training Step: 44876...  Training loss: 0.9705...  Val loss: 1.6827...  0.3634 sec/batch\n",
      "Epoch: 203/500...  Training Step: 44901...  Training loss: 0.9323...  Val loss: 1.7215...  0.3634 sec/batch\n",
      "Epoch: 203/500...  Training Step: 44926...  Training loss: 0.9559...  Val loss: 1.6939...  0.3630 sec/batch\n",
      "Epoch: 203/500...  Training Step: 44951...  Training loss: 0.9308...  Val loss: 1.6728...  0.3633 sec/batch\n",
      "Epoch: 203/500...  Training Step: 44976...  Training loss: 0.9502...  Val loss: 1.6966...  0.3626 sec/batch\n",
      "Epoch: 203/500...  Training Step: 45001...  Training loss: 0.9487...  Val loss: 1.7039...  0.3635 sec/batch\n",
      "Epoch: 203/500...  Training Step: 45026...  Training loss: 0.9573...  Val loss: 1.6925...  0.3632 sec/batch\n",
      "Epoch: 203/500...  Training Step: 45051...  Training loss: 0.9612...  Val loss: 1.6978...  0.3634 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45076...  Training loss: 0.9386...  Val loss: 1.6825...  0.3634 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45101...  Training loss: 0.9608...  Val loss: 1.6886...  0.3636 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45126...  Training loss: 0.9500...  Val loss: 1.6959...  0.3631 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45151...  Training loss: 0.9691...  Val loss: 1.6871...  0.3634 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45176...  Training loss: 0.9617...  Val loss: 1.6831...  0.3623 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45201...  Training loss: 0.9526...  Val loss: 1.7015...  0.3634 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45226...  Training loss: 0.9517...  Val loss: 1.7040...  0.3631 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45251...  Training loss: 0.9364...  Val loss: 1.7120...  0.3630 sec/batch\n",
      "Epoch: 204/500...  Training Step: 45276...  Training loss: 0.9359...  Val loss: 1.6993...  0.3636 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45301...  Training loss: 0.9525...  Val loss: 1.6928...  0.3640 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45326...  Training loss: 0.9378...  Val loss: 1.6821...  0.3633 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45351...  Training loss: 0.9658...  Val loss: 1.7023...  0.3632 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45376...  Training loss: 0.9567...  Val loss: 1.7056...  0.3633 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45401...  Training loss: 0.9338...  Val loss: 1.6942...  0.3633 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45426...  Training loss: 0.9506...  Val loss: 1.6904...  0.3634 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45451...  Training loss: 0.9509...  Val loss: 1.7018...  0.3632 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45476...  Training loss: 0.9357...  Val loss: 1.7126...  0.3637 sec/batch\n",
      "Epoch: 205/500...  Training Step: 45501...  Training loss: 0.9760...  Val loss: 1.6977...  0.3631 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45526...  Training loss: 0.9250...  Val loss: 1.7129...  0.3631 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45551...  Training loss: 0.9384...  Val loss: 1.6904...  0.3634 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45576...  Training loss: 0.9515...  Val loss: 1.7016...  0.3631 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45601...  Training loss: 0.9400...  Val loss: 1.7097...  0.3633 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45626...  Training loss: 0.9392...  Val loss: 1.7042...  0.3634 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45651...  Training loss: 0.9358...  Val loss: 1.6980...  0.3633 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45676...  Training loss: 0.9462...  Val loss: 1.6988...  0.3633 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45701...  Training loss: 0.9472...  Val loss: 1.7073...  0.3630 sec/batch\n",
      "Epoch: 206/500...  Training Step: 45726...  Training loss: 0.9617...  Val loss: 1.7019...  0.3630 sec/batch\n",
      "Epoch: 207/500...  Training Step: 45751...  Training loss: 0.9580...  Val loss: 1.6991...  0.3636 sec/batch\n",
      "Epoch: 207/500...  Training Step: 45776...  Training loss: 0.9502...  Val loss: 1.6893...  0.3629 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 207/500...  Training Step: 45801...  Training loss: 0.9572...  Val loss: 1.6902...  0.3637 sec/batch\n",
      "Epoch: 207/500...  Training Step: 45826...  Training loss: 0.9438...  Val loss: 1.7100...  0.3633 sec/batch\n",
      "Epoch: 207/500...  Training Step: 45851...  Training loss: 0.9540...  Val loss: 1.7210...  0.3635 sec/batch\n",
      "Epoch: 207/500...  Training Step: 45876...  Training loss: 0.9646...  Val loss: 1.7154...  0.3633 sec/batch\n",
      "Epoch: 207/500...  Training Step: 45901...  Training loss: 0.9443...  Val loss: 1.7194...  0.3635 sec/batch\n",
      "Epoch: 207/500...  Training Step: 45926...  Training loss: 0.9431...  Val loss: 1.7029...  0.3635 sec/batch\n",
      "Epoch: 207/500...  Training Step: 45951...  Training loss: 0.9667...  Val loss: 1.7103...  0.3638 sec/batch\n",
      "Epoch: 208/500...  Training Step: 45976...  Training loss: 0.9383...  Val loss: 1.7103...  0.3639 sec/batch\n",
      "Epoch: 208/500...  Training Step: 46001...  Training loss: 0.9316...  Val loss: 1.7104...  0.3633 sec/batch\n",
      "Epoch: 208/500...  Training Step: 46026...  Training loss: 0.9409...  Val loss: 1.6986...  0.3630 sec/batch\n",
      "Epoch: 208/500...  Training Step: 46051...  Training loss: 0.9300...  Val loss: 1.7110...  0.3629 sec/batch\n",
      "Epoch: 208/500...  Training Step: 46076...  Training loss: 0.9510...  Val loss: 1.7101...  0.3631 sec/batch\n",
      "Epoch: 208/500...  Training Step: 46101...  Training loss: 0.9596...  Val loss: 1.6960...  0.3635 sec/batch\n",
      "Epoch: 208/500...  Training Step: 46126...  Training loss: 0.9480...  Val loss: 1.7158...  0.3628 sec/batch\n",
      "Epoch: 208/500...  Training Step: 46151...  Training loss: 0.9445...  Val loss: 1.7062...  0.3633 sec/batch\n",
      "Epoch: 208/500...  Training Step: 46176...  Training loss: 0.9405...  Val loss: 1.7100...  0.3637 sec/batch\n",
      "Epoch: 209/500...  Training Step: 46201...  Training loss: 0.9339...  Val loss: 1.6990...  0.3631 sec/batch\n",
      "Epoch: 209/500...  Training Step: 46226...  Training loss: 0.9598...  Val loss: 1.7179...  0.3632 sec/batch\n",
      "Epoch: 209/500...  Training Step: 46251...  Training loss: 0.9645...  Val loss: 1.7061...  0.3635 sec/batch\n",
      "Epoch: 209/500...  Training Step: 46276...  Training loss: 0.9469...  Val loss: 1.7043...  0.3635 sec/batch\n",
      "Epoch: 209/500...  Training Step: 46301...  Training loss: 0.9326...  Val loss: 1.7172...  0.3627 sec/batch\n",
      "Epoch: 209/500...  Training Step: 46326...  Training loss: 0.9451...  Val loss: 1.7082...  0.3631 sec/batch\n",
      "Epoch: 209/500...  Training Step: 46351...  Training loss: 0.9363...  Val loss: 1.6943...  0.3653 sec/batch\n",
      "Epoch: 209/500...  Training Step: 46376...  Training loss: 0.9305...  Val loss: 1.7075...  0.3632 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46401...  Training loss: 0.9607...  Val loss: 1.6979...  0.3628 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46426...  Training loss: 0.9374...  Val loss: 1.6951...  0.3631 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46451...  Training loss: 0.9195...  Val loss: 1.7066...  0.3635 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46476...  Training loss: 0.9288...  Val loss: 1.7125...  0.3634 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46501...  Training loss: 0.9263...  Val loss: 1.6931...  0.3638 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46526...  Training loss: 0.9410...  Val loss: 1.7164...  0.3636 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46551...  Training loss: 0.9335...  Val loss: 1.7165...  0.3634 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46576...  Training loss: 0.9288...  Val loss: 1.7184...  0.3631 sec/batch\n",
      "Epoch: 210/500...  Training Step: 46601...  Training loss: 0.9474...  Val loss: 1.7071...  0.3627 sec/batch\n",
      "Epoch 210/500 time:81.94504642486572...  finished at 2017-10-30 14:24:41\n",
      "Epoch: 211/500...  Training Step: 46626...  Training loss: 0.9582...  Val loss: 1.6881...  0.3633 sec/batch\n",
      "Epoch: 211/500...  Training Step: 46651...  Training loss: 0.9227...  Val loss: 1.6987...  0.3634 sec/batch\n",
      "Epoch: 211/500...  Training Step: 46676...  Training loss: 0.9466...  Val loss: 1.7183...  0.3627 sec/batch\n",
      "Epoch: 211/500...  Training Step: 46701...  Training loss: 0.9358...  Val loss: 1.7150...  0.3633 sec/batch\n",
      "Epoch: 211/500...  Training Step: 46726...  Training loss: 0.9366...  Val loss: 1.6985...  0.3633 sec/batch\n",
      "Epoch: 211/500...  Training Step: 46751...  Training loss: 0.9298...  Val loss: 1.6946...  0.3638 sec/batch\n",
      "Epoch: 211/500...  Training Step: 46776...  Training loss: 0.9284...  Val loss: 1.7241...  0.3634 sec/batch\n",
      "Epoch: 211/500...  Training Step: 46801...  Training loss: 0.9275...  Val loss: 1.7220...  0.3631 sec/batch\n",
      "Epoch: 211/500...  Training Step: 46826...  Training loss: 0.9333...  Val loss: 1.7172...  0.3632 sec/batch\n",
      "Epoch: 212/500...  Training Step: 46851...  Training loss: 0.9429...  Val loss: 1.7127...  0.3634 sec/batch\n",
      "Epoch: 212/500...  Training Step: 46876...  Training loss: 0.9412...  Val loss: 1.7050...  0.3632 sec/batch\n",
      "Epoch: 212/500...  Training Step: 46901...  Training loss: 0.9462...  Val loss: 1.7354...  0.3631 sec/batch\n",
      "Epoch: 212/500...  Training Step: 46926...  Training loss: 0.9471...  Val loss: 1.7125...  0.3633 sec/batch\n",
      "Epoch: 212/500...  Training Step: 46951...  Training loss: 0.9337...  Val loss: 1.6924...  0.3635 sec/batch\n",
      "Epoch: 212/500...  Training Step: 46976...  Training loss: 0.9311...  Val loss: 1.7118...  0.3635 sec/batch\n",
      "Epoch: 212/500...  Training Step: 47001...  Training loss: 0.9482...  Val loss: 1.7433...  0.3627 sec/batch\n",
      "Epoch: 212/500...  Training Step: 47026...  Training loss: 0.9498...  Val loss: 1.7178...  0.3630 sec/batch\n",
      "Epoch: 212/500...  Training Step: 47051...  Training loss: 0.9282...  Val loss: 1.7153...  0.3637 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47076...  Training loss: 0.9394...  Val loss: 1.7184...  0.3632 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47101...  Training loss: 0.9538...  Val loss: 1.6873...  0.3632 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47126...  Training loss: 0.9514...  Val loss: 1.7224...  0.3637 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47151...  Training loss: 0.9575...  Val loss: 1.7080...  0.3639 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47176...  Training loss: 0.9235...  Val loss: 1.7069...  0.3628 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47201...  Training loss: 0.9155...  Val loss: 1.7175...  0.3627 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47226...  Training loss: 0.9491...  Val loss: 1.7207...  0.3639 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47251...  Training loss: 0.9271...  Val loss: 1.7247...  0.3634 sec/batch\n",
      "Epoch: 213/500...  Training Step: 47276...  Training loss: 0.9241...  Val loss: 1.7125...  0.3632 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47301...  Training loss: 0.9404...  Val loss: 1.7084...  0.3636 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47326...  Training loss: 0.9290...  Val loss: 1.6955...  0.3629 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47351...  Training loss: 0.9647...  Val loss: 1.7238...  0.3632 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47376...  Training loss: 0.9172...  Val loss: 1.7352...  0.3634 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47401...  Training loss: 0.9476...  Val loss: 1.7191...  0.3633 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47426...  Training loss: 0.9271...  Val loss: 1.7216...  0.3631 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47451...  Training loss: 0.9291...  Val loss: 1.7015...  0.3631 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47476...  Training loss: 0.9504...  Val loss: 1.7336...  0.3639 sec/batch\n",
      "Epoch: 214/500...  Training Step: 47501...  Training loss: 0.9500...  Val loss: 1.7409...  0.3631 sec/batch\n",
      "Epoch: 215/500...  Training Step: 47526...  Training loss: 0.9382...  Val loss: 1.7166...  0.3631 sec/batch\n",
      "Epoch: 215/500...  Training Step: 47551...  Training loss: 0.9238...  Val loss: 1.7134...  0.3634 sec/batch\n",
      "Epoch: 215/500...  Training Step: 47576...  Training loss: 0.9507...  Val loss: 1.7174...  0.3630 sec/batch\n",
      "Epoch: 215/500...  Training Step: 47601...  Training loss: 0.9202...  Val loss: 1.7196...  0.3630 sec/batch\n",
      "Epoch: 215/500...  Training Step: 47626...  Training loss: 0.9365...  Val loss: 1.7343...  0.3633 sec/batch\n",
      "Epoch: 215/500...  Training Step: 47651...  Training loss: 0.9316...  Val loss: 1.7161...  0.3631 sec/batch\n",
      "Epoch: 215/500...  Training Step: 47676...  Training loss: 0.9267...  Val loss: 1.7232...  0.3629 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 215/500...  Training Step: 47701...  Training loss: 0.9193...  Val loss: 1.7358...  0.3632 sec/batch\n",
      "Epoch: 215/500...  Training Step: 47726...  Training loss: 0.9487...  Val loss: 1.7288...  0.3633 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47751...  Training loss: 0.9572...  Val loss: 1.7253...  0.3631 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47776...  Training loss: 0.9227...  Val loss: 1.7239...  0.3631 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47801...  Training loss: 0.9476...  Val loss: 1.7198...  0.3633 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47826...  Training loss: 0.9340...  Val loss: 1.7242...  0.3637 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47851...  Training loss: 0.9398...  Val loss: 1.7248...  0.3631 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47876...  Training loss: 0.9440...  Val loss: 1.7273...  0.3634 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47901...  Training loss: 0.9668...  Val loss: 1.7231...  0.3632 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47926...  Training loss: 0.9358...  Val loss: 1.7103...  0.3633 sec/batch\n",
      "Epoch: 216/500...  Training Step: 47951...  Training loss: 0.9471...  Val loss: 1.7281...  0.3632 sec/batch\n",
      "Epoch: 217/500...  Training Step: 47976...  Training loss: 0.9362...  Val loss: 1.7295...  0.3644 sec/batch\n",
      "Epoch: 217/500...  Training Step: 48001...  Training loss: 0.9242...  Val loss: 1.7379...  0.3631 sec/batch\n",
      "Epoch: 217/500...  Training Step: 48026...  Training loss: 0.9193...  Val loss: 1.7168...  0.3620 sec/batch\n",
      "Epoch: 217/500...  Training Step: 48051...  Training loss: 0.9291...  Val loss: 1.7280...  0.3632 sec/batch\n",
      "Epoch: 217/500...  Training Step: 48076...  Training loss: 0.9148...  Val loss: 1.7372...  0.3633 sec/batch\n",
      "Epoch: 217/500...  Training Step: 48101...  Training loss: 0.9333...  Val loss: 1.7269...  0.3632 sec/batch\n",
      "Epoch: 217/500...  Training Step: 48126...  Training loss: 0.9425...  Val loss: 1.7062...  0.3633 sec/batch\n",
      "Epoch: 217/500...  Training Step: 48151...  Training loss: 0.9336...  Val loss: 1.7220...  0.3635 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48176...  Training loss: 0.9219...  Val loss: 1.7247...  0.3631 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48201...  Training loss: 0.9129...  Val loss: 1.7215...  0.3637 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48226...  Training loss: 0.9303...  Val loss: 1.7119...  0.3636 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48251...  Training loss: 0.9231...  Val loss: 1.7232...  0.3631 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48276...  Training loss: 0.9406...  Val loss: 1.7153...  0.3636 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48301...  Training loss: 0.9204...  Val loss: 1.7476...  0.3637 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48326...  Training loss: 0.9218...  Val loss: 1.7321...  0.3630 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48351...  Training loss: 0.9136...  Val loss: 1.7370...  0.3630 sec/batch\n",
      "Epoch: 218/500...  Training Step: 48376...  Training loss: 0.9105...  Val loss: 1.7232...  0.3637 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48401...  Training loss: 0.9446...  Val loss: 1.7322...  0.3635 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48426...  Training loss: 0.9112...  Val loss: 1.7265...  0.3636 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48451...  Training loss: 0.8991...  Val loss: 1.7413...  0.3634 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48476...  Training loss: 0.9343...  Val loss: 1.7272...  0.3632 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48501...  Training loss: 0.9415...  Val loss: 1.7048...  0.3625 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48526...  Training loss: 0.9190...  Val loss: 1.7374...  0.3634 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48551...  Training loss: 0.9170...  Val loss: 1.7443...  0.3633 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48576...  Training loss: 0.9235...  Val loss: 1.7286...  0.3636 sec/batch\n",
      "Epoch: 219/500...  Training Step: 48601...  Training loss: 0.9129...  Val loss: 1.7235...  0.3633 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48626...  Training loss: 0.9348...  Val loss: 1.7032...  0.3632 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48651...  Training loss: 0.9254...  Val loss: 1.7189...  0.3632 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48676...  Training loss: 0.9269...  Val loss: 1.7588...  0.3634 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48701...  Training loss: 0.9149...  Val loss: 1.7263...  0.3632 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48726...  Training loss: 0.9312...  Val loss: 1.7183...  0.3624 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48751...  Training loss: 0.9185...  Val loss: 1.7209...  0.3688 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48776...  Training loss: 0.9339...  Val loss: 1.7485...  0.3632 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48801...  Training loss: 0.9206...  Val loss: 1.7288...  0.3636 sec/batch\n",
      "Epoch: 220/500...  Training Step: 48826...  Training loss: 0.9193...  Val loss: 1.7289...  0.3636 sec/batch\n",
      "Epoch 220/500 time:81.6005187034607...  finished at 2017-10-30 14:38:18\n",
      "Epoch: 221/500...  Training Step: 48851...  Training loss: 0.9161...  Val loss: 1.7266...  0.3635 sec/batch\n",
      "Epoch: 221/500...  Training Step: 48876...  Training loss: 0.9208...  Val loss: 1.7184...  0.3635 sec/batch\n",
      "Epoch: 221/500...  Training Step: 48901...  Training loss: 0.9325...  Val loss: 1.7418...  0.3628 sec/batch\n",
      "Epoch: 221/500...  Training Step: 48926...  Training loss: 0.9287...  Val loss: 1.7145...  0.3631 sec/batch\n",
      "Epoch: 221/500...  Training Step: 48951...  Training loss: 0.9423...  Val loss: 1.7120...  0.3635 sec/batch\n",
      "Epoch: 221/500...  Training Step: 48976...  Training loss: 0.9191...  Val loss: 1.7180...  0.3632 sec/batch\n",
      "Epoch: 221/500...  Training Step: 49001...  Training loss: 0.9433...  Val loss: 1.7272...  0.3640 sec/batch\n",
      "Epoch: 221/500...  Training Step: 49026...  Training loss: 0.9334...  Val loss: 1.7488...  0.3632 sec/batch\n",
      "Epoch: 221/500...  Training Step: 49051...  Training loss: 0.9257...  Val loss: 1.7357...  0.3635 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49076...  Training loss: 0.9170...  Val loss: 1.7235...  0.3631 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49101...  Training loss: 0.9182...  Val loss: 1.7296...  0.3635 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49126...  Training loss: 0.9387...  Val loss: 1.7275...  0.3632 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49151...  Training loss: 0.9271...  Val loss: 1.7283...  0.3638 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49176...  Training loss: 0.9122...  Val loss: 1.7328...  0.3632 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49201...  Training loss: 0.9331...  Val loss: 1.7331...  0.3635 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49226...  Training loss: 0.9241...  Val loss: 1.7552...  0.3638 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49251...  Training loss: 0.9228...  Val loss: 1.7419...  0.3631 sec/batch\n",
      "Epoch: 222/500...  Training Step: 49276...  Training loss: 0.9283...  Val loss: 1.7403...  0.3634 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49301...  Training loss: 0.9242...  Val loss: 1.7238...  0.3632 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49326...  Training loss: 0.9247...  Val loss: 1.7098...  0.3638 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49351...  Training loss: 0.9135...  Val loss: 1.7440...  0.3628 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49376...  Training loss: 0.9131...  Val loss: 1.7240...  0.3639 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49401...  Training loss: 0.9213...  Val loss: 1.7499...  0.3633 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49426...  Training loss: 0.9198...  Val loss: 1.7236...  0.3635 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49451...  Training loss: 0.9172...  Val loss: 1.7332...  0.3637 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49476...  Training loss: 0.9300...  Val loss: 1.7527...  0.3631 sec/batch\n",
      "Epoch: 223/500...  Training Step: 49501...  Training loss: 0.9245...  Val loss: 1.7390...  0.3634 sec/batch\n",
      "Epoch: 224/500...  Training Step: 49526...  Training loss: 0.9187...  Val loss: 1.7168...  0.3636 sec/batch\n",
      "Epoch: 224/500...  Training Step: 49551...  Training loss: 0.9089...  Val loss: 1.7275...  0.3633 sec/batch\n",
      "Epoch: 224/500...  Training Step: 49576...  Training loss: 0.9219...  Val loss: 1.7217...  0.3630 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224/500...  Training Step: 49601...  Training loss: 0.9166...  Val loss: 1.7437...  0.3634 sec/batch\n",
      "Epoch: 224/500...  Training Step: 49626...  Training loss: 0.9226...  Val loss: 1.7496...  0.3634 sec/batch\n",
      "Epoch: 224/500...  Training Step: 49651...  Training loss: 0.9280...  Val loss: 1.7312...  0.3630 sec/batch\n",
      "Epoch: 224/500...  Training Step: 49676...  Training loss: 0.9241...  Val loss: 1.7314...  0.3640 sec/batch\n",
      "Epoch: 224/500...  Training Step: 49701...  Training loss: 0.9067...  Val loss: 1.7496...  0.3630 sec/batch\n",
      "Epoch: 224/500...  Training Step: 49726...  Training loss: 0.9080...  Val loss: 1.7511...  0.3632 sec/batch\n",
      "Epoch: 225/500...  Training Step: 49751...  Training loss: 0.9259...  Val loss: 1.7406...  0.3650 sec/batch\n",
      "Epoch: 225/500...  Training Step: 49776...  Training loss: 0.9095...  Val loss: 1.7320...  0.3638 sec/batch\n",
      "Epoch: 225/500...  Training Step: 49801...  Training loss: 0.9212...  Val loss: 1.7266...  0.3634 sec/batch\n",
      "Epoch: 225/500...  Training Step: 49826...  Training loss: 0.9251...  Val loss: 1.7308...  0.3635 sec/batch\n",
      "Epoch: 225/500...  Training Step: 49851...  Training loss: 0.9115...  Val loss: 1.7535...  0.3753 sec/batch\n",
      "Epoch: 225/500...  Training Step: 49876...  Training loss: 0.9268...  Val loss: 1.7365...  0.3638 sec/batch\n",
      "Epoch: 225/500...  Training Step: 49901...  Training loss: 0.9292...  Val loss: 1.7369...  0.3633 sec/batch\n",
      "Epoch: 225/500...  Training Step: 49926...  Training loss: 0.9098...  Val loss: 1.7335...  0.3632 sec/batch\n",
      "Epoch: 226/500...  Training Step: 49951...  Training loss: 1.0249...  Val loss: 1.7293...  0.3635 sec/batch\n",
      "Epoch: 226/500...  Training Step: 49976...  Training loss: 0.9338...  Val loss: 1.7438...  0.3633 sec/batch\n",
      "Epoch: 226/500...  Training Step: 50001...  Training loss: 0.9020...  Val loss: 1.7492...  0.3635 sec/batch\n",
      "Epoch: 226/500...  Training Step: 50026...  Training loss: 0.9057...  Val loss: 1.7313...  0.3635 sec/batch\n",
      "Epoch: 226/500...  Training Step: 50051...  Training loss: 0.9303...  Val loss: 1.7466...  0.3633 sec/batch\n",
      "Epoch: 226/500...  Training Step: 50076...  Training loss: 0.9048...  Val loss: 1.7577...  0.3635 sec/batch\n",
      "Epoch: 226/500...  Training Step: 50101...  Training loss: 0.9185...  Val loss: 1.7414...  0.3634 sec/batch\n",
      "Epoch: 226/500...  Training Step: 50126...  Training loss: 0.9061...  Val loss: 1.7381...  0.3639 sec/batch\n",
      "Epoch: 226/500...  Training Step: 50151...  Training loss: 0.9292...  Val loss: 1.7446...  0.3633 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50176...  Training loss: 0.9350...  Val loss: 1.7344...  0.3625 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50201...  Training loss: 0.9141...  Val loss: 1.7353...  0.3622 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50226...  Training loss: 0.9049...  Val loss: 1.7516...  0.3632 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50251...  Training loss: 0.9055...  Val loss: 1.7532...  0.3634 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50276...  Training loss: 0.9041...  Val loss: 1.7341...  0.3635 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50301...  Training loss: 0.9071...  Val loss: 1.7549...  0.3635 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50326...  Training loss: 0.9081...  Val loss: 1.7535...  0.3632 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50351...  Training loss: 0.9096...  Val loss: 1.7620...  0.3632 sec/batch\n",
      "Epoch: 227/500...  Training Step: 50376...  Training loss: 0.9198...  Val loss: 1.7399...  0.3637 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50401...  Training loss: 0.9294...  Val loss: 1.7341...  0.3632 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50426...  Training loss: 0.9269...  Val loss: 1.7462...  0.3631 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50451...  Training loss: 0.9023...  Val loss: 1.7555...  0.3634 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50476...  Training loss: 0.9036...  Val loss: 1.7359...  0.3630 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50501...  Training loss: 0.8957...  Val loss: 1.7275...  0.3632 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50526...  Training loss: 0.9111...  Val loss: 1.7488...  0.3633 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50551...  Training loss: 0.9097...  Val loss: 1.7567...  0.3636 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50576...  Training loss: 0.9115...  Val loss: 1.7388...  0.3635 sec/batch\n",
      "Epoch: 228/500...  Training Step: 50601...  Training loss: 0.9220...  Val loss: 1.7529...  0.3636 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50626...  Training loss: 0.9098...  Val loss: 1.7307...  0.3637 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50651...  Training loss: 0.9232...  Val loss: 1.7349...  0.3635 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50676...  Training loss: 0.9075...  Val loss: 1.7422...  0.3634 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50701...  Training loss: 0.9318...  Val loss: 1.7263...  0.3654 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50726...  Training loss: 0.9221...  Val loss: 1.7202...  0.3632 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50751...  Training loss: 0.9233...  Val loss: 1.7447...  0.3635 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50776...  Training loss: 0.9176...  Val loss: 1.7472...  0.3639 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50801...  Training loss: 0.8984...  Val loss: 1.7633...  0.3633 sec/batch\n",
      "Epoch: 229/500...  Training Step: 50826...  Training loss: 0.9011...  Val loss: 1.7367...  0.3656 sec/batch\n",
      "Epoch: 230/500...  Training Step: 50851...  Training loss: 0.9124...  Val loss: 1.7322...  0.3634 sec/batch\n",
      "Epoch: 230/500...  Training Step: 50876...  Training loss: 0.9047...  Val loss: 1.7349...  0.3636 sec/batch\n",
      "Epoch: 230/500...  Training Step: 50901...  Training loss: 0.9210...  Val loss: 1.7523...  0.3632 sec/batch\n",
      "Epoch: 230/500...  Training Step: 50926...  Training loss: 0.9251...  Val loss: 1.7444...  0.3632 sec/batch\n",
      "Epoch: 230/500...  Training Step: 50951...  Training loss: 0.9062...  Val loss: 1.7249...  0.3645 sec/batch\n",
      "Epoch: 230/500...  Training Step: 50976...  Training loss: 0.9142...  Val loss: 1.7363...  0.3633 sec/batch\n",
      "Epoch: 230/500...  Training Step: 51001...  Training loss: 0.9121...  Val loss: 1.7491...  0.3633 sec/batch\n",
      "Epoch: 230/500...  Training Step: 51026...  Training loss: 0.9035...  Val loss: 1.7575...  0.3632 sec/batch\n",
      "Epoch: 230/500...  Training Step: 51051...  Training loss: 0.9406...  Val loss: 1.7558...  0.3630 sec/batch\n",
      "Epoch 230/500 time:82.0525951385498...  finished at 2017-10-30 14:51:56\n",
      "Epoch: 231/500...  Training Step: 51076...  Training loss: 0.8916...  Val loss: 1.7431...  0.3629 sec/batch\n",
      "Epoch: 231/500...  Training Step: 51101...  Training loss: 0.8989...  Val loss: 1.7449...  0.3632 sec/batch\n",
      "Epoch: 231/500...  Training Step: 51126...  Training loss: 0.9126...  Val loss: 1.7472...  0.3632 sec/batch\n",
      "Epoch: 231/500...  Training Step: 51151...  Training loss: 0.9105...  Val loss: 1.7473...  0.3635 sec/batch\n",
      "Epoch: 231/500...  Training Step: 51176...  Training loss: 0.9124...  Val loss: 1.7432...  0.3633 sec/batch\n",
      "Epoch: 231/500...  Training Step: 51201...  Training loss: 0.9068...  Val loss: 1.7464...  0.3659 sec/batch\n",
      "Epoch: 231/500...  Training Step: 51226...  Training loss: 0.9096...  Val loss: 1.7411...  0.3636 sec/batch\n",
      "Epoch: 231/500...  Training Step: 51251...  Training loss: 0.9066...  Val loss: 1.7738...  0.3636 sec/batch\n",
      "Epoch: 231/500...  Training Step: 51276...  Training loss: 0.9221...  Val loss: 1.7427...  0.3637 sec/batch\n",
      "Epoch: 232/500...  Training Step: 51301...  Training loss: 0.9234...  Val loss: 1.7482...  0.3630 sec/batch\n",
      "Epoch: 232/500...  Training Step: 51326...  Training loss: 0.9174...  Val loss: 1.7452...  0.3665 sec/batch\n",
      "Epoch: 232/500...  Training Step: 51351...  Training loss: 0.9127...  Val loss: 1.7460...  0.3632 sec/batch\n",
      "Epoch: 232/500...  Training Step: 51376...  Training loss: 0.9083...  Val loss: 1.7603...  0.3638 sec/batch\n",
      "Epoch: 232/500...  Training Step: 51401...  Training loss: 0.9084...  Val loss: 1.7615...  0.3634 sec/batch\n",
      "Epoch: 232/500...  Training Step: 51426...  Training loss: 0.9267...  Val loss: 1.7626...  0.3636 sec/batch\n",
      "Epoch: 232/500...  Training Step: 51451...  Training loss: 0.9054...  Val loss: 1.7778...  0.3664 sec/batch\n",
      "Epoch: 232/500...  Training Step: 51476...  Training loss: 0.9222...  Val loss: 1.7725...  0.3631 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232/500...  Training Step: 51501...  Training loss: 0.9256...  Val loss: 1.7611...  0.3634 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51526...  Training loss: 0.8978...  Val loss: 1.7540...  0.3633 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51551...  Training loss: 0.9068...  Val loss: 1.7497...  0.3638 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51576...  Training loss: 0.9085...  Val loss: 1.7547...  0.3632 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51601...  Training loss: 0.8952...  Val loss: 1.7758...  0.3629 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51626...  Training loss: 0.9074...  Val loss: 1.7650...  0.3635 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51651...  Training loss: 0.9216...  Val loss: 1.7369...  0.3635 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51676...  Training loss: 0.9260...  Val loss: 1.7653...  0.3635 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51701...  Training loss: 0.9073...  Val loss: 1.7647...  0.3635 sec/batch\n",
      "Epoch: 233/500...  Training Step: 51726...  Training loss: 0.9101...  Val loss: 1.7676...  0.3633 sec/batch\n",
      "Epoch: 234/500...  Training Step: 51751...  Training loss: 0.9048...  Val loss: 1.7583...  0.3632 sec/batch\n",
      "Epoch: 234/500...  Training Step: 51776...  Training loss: 0.9183...  Val loss: 1.7656...  0.3628 sec/batch\n",
      "Epoch: 234/500...  Training Step: 51801...  Training loss: 0.9363...  Val loss: 1.7573...  0.3637 sec/batch\n",
      "Epoch: 234/500...  Training Step: 51826...  Training loss: 0.9078...  Val loss: 1.7679...  0.3634 sec/batch\n",
      "Epoch: 234/500...  Training Step: 51851...  Training loss: 0.8968...  Val loss: 1.7768...  0.3632 sec/batch\n",
      "Epoch: 234/500...  Training Step: 51876...  Training loss: 0.9186...  Val loss: 1.7512...  0.3634 sec/batch\n",
      "Epoch: 234/500...  Training Step: 51901...  Training loss: 0.9028...  Val loss: 1.7379...  0.3632 sec/batch\n",
      "Epoch: 234/500...  Training Step: 51926...  Training loss: 0.8953...  Val loss: 1.7756...  0.3631 sec/batch\n",
      "Epoch: 235/500...  Training Step: 51951...  Training loss: 0.9163...  Val loss: 1.7549...  0.3635 sec/batch\n",
      "Epoch: 235/500...  Training Step: 51976...  Training loss: 0.9032...  Val loss: 1.7562...  0.3637 sec/batch\n",
      "Epoch: 235/500...  Training Step: 52001...  Training loss: 0.8859...  Val loss: 1.7582...  0.3634 sec/batch\n",
      "Epoch: 235/500...  Training Step: 52026...  Training loss: 0.9015...  Val loss: 1.7625...  0.3632 sec/batch\n",
      "Epoch: 235/500...  Training Step: 52051...  Training loss: 0.8918...  Val loss: 1.7548...  0.3635 sec/batch\n",
      "Epoch: 235/500...  Training Step: 52076...  Training loss: 0.8926...  Val loss: 1.7755...  0.3629 sec/batch\n",
      "Epoch: 235/500...  Training Step: 52101...  Training loss: 0.8980...  Val loss: 1.7501...  0.3629 sec/batch\n",
      "Epoch: 235/500...  Training Step: 52126...  Training loss: 0.8973...  Val loss: 1.7698...  0.3634 sec/batch\n",
      "Epoch: 235/500...  Training Step: 52151...  Training loss: 0.9125...  Val loss: 1.7636...  0.3636 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52176...  Training loss: 0.9246...  Val loss: 1.7425...  0.3636 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52201...  Training loss: 0.8815...  Val loss: 1.7471...  0.3637 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52226...  Training loss: 0.9065...  Val loss: 1.7616...  0.3636 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52251...  Training loss: 0.9004...  Val loss: 1.7563...  0.3634 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52276...  Training loss: 0.8958...  Val loss: 1.7590...  0.3636 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52301...  Training loss: 0.8966...  Val loss: 1.7568...  0.3636 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52326...  Training loss: 0.9069...  Val loss: 1.7651...  0.3630 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52351...  Training loss: 0.8999...  Val loss: 1.7587...  0.3635 sec/batch\n",
      "Epoch: 236/500...  Training Step: 52376...  Training loss: 0.8887...  Val loss: 1.7681...  0.3636 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52401...  Training loss: 0.9100...  Val loss: 1.7629...  0.3634 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52426...  Training loss: 0.9110...  Val loss: 1.7588...  0.3634 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52451...  Training loss: 0.9065...  Val loss: 1.7758...  0.3634 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52476...  Training loss: 0.9193...  Val loss: 1.7615...  0.3630 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52501...  Training loss: 0.9076...  Val loss: 1.7352...  0.3636 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52526...  Training loss: 0.8858...  Val loss: 1.7589...  0.3633 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52551...  Training loss: 0.9048...  Val loss: 1.7904...  0.3640 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52576...  Training loss: 0.9175...  Val loss: 1.7636...  0.3634 sec/batch\n",
      "Epoch: 237/500...  Training Step: 52601...  Training loss: 0.8991...  Val loss: 1.7619...  0.3634 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52626...  Training loss: 0.9105...  Val loss: 1.7636...  0.3633 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52651...  Training loss: 0.9102...  Val loss: 1.7553...  0.3634 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52676...  Training loss: 0.9120...  Val loss: 1.7697...  0.3637 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52701...  Training loss: 0.9135...  Val loss: 1.7519...  0.3633 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52726...  Training loss: 0.8940...  Val loss: 1.7529...  0.3627 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52751...  Training loss: 0.8820...  Val loss: 1.7615...  0.3635 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52776...  Training loss: 0.9081...  Val loss: 1.7759...  0.3632 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52801...  Training loss: 0.8936...  Val loss: 1.7794...  0.3633 sec/batch\n",
      "Epoch: 238/500...  Training Step: 52826...  Training loss: 0.8837...  Val loss: 1.7509...  0.3634 sec/batch\n",
      "Epoch: 239/500...  Training Step: 52851...  Training loss: 0.9114...  Val loss: 1.7615...  0.3636 sec/batch\n",
      "Epoch: 239/500...  Training Step: 52876...  Training loss: 0.8895...  Val loss: 1.7568...  0.3635 sec/batch\n",
      "Epoch: 239/500...  Training Step: 52901...  Training loss: 0.9180...  Val loss: 1.7743...  0.3635 sec/batch\n",
      "Epoch: 239/500...  Training Step: 52926...  Training loss: 0.8883...  Val loss: 1.7833...  0.3636 sec/batch\n",
      "Epoch: 239/500...  Training Step: 52951...  Training loss: 0.9134...  Val loss: 1.7555...  0.3638 sec/batch\n",
      "Epoch: 239/500...  Training Step: 52976...  Training loss: 0.8945...  Val loss: 1.7609...  0.3633 sec/batch\n",
      "Epoch: 239/500...  Training Step: 53001...  Training loss: 0.8961...  Val loss: 1.7593...  0.3631 sec/batch\n",
      "Epoch: 239/500...  Training Step: 53026...  Training loss: 0.9159...  Val loss: 1.7847...  0.3632 sec/batch\n",
      "Epoch: 239/500...  Training Step: 53051...  Training loss: 0.9059...  Val loss: 1.7946...  0.3636 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53076...  Training loss: 0.9043...  Val loss: 1.7609...  0.3632 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53101...  Training loss: 0.8908...  Val loss: 1.7669...  0.3631 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53126...  Training loss: 0.9259...  Val loss: 1.7689...  0.3638 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53151...  Training loss: 0.8856...  Val loss: 1.7685...  0.3635 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53176...  Training loss: 0.8921...  Val loss: 1.7742...  0.3639 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53201...  Training loss: 0.8990...  Val loss: 1.7704...  0.3635 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53226...  Training loss: 0.9006...  Val loss: 1.7782...  0.3638 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53251...  Training loss: 0.8905...  Val loss: 1.7903...  0.3630 sec/batch\n",
      "Epoch: 240/500...  Training Step: 53276...  Training loss: 0.9203...  Val loss: 1.7773...  0.3635 sec/batch\n",
      "Epoch 240/500 time:81.5801510810852...  finished at 2017-10-30 15:05:34\n",
      "Epoch: 241/500...  Training Step: 53301...  Training loss: 0.9095...  Val loss: 1.7774...  0.3632 sec/batch\n",
      "Epoch: 241/500...  Training Step: 53326...  Training loss: 0.8917...  Val loss: 1.7709...  0.3629 sec/batch\n",
      "Epoch: 241/500...  Training Step: 53351...  Training loss: 0.9081...  Val loss: 1.7720...  0.3632 sec/batch\n",
      "Epoch: 241/500...  Training Step: 53376...  Training loss: 0.8953...  Val loss: 1.7717...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 241/500...  Training Step: 53401...  Training loss: 0.8868...  Val loss: 1.7791...  0.3635 sec/batch\n",
      "Epoch: 241/500...  Training Step: 53426...  Training loss: 0.9136...  Val loss: 1.7764...  0.3632 sec/batch\n",
      "Epoch: 241/500...  Training Step: 53451...  Training loss: 0.9383...  Val loss: 1.7832...  0.3634 sec/batch\n",
      "Epoch: 241/500...  Training Step: 53476...  Training loss: 0.8936...  Val loss: 1.7647...  0.3631 sec/batch\n",
      "Epoch: 241/500...  Training Step: 53501...  Training loss: 0.8986...  Val loss: 1.7774...  0.3633 sec/batch\n",
      "Epoch: 242/500...  Training Step: 53526...  Training loss: 0.9056...  Val loss: 1.7720...  0.3634 sec/batch\n",
      "Epoch: 242/500...  Training Step: 53551...  Training loss: 0.8945...  Val loss: 1.7797...  0.3631 sec/batch\n",
      "Epoch: 242/500...  Training Step: 53576...  Training loss: 0.8940...  Val loss: 1.7704...  0.3630 sec/batch\n",
      "Epoch: 242/500...  Training Step: 53601...  Training loss: 0.9017...  Val loss: 1.7826...  0.3637 sec/batch\n",
      "Epoch: 242/500...  Training Step: 53626...  Training loss: 0.8857...  Val loss: 1.7949...  0.3638 sec/batch\n",
      "Epoch: 242/500...  Training Step: 53651...  Training loss: 0.9030...  Val loss: 1.7637...  0.3640 sec/batch\n",
      "Epoch: 242/500...  Training Step: 53676...  Training loss: 0.9049...  Val loss: 1.7511...  0.3622 sec/batch\n",
      "Epoch: 242/500...  Training Step: 53701...  Training loss: 0.8882...  Val loss: 1.7897...  0.3631 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53726...  Training loss: 0.8820...  Val loss: 1.7753...  0.3632 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53751...  Training loss: 0.8885...  Val loss: 1.7796...  0.3633 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53776...  Training loss: 0.9030...  Val loss: 1.7679...  0.3635 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53801...  Training loss: 0.8829...  Val loss: 1.7717...  0.3632 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53826...  Training loss: 0.9139...  Val loss: 1.7795...  0.3635 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53851...  Training loss: 0.9061...  Val loss: 1.8096...  0.3636 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53876...  Training loss: 0.8862...  Val loss: 1.7738...  0.3632 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53901...  Training loss: 0.8780...  Val loss: 1.7766...  0.3635 sec/batch\n",
      "Epoch: 243/500...  Training Step: 53926...  Training loss: 0.8647...  Val loss: 1.7752...  0.3635 sec/batch\n",
      "Epoch: 244/500...  Training Step: 53951...  Training loss: 0.9024...  Val loss: 1.7765...  0.3631 sec/batch\n",
      "Epoch: 244/500...  Training Step: 53976...  Training loss: 0.8797...  Val loss: 1.7792...  0.3638 sec/batch\n",
      "Epoch: 244/500...  Training Step: 54001...  Training loss: 0.8724...  Val loss: 1.7814...  0.3629 sec/batch\n",
      "Epoch: 244/500...  Training Step: 54026...  Training loss: 0.8995...  Val loss: 1.7784...  0.3632 sec/batch\n",
      "Epoch: 244/500...  Training Step: 54051...  Training loss: 0.9060...  Val loss: 1.7633...  0.3634 sec/batch\n",
      "Epoch: 244/500...  Training Step: 54076...  Training loss: 0.8898...  Val loss: 1.7922...  0.3636 sec/batch\n",
      "Epoch: 244/500...  Training Step: 54101...  Training loss: 0.8763...  Val loss: 1.7947...  0.3632 sec/batch\n",
      "Epoch: 244/500...  Training Step: 54126...  Training loss: 0.8943...  Val loss: 1.7761...  0.3629 sec/batch\n",
      "Epoch: 244/500...  Training Step: 54151...  Training loss: 0.8854...  Val loss: 1.7695...  0.3635 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54176...  Training loss: 0.8968...  Val loss: 1.7428...  0.3636 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54201...  Training loss: 0.8956...  Val loss: 1.7694...  0.3633 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54226...  Training loss: 0.8838...  Val loss: 1.8036...  0.3637 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54251...  Training loss: 0.8904...  Val loss: 1.7763...  0.3633 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54276...  Training loss: 0.8711...  Val loss: 1.7805...  0.3634 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54301...  Training loss: 0.8840...  Val loss: 1.7896...  0.3633 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54326...  Training loss: 0.9021...  Val loss: 1.8026...  0.3633 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54351...  Training loss: 0.8943...  Val loss: 1.7744...  0.3630 sec/batch\n",
      "Epoch: 245/500...  Training Step: 54376...  Training loss: 0.8942...  Val loss: 1.7843...  0.3638 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54401...  Training loss: 0.8947...  Val loss: 1.7692...  0.3634 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54426...  Training loss: 0.8795...  Val loss: 1.7717...  0.3634 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54451...  Training loss: 0.8932...  Val loss: 1.7856...  0.3638 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54476...  Training loss: 0.8866...  Val loss: 1.7712...  0.3631 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54501...  Training loss: 0.9094...  Val loss: 1.7705...  0.3633 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54526...  Training loss: 0.8791...  Val loss: 1.7829...  0.3637 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54551...  Training loss: 0.8934...  Val loss: 1.7839...  0.3633 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54576...  Training loss: 0.9117...  Val loss: 1.7891...  0.3631 sec/batch\n",
      "Epoch: 246/500...  Training Step: 54601...  Training loss: 0.8786...  Val loss: 1.7712...  0.3634 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54626...  Training loss: 0.8817...  Val loss: 1.7695...  0.3630 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54651...  Training loss: 0.8887...  Val loss: 1.7772...  0.3630 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54676...  Training loss: 0.9041...  Val loss: 1.7786...  0.3632 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54701...  Training loss: 0.8904...  Val loss: 1.7861...  0.3632 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54726...  Training loss: 0.8875...  Val loss: 1.7811...  0.3632 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54751...  Training loss: 0.8950...  Val loss: 1.7819...  0.3636 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54776...  Training loss: 0.8934...  Val loss: 1.8034...  0.3631 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54801...  Training loss: 0.8861...  Val loss: 1.7914...  0.3638 sec/batch\n",
      "Epoch: 247/500...  Training Step: 54826...  Training loss: 0.8878...  Val loss: 1.7836...  0.3634 sec/batch\n",
      "Epoch: 248/500...  Training Step: 54851...  Training loss: 0.9057...  Val loss: 1.7750...  0.3634 sec/batch\n",
      "Epoch: 248/500...  Training Step: 54876...  Training loss: 0.8931...  Val loss: 1.7649...  0.3633 sec/batch\n",
      "Epoch: 248/500...  Training Step: 54901...  Training loss: 0.8759...  Val loss: 1.7846...  0.3636 sec/batch\n",
      "Epoch: 248/500...  Training Step: 54926...  Training loss: 0.8774...  Val loss: 1.7769...  0.3638 sec/batch\n",
      "Epoch: 248/500...  Training Step: 54951...  Training loss: 0.8891...  Val loss: 1.7862...  0.3632 sec/batch\n",
      "Epoch: 248/500...  Training Step: 54976...  Training loss: 0.8787...  Val loss: 1.7669...  0.3631 sec/batch\n",
      "Epoch: 248/500...  Training Step: 55001...  Training loss: 0.8785...  Val loss: 1.7865...  0.3637 sec/batch\n",
      "Epoch: 248/500...  Training Step: 55026...  Training loss: 0.8889...  Val loss: 1.7970...  0.3633 sec/batch\n",
      "Epoch: 248/500...  Training Step: 55051...  Training loss: 0.8822...  Val loss: 1.7853...  0.3630 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55076...  Training loss: 0.8868...  Val loss: 1.7581...  0.3639 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55101...  Training loss: 0.8753...  Val loss: 1.7779...  0.3633 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55126...  Training loss: 0.8952...  Val loss: 1.7788...  0.3635 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55151...  Training loss: 0.8855...  Val loss: 1.7855...  0.3634 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55176...  Training loss: 0.8784...  Val loss: 1.7755...  0.3629 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55201...  Training loss: 0.9042...  Val loss: 1.7815...  0.3633 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55226...  Training loss: 0.8854...  Val loss: 1.7863...  0.3633 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55251...  Training loss: 0.8765...  Val loss: 1.7976...  0.3633 sec/batch\n",
      "Epoch: 249/500...  Training Step: 55276...  Training loss: 0.8778...  Val loss: 1.7939...  0.3639 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250/500...  Training Step: 55301...  Training loss: 0.8927...  Val loss: 1.7805...  0.3630 sec/batch\n",
      "Epoch: 250/500...  Training Step: 55326...  Training loss: 0.8725...  Val loss: 1.7843...  0.3632 sec/batch\n",
      "Epoch: 250/500...  Training Step: 55351...  Training loss: 0.8864...  Val loss: 1.7724...  0.3635 sec/batch\n",
      "Epoch: 250/500...  Training Step: 55376...  Training loss: 0.8741...  Val loss: 1.7816...  0.3634 sec/batch\n",
      "Epoch: 250/500...  Training Step: 55401...  Training loss: 0.8891...  Val loss: 1.7986...  0.3633 sec/batch\n",
      "Epoch: 250/500...  Training Step: 55426...  Training loss: 0.9057...  Val loss: 1.7737...  0.3635 sec/batch\n",
      "Epoch: 250/500...  Training Step: 55451...  Training loss: 0.8984...  Val loss: 1.7908...  0.3635 sec/batch\n",
      "Epoch: 250/500...  Training Step: 55476...  Training loss: 0.8872...  Val loss: 1.7865...  0.3631 sec/batch\n",
      "Epoch 250/500 time:81.48121356964111...  finished at 2017-10-30 15:19:11\n",
      "Epoch: 251/500...  Training Step: 55501...  Training loss: 0.9813...  Val loss: 1.7824...  0.3633 sec/batch\n",
      "Epoch: 251/500...  Training Step: 55526...  Training loss: 0.8967...  Val loss: 1.7830...  0.3631 sec/batch\n",
      "Epoch: 251/500...  Training Step: 55551...  Training loss: 0.8643...  Val loss: 1.7974...  0.3632 sec/batch\n",
      "Epoch: 251/500...  Training Step: 55576...  Training loss: 0.8799...  Val loss: 1.7803...  0.3634 sec/batch\n",
      "Epoch: 251/500...  Training Step: 55601...  Training loss: 0.8960...  Val loss: 1.7935...  0.3631 sec/batch\n",
      "Epoch: 251/500...  Training Step: 55626...  Training loss: 0.8845...  Val loss: 1.8104...  0.3635 sec/batch\n",
      "Epoch: 251/500...  Training Step: 55651...  Training loss: 0.8872...  Val loss: 1.7962...  0.3633 sec/batch\n",
      "Epoch: 251/500...  Training Step: 55676...  Training loss: 0.8763...  Val loss: 1.7920...  0.3637 sec/batch\n",
      "Epoch: 251/500...  Training Step: 55701...  Training loss: 0.8845...  Val loss: 1.7999...  0.3636 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55726...  Training loss: 0.8959...  Val loss: 1.7769...  0.3635 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55751...  Training loss: 0.8812...  Val loss: 1.7850...  0.3630 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55776...  Training loss: 0.8721...  Val loss: 1.7982...  0.3632 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55801...  Training loss: 0.8761...  Val loss: 1.7950...  0.3633 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55826...  Training loss: 0.8724...  Val loss: 1.7865...  0.3634 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55851...  Training loss: 0.8865...  Val loss: 1.7944...  0.3634 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55876...  Training loss: 0.8819...  Val loss: 1.7986...  0.3633 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55901...  Training loss: 0.8766...  Val loss: 1.8015...  0.3633 sec/batch\n",
      "Epoch: 252/500...  Training Step: 55926...  Training loss: 0.8754...  Val loss: 1.7978...  0.3633 sec/batch\n",
      "Epoch: 253/500...  Training Step: 55951...  Training loss: 0.8991...  Val loss: 1.7888...  0.3633 sec/batch\n",
      "Epoch: 253/500...  Training Step: 55976...  Training loss: 0.8886...  Val loss: 1.8092...  0.3638 sec/batch\n",
      "Epoch: 253/500...  Training Step: 56001...  Training loss: 0.8589...  Val loss: 1.8047...  0.3749 sec/batch\n",
      "Epoch: 253/500...  Training Step: 56026...  Training loss: 0.8773...  Val loss: 1.7771...  0.3635 sec/batch\n",
      "Epoch: 253/500...  Training Step: 56051...  Training loss: 0.8627...  Val loss: 1.7814...  0.3637 sec/batch\n",
      "Epoch: 253/500...  Training Step: 56076...  Training loss: 0.8784...  Val loss: 1.8047...  0.3637 sec/batch\n",
      "Epoch: 253/500...  Training Step: 56101...  Training loss: 0.8828...  Val loss: 1.8132...  0.3630 sec/batch\n",
      "Epoch: 253/500...  Training Step: 56126...  Training loss: 0.8881...  Val loss: 1.7956...  0.3638 sec/batch\n",
      "Epoch: 253/500...  Training Step: 56151...  Training loss: 0.8799...  Val loss: 1.8099...  0.3634 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56176...  Training loss: 0.8702...  Val loss: 1.7853...  0.3631 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56201...  Training loss: 0.8810...  Val loss: 1.7886...  0.3645 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56226...  Training loss: 0.8829...  Val loss: 1.7943...  0.3633 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56251...  Training loss: 0.9014...  Val loss: 1.7757...  0.3641 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56276...  Training loss: 0.8930...  Val loss: 1.7787...  0.3632 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56301...  Training loss: 0.8846...  Val loss: 1.8061...  0.3633 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56326...  Training loss: 0.8829...  Val loss: 1.7847...  0.3633 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56351...  Training loss: 0.8636...  Val loss: 1.8137...  0.3635 sec/batch\n",
      "Epoch: 254/500...  Training Step: 56376...  Training loss: 0.8664...  Val loss: 1.7827...  0.3637 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56401...  Training loss: 0.8899...  Val loss: 1.7782...  0.3633 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56426...  Training loss: 0.8769...  Val loss: 1.7862...  0.3631 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56451...  Training loss: 0.8954...  Val loss: 1.7927...  0.3631 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56476...  Training loss: 0.8981...  Val loss: 1.7922...  0.3637 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56501...  Training loss: 0.8766...  Val loss: 1.7766...  0.3652 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56526...  Training loss: 0.8757...  Val loss: 1.7874...  0.3636 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56551...  Training loss: 0.8843...  Val loss: 1.7917...  0.3632 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56576...  Training loss: 0.8674...  Val loss: 1.8041...  0.3661 sec/batch\n",
      "Epoch: 255/500...  Training Step: 56601...  Training loss: 0.9026...  Val loss: 1.7959...  0.3634 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56626...  Training loss: 0.8570...  Val loss: 1.7897...  0.3632 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56651...  Training loss: 0.8603...  Val loss: 1.7805...  0.3635 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56676...  Training loss: 0.8733...  Val loss: 1.8058...  0.3629 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56701...  Training loss: 0.8741...  Val loss: 1.7912...  0.3657 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56726...  Training loss: 0.8753...  Val loss: 1.7876...  0.3631 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56751...  Training loss: 0.8739...  Val loss: 1.7992...  0.3634 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56776...  Training loss: 0.8888...  Val loss: 1.7864...  0.3632 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56801...  Training loss: 0.8744...  Val loss: 1.8087...  0.3635 sec/batch\n",
      "Epoch: 256/500...  Training Step: 56826...  Training loss: 0.8809...  Val loss: 1.7903...  0.3663 sec/batch\n",
      "Epoch: 257/500...  Training Step: 56851...  Training loss: 0.8899...  Val loss: 1.7980...  0.3633 sec/batch\n",
      "Epoch: 257/500...  Training Step: 56876...  Training loss: 0.8881...  Val loss: 1.7824...  0.3635 sec/batch\n",
      "Epoch: 257/500...  Training Step: 56901...  Training loss: 0.8830...  Val loss: 1.7995...  0.3629 sec/batch\n",
      "Epoch: 257/500...  Training Step: 56926...  Training loss: 0.8775...  Val loss: 1.8024...  0.3636 sec/batch\n",
      "Epoch: 257/500...  Training Step: 56951...  Training loss: 0.8698...  Val loss: 1.8056...  0.3661 sec/batch\n",
      "Epoch: 257/500...  Training Step: 56976...  Training loss: 0.8974...  Val loss: 1.8077...  0.3641 sec/batch\n",
      "Epoch: 257/500...  Training Step: 57001...  Training loss: 0.8680...  Val loss: 1.8187...  0.3632 sec/batch\n",
      "Epoch: 257/500...  Training Step: 57026...  Training loss: 0.8825...  Val loss: 1.8220...  0.3657 sec/batch\n",
      "Epoch: 257/500...  Training Step: 57051...  Training loss: 0.8937...  Val loss: 1.8072...  0.3633 sec/batch\n",
      "Epoch: 258/500...  Training Step: 57076...  Training loss: 0.8551...  Val loss: 1.7997...  0.3631 sec/batch\n",
      "Epoch: 258/500...  Training Step: 57101...  Training loss: 0.8784...  Val loss: 1.7951...  0.3633 sec/batch\n",
      "Epoch: 258/500...  Training Step: 57126...  Training loss: 0.8677...  Val loss: 1.8076...  0.3633 sec/batch\n",
      "Epoch: 258/500...  Training Step: 57151...  Training loss: 0.8595...  Val loss: 1.8095...  0.3667 sec/batch\n",
      "Epoch: 258/500...  Training Step: 57176...  Training loss: 0.8886...  Val loss: 1.7964...  0.3631 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 258/500...  Training Step: 57201...  Training loss: 0.8785...  Val loss: 1.7736...  0.3634 sec/batch\n",
      "Epoch: 258/500...  Training Step: 57226...  Training loss: 0.8871...  Val loss: 1.8047...  0.3636 sec/batch\n",
      "Epoch: 258/500...  Training Step: 57251...  Training loss: 0.8879...  Val loss: 1.8123...  0.3632 sec/batch\n",
      "Epoch: 258/500...  Training Step: 57276...  Training loss: 0.8692...  Val loss: 1.8111...  0.3669 sec/batch\n",
      "Epoch: 259/500...  Training Step: 57301...  Training loss: 0.8476...  Val loss: 1.8075...  0.3634 sec/batch\n",
      "Epoch: 259/500...  Training Step: 57326...  Training loss: 0.8830...  Val loss: 1.8141...  0.3635 sec/batch\n",
      "Epoch: 259/500...  Training Step: 57351...  Training loss: 0.9147...  Val loss: 1.7957...  0.3626 sec/batch\n",
      "Epoch: 259/500...  Training Step: 57376...  Training loss: 0.8765...  Val loss: 1.8159...  0.3636 sec/batch\n",
      "Epoch: 259/500...  Training Step: 57401...  Training loss: 0.8678...  Val loss: 1.8238...  0.3669 sec/batch\n",
      "Epoch: 259/500...  Training Step: 57426...  Training loss: 0.8846...  Val loss: 1.7948...  0.3632 sec/batch\n",
      "Epoch: 259/500...  Training Step: 57451...  Training loss: 0.8628...  Val loss: 1.7924...  0.3639 sec/batch\n",
      "Epoch: 259/500...  Training Step: 57476...  Training loss: 0.8720...  Val loss: 1.8205...  0.3636 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57501...  Training loss: 0.8786...  Val loss: 1.7937...  0.3635 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57526...  Training loss: 0.8743...  Val loss: 1.7930...  0.3633 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57551...  Training loss: 0.8585...  Val loss: 1.8035...  0.3633 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57576...  Training loss: 0.8602...  Val loss: 1.8110...  0.3633 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57601...  Training loss: 0.8595...  Val loss: 1.8107...  0.3671 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57626...  Training loss: 0.8714...  Val loss: 1.8256...  0.3634 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57651...  Training loss: 0.8652...  Val loss: 1.7936...  0.3636 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57676...  Training loss: 0.8602...  Val loss: 1.8209...  0.3632 sec/batch\n",
      "Epoch: 260/500...  Training Step: 57701...  Training loss: 0.8884...  Val loss: 1.8061...  0.3632 sec/batch\n",
      "Epoch 260/500 time:82.03067541122437...  finished at 2017-10-30 15:32:50\n",
      "Epoch: 261/500...  Training Step: 57726...  Training loss: 0.8902...  Val loss: 1.7934...  0.3631 sec/batch\n",
      "Epoch: 261/500...  Training Step: 57751...  Training loss: 0.8524...  Val loss: 1.7981...  0.3633 sec/batch\n",
      "Epoch: 261/500...  Training Step: 57776...  Training loss: 0.8716...  Val loss: 1.8022...  0.3633 sec/batch\n",
      "Epoch: 261/500...  Training Step: 57801...  Training loss: 0.8684...  Val loss: 1.7935...  0.3632 sec/batch\n",
      "Epoch: 261/500...  Training Step: 57826...  Training loss: 0.8700...  Val loss: 1.8070...  0.3631 sec/batch\n",
      "Epoch: 261/500...  Training Step: 57851...  Training loss: 0.8520...  Val loss: 1.8014...  0.3635 sec/batch\n",
      "Epoch: 261/500...  Training Step: 57876...  Training loss: 0.8737...  Val loss: 1.8063...  0.3631 sec/batch\n",
      "Epoch: 261/500...  Training Step: 57901...  Training loss: 0.8768...  Val loss: 1.8070...  0.3633 sec/batch\n",
      "Epoch: 261/500...  Training Step: 57926...  Training loss: 0.8609...  Val loss: 1.8193...  0.3631 sec/batch\n",
      "Epoch: 262/500...  Training Step: 57951...  Training loss: 0.8768...  Val loss: 1.8016...  0.3632 sec/batch\n",
      "Epoch: 262/500...  Training Step: 57976...  Training loss: 0.8689...  Val loss: 1.8178...  0.3639 sec/batch\n",
      "Epoch: 262/500...  Training Step: 58001...  Training loss: 0.8733...  Val loss: 1.8205...  0.3635 sec/batch\n",
      "Epoch: 262/500...  Training Step: 58026...  Training loss: 0.8755...  Val loss: 1.8012...  0.3634 sec/batch\n",
      "Epoch: 262/500...  Training Step: 58051...  Training loss: 0.8672...  Val loss: 1.7957...  0.3632 sec/batch\n",
      "Epoch: 262/500...  Training Step: 58076...  Training loss: 0.8578...  Val loss: 1.8051...  0.3634 sec/batch\n",
      "Epoch: 262/500...  Training Step: 58101...  Training loss: 0.8786...  Val loss: 1.8283...  0.3632 sec/batch\n",
      "Epoch: 262/500...  Training Step: 58126...  Training loss: 0.8928...  Val loss: 1.8110...  0.3638 sec/batch\n",
      "Epoch: 262/500...  Training Step: 58151...  Training loss: 0.8637...  Val loss: 1.8107...  0.3638 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58176...  Training loss: 0.8692...  Val loss: 1.8063...  0.3634 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58201...  Training loss: 0.8658...  Val loss: 1.8043...  0.3632 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58226...  Training loss: 0.8719...  Val loss: 1.8108...  0.3634 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58251...  Training loss: 0.8774...  Val loss: 1.7956...  0.3632 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58276...  Training loss: 0.8663...  Val loss: 1.7996...  0.3633 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58301...  Training loss: 0.8449...  Val loss: 1.8061...  0.3633 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58326...  Training loss: 0.8873...  Val loss: 1.8291...  0.3630 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58351...  Training loss: 0.8618...  Val loss: 1.8197...  0.3634 sec/batch\n",
      "Epoch: 263/500...  Training Step: 58376...  Training loss: 0.8650...  Val loss: 1.8078...  0.3635 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58401...  Training loss: 0.8779...  Val loss: 1.7991...  0.3631 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58426...  Training loss: 0.8543...  Val loss: 1.7999...  0.3633 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58451...  Training loss: 0.8764...  Val loss: 1.8157...  0.3640 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58476...  Training loss: 0.8556...  Val loss: 1.8203...  0.3635 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58501...  Training loss: 0.8781...  Val loss: 1.8040...  0.3638 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58526...  Training loss: 0.8609...  Val loss: 1.7997...  0.3637 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58551...  Training loss: 0.8489...  Val loss: 1.8043...  0.3634 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58576...  Training loss: 0.8815...  Val loss: 1.8259...  0.3631 sec/batch\n",
      "Epoch: 264/500...  Training Step: 58601...  Training loss: 0.8766...  Val loss: 1.8402...  0.3630 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58626...  Training loss: 0.8623...  Val loss: 1.7986...  0.3637 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58651...  Training loss: 0.8559...  Val loss: 1.8202...  0.3634 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58676...  Training loss: 0.8781...  Val loss: 1.8205...  0.3635 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58701...  Training loss: 0.8444...  Val loss: 1.8048...  0.3641 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58726...  Training loss: 0.8796...  Val loss: 1.8094...  0.3633 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58751...  Training loss: 0.8784...  Val loss: 1.8163...  0.3630 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58776...  Training loss: 0.8730...  Val loss: 1.8159...  0.3636 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58801...  Training loss: 0.8546...  Val loss: 1.8302...  0.3632 sec/batch\n",
      "Epoch: 265/500...  Training Step: 58826...  Training loss: 0.8797...  Val loss: 1.8254...  0.3633 sec/batch\n",
      "Epoch: 266/500...  Training Step: 58851...  Training loss: 0.8837...  Val loss: 1.8188...  0.3636 sec/batch\n",
      "Epoch: 266/500...  Training Step: 58876...  Training loss: 0.8625...  Val loss: 1.8120...  0.3630 sec/batch\n",
      "Epoch: 266/500...  Training Step: 58901...  Training loss: 0.8838...  Val loss: 1.8124...  0.3627 sec/batch\n",
      "Epoch: 266/500...  Training Step: 58926...  Training loss: 0.8722...  Val loss: 1.8201...  0.3635 sec/batch\n",
      "Epoch: 266/500...  Training Step: 58951...  Training loss: 0.8655...  Val loss: 1.8059...  0.3632 sec/batch\n",
      "Epoch: 266/500...  Training Step: 58976...  Training loss: 0.8812...  Val loss: 1.8258...  0.3635 sec/batch\n",
      "Epoch: 266/500...  Training Step: 59001...  Training loss: 0.9012...  Val loss: 1.8279...  0.3637 sec/batch\n",
      "Epoch: 266/500...  Training Step: 59026...  Training loss: 0.8552...  Val loss: 1.8093...  0.3630 sec/batch\n",
      "Epoch: 266/500...  Training Step: 59051...  Training loss: 0.8658...  Val loss: 1.8258...  0.3640 sec/batch\n",
      "Epoch: 267/500...  Training Step: 59076...  Training loss: 0.8739...  Val loss: 1.8250...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 267/500...  Training Step: 59101...  Training loss: 0.8691...  Val loss: 1.8322...  0.3630 sec/batch\n",
      "Epoch: 267/500...  Training Step: 59126...  Training loss: 0.8578...  Val loss: 1.8130...  0.3634 sec/batch\n",
      "Epoch: 267/500...  Training Step: 59151...  Training loss: 0.8639...  Val loss: 1.8321...  0.3634 sec/batch\n",
      "Epoch: 267/500...  Training Step: 59176...  Training loss: 0.8546...  Val loss: 1.8422...  0.3631 sec/batch\n",
      "Epoch: 267/500...  Training Step: 59201...  Training loss: 0.8767...  Val loss: 1.8130...  0.3628 sec/batch\n",
      "Epoch: 267/500...  Training Step: 59226...  Training loss: 0.8733...  Val loss: 1.8010...  0.3637 sec/batch\n",
      "Epoch: 267/500...  Training Step: 59251...  Training loss: 0.8693...  Val loss: 1.8396...  0.3632 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59276...  Training loss: 0.8572...  Val loss: 1.8173...  0.3633 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59301...  Training loss: 0.8579...  Val loss: 1.8223...  0.3635 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59326...  Training loss: 0.8676...  Val loss: 1.8062...  0.3631 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59351...  Training loss: 0.8550...  Val loss: 1.8198...  0.3630 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59376...  Training loss: 0.8778...  Val loss: 1.8230...  0.3644 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59401...  Training loss: 0.8780...  Val loss: 1.8447...  0.3631 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59426...  Training loss: 0.8644...  Val loss: 1.8129...  0.3640 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59451...  Training loss: 0.8473...  Val loss: 1.8222...  0.3623 sec/batch\n",
      "Epoch: 268/500...  Training Step: 59476...  Training loss: 0.8402...  Val loss: 1.8178...  0.3626 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59501...  Training loss: 0.8672...  Val loss: 1.8243...  0.3631 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59526...  Training loss: 0.8465...  Val loss: 1.8322...  0.3634 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59551...  Training loss: 0.8394...  Val loss: 1.8198...  0.3631 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59576...  Training loss: 0.8711...  Val loss: 1.8213...  0.3628 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59601...  Training loss: 0.8739...  Val loss: 1.8016...  0.3631 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59626...  Training loss: 0.8562...  Val loss: 1.8313...  0.3631 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59651...  Training loss: 0.8468...  Val loss: 1.8260...  0.3636 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59676...  Training loss: 0.8572...  Val loss: 1.8229...  0.3632 sec/batch\n",
      "Epoch: 269/500...  Training Step: 59701...  Training loss: 0.8619...  Val loss: 1.8253...  0.3636 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59726...  Training loss: 0.8741...  Val loss: 1.7986...  0.3632 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59751...  Training loss: 0.8639...  Val loss: 1.8205...  0.3634 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59776...  Training loss: 0.8598...  Val loss: 1.8488...  0.3626 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59801...  Training loss: 0.8572...  Val loss: 1.8275...  0.3634 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59826...  Training loss: 0.8524...  Val loss: 1.8369...  0.3636 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59851...  Training loss: 0.8678...  Val loss: 1.8367...  0.3629 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59876...  Training loss: 0.8743...  Val loss: 1.8372...  0.3630 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59901...  Training loss: 0.8533...  Val loss: 1.8225...  0.3633 sec/batch\n",
      "Epoch: 270/500...  Training Step: 59926...  Training loss: 0.8641...  Val loss: 1.8336...  0.3632 sec/batch\n",
      "Epoch 270/500 time:81.57320189476013...  finished at 2017-10-30 15:46:27\n",
      "Epoch: 271/500...  Training Step: 59951...  Training loss: 0.8561...  Val loss: 1.8195...  0.3635 sec/batch\n",
      "Epoch: 271/500...  Training Step: 59976...  Training loss: 0.8600...  Val loss: 1.8139...  0.3631 sec/batch\n",
      "Epoch: 271/500...  Training Step: 60001...  Training loss: 0.8582...  Val loss: 1.8262...  0.3633 sec/batch\n",
      "Epoch: 271/500...  Training Step: 60026...  Training loss: 0.8576...  Val loss: 1.8211...  0.3628 sec/batch\n",
      "Epoch: 271/500...  Training Step: 60051...  Training loss: 0.8674...  Val loss: 1.8085...  0.3631 sec/batch\n",
      "Epoch: 271/500...  Training Step: 60076...  Training loss: 0.8533...  Val loss: 1.8185...  0.3634 sec/batch\n",
      "Epoch: 271/500...  Training Step: 60101...  Training loss: 0.8776...  Val loss: 1.8235...  0.3636 sec/batch\n",
      "Epoch: 271/500...  Training Step: 60126...  Training loss: 0.8772...  Val loss: 1.8419...  0.3633 sec/batch\n",
      "Epoch: 271/500...  Training Step: 60151...  Training loss: 0.8574...  Val loss: 1.8326...  0.3636 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60176...  Training loss: 0.8497...  Val loss: 1.8192...  0.3631 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60201...  Training loss: 0.8583...  Val loss: 1.8278...  0.3748 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60226...  Training loss: 0.8634...  Val loss: 1.8162...  0.3636 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60251...  Training loss: 0.8613...  Val loss: 1.8341...  0.3637 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60276...  Training loss: 0.8635...  Val loss: 1.8244...  0.3637 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60301...  Training loss: 0.8832...  Val loss: 1.8302...  0.3631 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60326...  Training loss: 0.8683...  Val loss: 1.8480...  0.3634 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60351...  Training loss: 0.8525...  Val loss: 1.8361...  0.3634 sec/batch\n",
      "Epoch: 272/500...  Training Step: 60376...  Training loss: 0.8580...  Val loss: 1.8312...  0.3631 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60401...  Training loss: 0.8637...  Val loss: 1.8183...  0.3632 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60426...  Training loss: 0.8649...  Val loss: 1.8178...  0.3632 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60451...  Training loss: 0.8495...  Val loss: 1.8446...  0.3633 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60476...  Training loss: 0.8532...  Val loss: 1.8230...  0.3637 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60501...  Training loss: 0.8704...  Val loss: 1.8285...  0.3629 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60526...  Training loss: 0.8675...  Val loss: 1.8079...  0.3637 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60551...  Training loss: 0.8573...  Val loss: 1.8297...  0.3635 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60576...  Training loss: 0.8643...  Val loss: 1.8416...  0.3633 sec/batch\n",
      "Epoch: 273/500...  Training Step: 60601...  Training loss: 0.8451...  Val loss: 1.8406...  0.3632 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60626...  Training loss: 0.8567...  Val loss: 1.7995...  0.3632 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60651...  Training loss: 0.8462...  Val loss: 1.8201...  0.3631 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60676...  Training loss: 0.8652...  Val loss: 1.8184...  0.3632 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60701...  Training loss: 0.8515...  Val loss: 1.8267...  0.3636 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60726...  Training loss: 0.8511...  Val loss: 1.8218...  0.3634 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60751...  Training loss: 0.8553...  Val loss: 1.8200...  0.3637 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60776...  Training loss: 0.8570...  Val loss: 1.8294...  0.3632 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60801...  Training loss: 0.8427...  Val loss: 1.8411...  0.3634 sec/batch\n",
      "Epoch: 274/500...  Training Step: 60826...  Training loss: 0.8463...  Val loss: 1.8378...  0.3635 sec/batch\n",
      "Epoch: 275/500...  Training Step: 60851...  Training loss: 0.8634...  Val loss: 1.8267...  0.3634 sec/batch\n",
      "Epoch: 275/500...  Training Step: 60876...  Training loss: 0.8513...  Val loss: 1.8352...  0.3637 sec/batch\n",
      "Epoch: 275/500...  Training Step: 60901...  Training loss: 0.8536...  Val loss: 1.8108...  0.3635 sec/batch\n",
      "Epoch: 275/500...  Training Step: 60926...  Training loss: 0.8497...  Val loss: 1.8327...  0.3630 sec/batch\n",
      "Epoch: 275/500...  Training Step: 60951...  Training loss: 0.8565...  Val loss: 1.8398...  0.3632 sec/batch\n",
      "Epoch: 275/500...  Training Step: 60976...  Training loss: 0.8684...  Val loss: 1.8263...  0.3629 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275/500...  Training Step: 61001...  Training loss: 0.8626...  Val loss: 1.8305...  0.3633 sec/batch\n",
      "Epoch: 275/500...  Training Step: 61026...  Training loss: 0.8531...  Val loss: 1.8292...  0.3631 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61051...  Training loss: 0.9560...  Val loss: 1.8326...  0.3630 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61076...  Training loss: 0.8660...  Val loss: 1.8299...  0.3634 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61101...  Training loss: 0.8336...  Val loss: 1.8366...  0.3631 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61126...  Training loss: 0.8543...  Val loss: 1.8297...  0.3631 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61151...  Training loss: 0.8652...  Val loss: 1.8400...  0.3634 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61176...  Training loss: 0.8434...  Val loss: 1.8454...  0.3634 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61201...  Training loss: 0.8516...  Val loss: 1.8231...  0.3630 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61226...  Training loss: 0.8451...  Val loss: 1.8329...  0.3639 sec/batch\n",
      "Epoch: 276/500...  Training Step: 61251...  Training loss: 0.8574...  Val loss: 1.8399...  0.3634 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61276...  Training loss: 0.8663...  Val loss: 1.8283...  0.3628 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61301...  Training loss: 0.8583...  Val loss: 1.8339...  0.3632 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61326...  Training loss: 0.8494...  Val loss: 1.8344...  0.3638 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61351...  Training loss: 0.8414...  Val loss: 1.8438...  0.3632 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61376...  Training loss: 0.8522...  Val loss: 1.8437...  0.3636 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61401...  Training loss: 0.8563...  Val loss: 1.8408...  0.3634 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61426...  Training loss: 0.8565...  Val loss: 1.8357...  0.3635 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61451...  Training loss: 0.8522...  Val loss: 1.8397...  0.3631 sec/batch\n",
      "Epoch: 277/500...  Training Step: 61476...  Training loss: 0.8564...  Val loss: 1.8372...  0.3633 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61501...  Training loss: 0.8680...  Val loss: 1.8341...  0.3633 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61526...  Training loss: 0.8793...  Val loss: 1.8656...  0.3635 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61551...  Training loss: 0.8390...  Val loss: 1.8401...  0.3630 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61576...  Training loss: 0.8559...  Val loss: 1.8294...  0.3635 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61601...  Training loss: 0.8457...  Val loss: 1.8384...  0.3630 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61626...  Training loss: 0.8579...  Val loss: 1.8578...  0.3633 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61651...  Training loss: 0.8592...  Val loss: 1.8504...  0.3628 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61676...  Training loss: 0.8534...  Val loss: 1.8421...  0.3635 sec/batch\n",
      "Epoch: 278/500...  Training Step: 61701...  Training loss: 0.8480...  Val loss: 1.8496...  0.3630 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61726...  Training loss: 0.8347...  Val loss: 1.8189...  0.3636 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61751...  Training loss: 0.8576...  Val loss: 1.8408...  0.3629 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61776...  Training loss: 0.8544...  Val loss: 1.8350...  0.3638 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61801...  Training loss: 0.8714...  Val loss: 1.8237...  0.3633 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61826...  Training loss: 0.8677...  Val loss: 1.8225...  0.3638 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61851...  Training loss: 0.8524...  Val loss: 1.8542...  0.3631 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61876...  Training loss: 0.8577...  Val loss: 1.8336...  0.3635 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61901...  Training loss: 0.8438...  Val loss: 1.8504...  0.3631 sec/batch\n",
      "Epoch: 279/500...  Training Step: 61926...  Training loss: 0.8382...  Val loss: 1.8270...  0.3626 sec/batch\n",
      "Epoch: 280/500...  Training Step: 61951...  Training loss: 0.8564...  Val loss: 1.8276...  0.3629 sec/batch\n",
      "Epoch: 280/500...  Training Step: 61976...  Training loss: 0.8436...  Val loss: 1.8390...  0.3628 sec/batch\n",
      "Epoch: 280/500...  Training Step: 62001...  Training loss: 0.8592...  Val loss: 1.8483...  0.3628 sec/batch\n",
      "Epoch: 280/500...  Training Step: 62026...  Training loss: 0.8578...  Val loss: 1.8365...  0.3630 sec/batch\n",
      "Epoch: 280/500...  Training Step: 62051...  Training loss: 0.8434...  Val loss: 1.8284...  0.3634 sec/batch\n",
      "Epoch: 280/500...  Training Step: 62076...  Training loss: 0.8556...  Val loss: 1.8312...  0.3632 sec/batch\n",
      "Epoch: 280/500...  Training Step: 62101...  Training loss: 0.8571...  Val loss: 1.8518...  0.3635 sec/batch\n",
      "Epoch: 280/500...  Training Step: 62126...  Training loss: 0.8495...  Val loss: 1.8585...  0.3635 sec/batch\n",
      "Epoch: 280/500...  Training Step: 62151...  Training loss: 0.8684...  Val loss: 1.8477...  0.3631 sec/batch\n",
      "Epoch 280/500 time:81.92374801635742...  finished at 2017-10-30 16:00:05\n",
      "Epoch: 281/500...  Training Step: 62176...  Training loss: 0.8186...  Val loss: 1.8300...  0.3632 sec/batch\n",
      "Epoch: 281/500...  Training Step: 62201...  Training loss: 0.8288...  Val loss: 1.8439...  0.3636 sec/batch\n",
      "Epoch: 281/500...  Training Step: 62226...  Training loss: 0.8447...  Val loss: 1.8382...  0.3633 sec/batch\n",
      "Epoch: 281/500...  Training Step: 62251...  Training loss: 0.8371...  Val loss: 1.8344...  0.3635 sec/batch\n",
      "Epoch: 281/500...  Training Step: 62276...  Training loss: 0.8521...  Val loss: 1.8253...  0.3632 sec/batch\n",
      "Epoch: 281/500...  Training Step: 62301...  Training loss: 0.8333...  Val loss: 1.8356...  0.3635 sec/batch\n",
      "Epoch: 281/500...  Training Step: 62326...  Training loss: 0.8558...  Val loss: 1.8291...  0.3635 sec/batch\n",
      "Epoch: 281/500...  Training Step: 62351...  Training loss: 0.8427...  Val loss: 1.8480...  0.3637 sec/batch\n",
      "Epoch: 281/500...  Training Step: 62376...  Training loss: 0.8461...  Val loss: 1.8321...  0.3632 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62401...  Training loss: 0.8570...  Val loss: 1.8338...  0.3634 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62426...  Training loss: 0.8549...  Val loss: 1.8332...  0.3638 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62451...  Training loss: 0.8509...  Val loss: 1.8452...  0.3633 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62476...  Training loss: 0.8475...  Val loss: 1.8302...  0.3634 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62501...  Training loss: 0.8573...  Val loss: 1.8542...  0.3631 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62526...  Training loss: 0.8614...  Val loss: 1.8437...  0.3633 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62551...  Training loss: 0.8440...  Val loss: 1.8556...  0.3628 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62576...  Training loss: 0.8519...  Val loss: 1.8630...  0.3632 sec/batch\n",
      "Epoch: 282/500...  Training Step: 62601...  Training loss: 0.8554...  Val loss: 1.8570...  0.3633 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62626...  Training loss: 0.8385...  Val loss: 1.8285...  0.3635 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62651...  Training loss: 0.8474...  Val loss: 1.8459...  0.3634 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62676...  Training loss: 0.8416...  Val loss: 1.8547...  0.3636 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62701...  Training loss: 0.8412...  Val loss: 1.8412...  0.3632 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62726...  Training loss: 0.8558...  Val loss: 1.8352...  0.3629 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62751...  Training loss: 0.8510...  Val loss: 1.8158...  0.3631 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62776...  Training loss: 0.8438...  Val loss: 1.8339...  0.3633 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62801...  Training loss: 0.8430...  Val loss: 1.8555...  0.3632 sec/batch\n",
      "Epoch: 283/500...  Training Step: 62826...  Training loss: 0.8437...  Val loss: 1.8446...  0.3634 sec/batch\n",
      "Epoch: 284/500...  Training Step: 62851...  Training loss: 0.8387...  Val loss: 1.8430...  0.3631 sec/batch\n",
      "Epoch: 284/500...  Training Step: 62876...  Training loss: 0.8570...  Val loss: 1.8490...  0.3629 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284/500...  Training Step: 62901...  Training loss: 0.8635...  Val loss: 1.8318...  0.3630 sec/batch\n",
      "Epoch: 284/500...  Training Step: 62926...  Training loss: 0.8436...  Val loss: 1.8595...  0.3633 sec/batch\n",
      "Epoch: 284/500...  Training Step: 62951...  Training loss: 0.8257...  Val loss: 1.8578...  0.3635 sec/batch\n",
      "Epoch: 284/500...  Training Step: 62976...  Training loss: 0.8511...  Val loss: 1.8449...  0.3626 sec/batch\n",
      "Epoch: 284/500...  Training Step: 63001...  Training loss: 0.8472...  Val loss: 1.8291...  0.3635 sec/batch\n",
      "Epoch: 284/500...  Training Step: 63026...  Training loss: 0.8352...  Val loss: 1.8621...  0.3636 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63051...  Training loss: 0.8537...  Val loss: 1.8381...  0.3637 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63076...  Training loss: 0.8445...  Val loss: 1.8294...  0.3633 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63101...  Training loss: 0.8389...  Val loss: 1.8492...  0.3627 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63126...  Training loss: 0.8325...  Val loss: 1.8563...  0.3634 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63151...  Training loss: 0.8296...  Val loss: 1.8509...  0.3632 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63176...  Training loss: 0.8446...  Val loss: 1.8605...  0.3630 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63201...  Training loss: 0.8404...  Val loss: 1.8350...  0.3628 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63226...  Training loss: 0.8451...  Val loss: 1.8488...  0.3634 sec/batch\n",
      "Epoch: 285/500...  Training Step: 63251...  Training loss: 0.8508...  Val loss: 1.8529...  0.3631 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63276...  Training loss: 0.8533...  Val loss: 1.8341...  0.3634 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63301...  Training loss: 0.8271...  Val loss: 1.8423...  0.3634 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63326...  Training loss: 0.8476...  Val loss: 1.8575...  0.3631 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63351...  Training loss: 0.8397...  Val loss: 1.8507...  0.3626 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63376...  Training loss: 0.8367...  Val loss: 1.8591...  0.3636 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63401...  Training loss: 0.8439...  Val loss: 1.8493...  0.3628 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63426...  Training loss: 0.8555...  Val loss: 1.8452...  0.3633 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63451...  Training loss: 0.8461...  Val loss: 1.8421...  0.3636 sec/batch\n",
      "Epoch: 286/500...  Training Step: 63476...  Training loss: 0.8266...  Val loss: 1.8718...  0.3624 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63501...  Training loss: 0.8400...  Val loss: 1.8544...  0.3632 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63526...  Training loss: 0.8424...  Val loss: 1.8718...  0.3633 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63551...  Training loss: 0.8465...  Val loss: 1.8711...  0.3633 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63576...  Training loss: 0.8525...  Val loss: 1.8437...  0.3635 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63601...  Training loss: 0.8336...  Val loss: 1.8400...  0.3635 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63626...  Training loss: 0.8313...  Val loss: 1.8444...  0.3636 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63651...  Training loss: 0.8519...  Val loss: 1.8679...  0.3634 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63676...  Training loss: 0.8703...  Val loss: 1.8545...  0.3632 sec/batch\n",
      "Epoch: 287/500...  Training Step: 63701...  Training loss: 0.8351...  Val loss: 1.8562...  0.3633 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63726...  Training loss: 0.8487...  Val loss: 1.8553...  0.3636 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63751...  Training loss: 0.8412...  Val loss: 1.8493...  0.3637 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63776...  Training loss: 0.8430...  Val loss: 1.8502...  0.3635 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63801...  Training loss: 0.8492...  Val loss: 1.8364...  0.3631 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63826...  Training loss: 0.8411...  Val loss: 1.8400...  0.3633 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63851...  Training loss: 0.8293...  Val loss: 1.8657...  0.3634 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63876...  Training loss: 0.8518...  Val loss: 1.8619...  0.3632 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63901...  Training loss: 0.8423...  Val loss: 1.8544...  0.3632 sec/batch\n",
      "Epoch: 288/500...  Training Step: 63926...  Training loss: 0.8393...  Val loss: 1.8470...  0.3634 sec/batch\n",
      "Epoch: 289/500...  Training Step: 63951...  Training loss: 0.8566...  Val loss: 1.8381...  0.3636 sec/batch\n",
      "Epoch: 289/500...  Training Step: 63976...  Training loss: 0.8311...  Val loss: 1.8554...  0.3647 sec/batch\n",
      "Epoch: 289/500...  Training Step: 64001...  Training loss: 0.8521...  Val loss: 1.8650...  0.3641 sec/batch\n",
      "Epoch: 289/500...  Training Step: 64026...  Training loss: 0.8183...  Val loss: 1.8729...  0.3634 sec/batch\n",
      "Epoch: 289/500...  Training Step: 64051...  Training loss: 0.8529...  Val loss: 1.8412...  0.3632 sec/batch\n",
      "Epoch: 289/500...  Training Step: 64076...  Training loss: 0.8268...  Val loss: 1.8474...  0.3630 sec/batch\n",
      "Epoch: 289/500...  Training Step: 64101...  Training loss: 0.8308...  Val loss: 1.8526...  0.3644 sec/batch\n",
      "Epoch: 289/500...  Training Step: 64126...  Training loss: 0.8536...  Val loss: 1.8734...  0.3630 sec/batch\n",
      "Epoch: 289/500...  Training Step: 64151...  Training loss: 0.8544...  Val loss: 1.8756...  0.3634 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64176...  Training loss: 0.8427...  Val loss: 1.8328...  0.3634 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64201...  Training loss: 0.8302...  Val loss: 1.8662...  0.3636 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64226...  Training loss: 0.8511...  Val loss: 1.8642...  0.3635 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64251...  Training loss: 0.8267...  Val loss: 1.8541...  0.3637 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64276...  Training loss: 0.8552...  Val loss: 1.8594...  0.3656 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64301...  Training loss: 0.8531...  Val loss: 1.8576...  0.3633 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64326...  Training loss: 0.8410...  Val loss: 1.8640...  0.3628 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64351...  Training loss: 0.8282...  Val loss: 1.8705...  0.3635 sec/batch\n",
      "Epoch: 290/500...  Training Step: 64376...  Training loss: 0.8619...  Val loss: 1.8588...  0.3632 sec/batch\n",
      "Epoch 290/500 time:81.61149191856384...  finished at 2017-10-30 16:13:42\n",
      "Epoch: 291/500...  Training Step: 64401...  Training loss: 0.8410...  Val loss: 1.8513...  0.3635 sec/batch\n",
      "Epoch: 291/500...  Training Step: 64426...  Training loss: 0.8260...  Val loss: 1.8600...  0.3631 sec/batch\n",
      "Epoch: 291/500...  Training Step: 64451...  Training loss: 0.8414...  Val loss: 1.8593...  0.3636 sec/batch\n",
      "Epoch: 291/500...  Training Step: 64476...  Training loss: 0.8414...  Val loss: 1.8450...  0.3631 sec/batch\n",
      "Epoch: 291/500...  Training Step: 64501...  Training loss: 0.8345...  Val loss: 1.8486...  0.3635 sec/batch\n",
      "Epoch: 291/500...  Training Step: 64526...  Training loss: 0.8531...  Val loss: 1.8719...  0.3638 sec/batch\n",
      "Epoch: 291/500...  Training Step: 64551...  Training loss: 0.8680...  Val loss: 1.8719...  0.3633 sec/batch\n",
      "Epoch: 291/500...  Training Step: 64576...  Training loss: 0.8488...  Val loss: 1.8418...  0.3632 sec/batch\n",
      "Epoch: 291/500...  Training Step: 64601...  Training loss: 0.8555...  Val loss: 1.8805...  0.3638 sec/batch\n",
      "Epoch: 292/500...  Training Step: 64626...  Training loss: 0.8312...  Val loss: 1.8543...  0.3635 sec/batch\n",
      "Epoch: 292/500...  Training Step: 64651...  Training loss: 0.8436...  Val loss: 1.8721...  0.3634 sec/batch\n",
      "Epoch: 292/500...  Training Step: 64676...  Training loss: 0.8281...  Val loss: 1.8631...  0.3634 sec/batch\n",
      "Epoch: 292/500...  Training Step: 64701...  Training loss: 0.8327...  Val loss: 1.8560...  0.3636 sec/batch\n",
      "Epoch: 292/500...  Training Step: 64726...  Training loss: 0.8258...  Val loss: 1.8694...  0.3628 sec/batch\n",
      "Epoch: 292/500...  Training Step: 64751...  Training loss: 0.8396...  Val loss: 1.8524...  0.3633 sec/batch\n",
      "Epoch: 292/500...  Training Step: 64776...  Training loss: 0.8457...  Val loss: 1.8505...  0.3623 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 292/500...  Training Step: 64801...  Training loss: 0.8354...  Val loss: 1.8795...  0.3635 sec/batch\n",
      "Epoch: 293/500...  Training Step: 64826...  Training loss: 0.8150...  Val loss: 1.8644...  0.3634 sec/batch\n",
      "Epoch: 293/500...  Training Step: 64851...  Training loss: 0.8314...  Val loss: 1.8574...  0.3638 sec/batch\n",
      "Epoch: 293/500...  Training Step: 64876...  Training loss: 0.8474...  Val loss: 1.8461...  0.3632 sec/batch\n",
      "Epoch: 293/500...  Training Step: 64901...  Training loss: 0.8273...  Val loss: 1.8650...  0.3630 sec/batch\n",
      "Epoch: 293/500...  Training Step: 64926...  Training loss: 0.8497...  Val loss: 1.8499...  0.3634 sec/batch\n",
      "Epoch: 293/500...  Training Step: 64951...  Training loss: 0.8340...  Val loss: 1.8843...  0.3631 sec/batch\n",
      "Epoch: 293/500...  Training Step: 64976...  Training loss: 0.8255...  Val loss: 1.8581...  0.3635 sec/batch\n",
      "Epoch: 293/500...  Training Step: 65001...  Training loss: 0.8122...  Val loss: 1.8654...  0.3641 sec/batch\n",
      "Epoch: 293/500...  Training Step: 65026...  Training loss: 0.8131...  Val loss: 1.8641...  0.3634 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65051...  Training loss: 0.8362...  Val loss: 1.8651...  0.3631 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65076...  Training loss: 0.8185...  Val loss: 1.8748...  0.3636 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65101...  Training loss: 0.8178...  Val loss: 1.8669...  0.3635 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65126...  Training loss: 0.8435...  Val loss: 1.8619...  0.3630 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65151...  Training loss: 0.8518...  Val loss: 1.8599...  0.3634 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65176...  Training loss: 0.8267...  Val loss: 1.8847...  0.3632 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65201...  Training loss: 0.8217...  Val loss: 1.8711...  0.3630 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65226...  Training loss: 0.8346...  Val loss: 1.8585...  0.3635 sec/batch\n",
      "Epoch: 294/500...  Training Step: 65251...  Training loss: 0.8185...  Val loss: 1.8643...  0.3638 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65276...  Training loss: 0.8405...  Val loss: 1.8295...  0.3633 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65301...  Training loss: 0.8374...  Val loss: 1.8675...  0.3635 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65326...  Training loss: 0.8311...  Val loss: 1.8846...  0.3632 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65351...  Training loss: 0.8232...  Val loss: 1.8694...  0.3634 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65376...  Training loss: 0.8297...  Val loss: 1.8723...  0.3634 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65401...  Training loss: 0.8403...  Val loss: 1.8768...  0.3632 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65426...  Training loss: 0.8450...  Val loss: 1.8742...  0.3639 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65451...  Training loss: 0.8271...  Val loss: 1.8624...  0.3635 sec/batch\n",
      "Epoch: 295/500...  Training Step: 65476...  Training loss: 0.8395...  Val loss: 1.8790...  0.3636 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65501...  Training loss: 0.8368...  Val loss: 1.8630...  0.3637 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65526...  Training loss: 0.8299...  Val loss: 1.8587...  0.3633 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65551...  Training loss: 0.8336...  Val loss: 1.8755...  0.3629 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65576...  Training loss: 0.8350...  Val loss: 1.8476...  0.3632 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65601...  Training loss: 0.8597...  Val loss: 1.8623...  0.3636 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65626...  Training loss: 0.8292...  Val loss: 1.8636...  0.3635 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65651...  Training loss: 0.8441...  Val loss: 1.8690...  0.3631 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65676...  Training loss: 0.8564...  Val loss: 1.8762...  0.3636 sec/batch\n",
      "Epoch: 296/500...  Training Step: 65701...  Training loss: 0.8356...  Val loss: 1.8691...  0.3636 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65726...  Training loss: 0.8230...  Val loss: 1.8584...  0.3636 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65751...  Training loss: 0.8253...  Val loss: 1.8798...  0.3639 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65776...  Training loss: 0.8398...  Val loss: 1.8622...  0.3633 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65801...  Training loss: 0.8357...  Val loss: 1.8750...  0.3634 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65826...  Training loss: 0.8291...  Val loss: 1.8709...  0.3639 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65851...  Training loss: 0.8435...  Val loss: 1.8748...  0.3633 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65876...  Training loss: 0.8329...  Val loss: 1.8860...  0.3634 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65901...  Training loss: 0.8354...  Val loss: 1.8771...  0.3632 sec/batch\n",
      "Epoch: 297/500...  Training Step: 65926...  Training loss: 0.8342...  Val loss: 1.8798...  0.3631 sec/batch\n",
      "Epoch: 298/500...  Training Step: 65951...  Training loss: 0.8377...  Val loss: 1.8553...  0.3633 sec/batch\n",
      "Epoch: 298/500...  Training Step: 65976...  Training loss: 0.8513...  Val loss: 1.8517...  0.3632 sec/batch\n",
      "Epoch: 298/500...  Training Step: 66001...  Training loss: 0.8233...  Val loss: 1.8861...  0.3635 sec/batch\n",
      "Epoch: 298/500...  Training Step: 66026...  Training loss: 0.8300...  Val loss: 1.8624...  0.3634 sec/batch\n",
      "Epoch: 298/500...  Training Step: 66051...  Training loss: 0.8448...  Val loss: 1.8696...  0.3632 sec/batch\n",
      "Epoch: 298/500...  Training Step: 66076...  Training loss: 0.8313...  Val loss: 1.8562...  0.3634 sec/batch\n",
      "Epoch: 298/500...  Training Step: 66101...  Training loss: 0.8287...  Val loss: 1.8838...  0.3629 sec/batch\n",
      "Epoch: 298/500...  Training Step: 66126...  Training loss: 0.8291...  Val loss: 1.8887...  0.3635 sec/batch\n",
      "Epoch: 298/500...  Training Step: 66151...  Training loss: 0.8247...  Val loss: 1.8785...  0.3634 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66176...  Training loss: 0.8283...  Val loss: 1.8389...  0.3634 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66201...  Training loss: 0.8198...  Val loss: 1.8595...  0.3631 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66226...  Training loss: 0.8369...  Val loss: 1.8628...  0.3638 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66251...  Training loss: 0.8243...  Val loss: 1.8822...  0.3636 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66276...  Training loss: 0.8177...  Val loss: 1.8580...  0.3633 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66301...  Training loss: 0.8321...  Val loss: 1.8658...  0.3632 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66326...  Training loss: 0.8287...  Val loss: 1.8702...  0.3633 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66351...  Training loss: 0.8258...  Val loss: 1.8792...  0.3633 sec/batch\n",
      "Epoch: 299/500...  Training Step: 66376...  Training loss: 0.8149...  Val loss: 1.8788...  0.3628 sec/batch\n",
      "Epoch: 300/500...  Training Step: 66401...  Training loss: 0.8319...  Val loss: 1.8531...  0.3633 sec/batch\n",
      "Epoch: 300/500...  Training Step: 66426...  Training loss: 0.8287...  Val loss: 1.8674...  0.3634 sec/batch\n",
      "Epoch: 300/500...  Training Step: 66451...  Training loss: 0.8327...  Val loss: 1.8521...  0.3632 sec/batch\n",
      "Epoch: 300/500...  Training Step: 66476...  Training loss: 0.8287...  Val loss: 1.8739...  0.3634 sec/batch\n",
      "Epoch: 300/500...  Training Step: 66501...  Training loss: 0.8263...  Val loss: 1.8833...  0.3637 sec/batch\n",
      "Epoch: 300/500...  Training Step: 66526...  Training loss: 0.8418...  Val loss: 1.8671...  0.3636 sec/batch\n",
      "Epoch: 300/500...  Training Step: 66551...  Training loss: 0.8491...  Val loss: 1.8720...  0.3635 sec/batch\n",
      "Epoch: 300/500...  Training Step: 66576...  Training loss: 0.8370...  Val loss: 1.8814...  0.3632 sec/batch\n",
      "Epoch 300/500 time:81.84382057189941...  finished at 2017-10-30 16:27:20\n",
      "Epoch: 301/500...  Training Step: 66601...  Training loss: 0.9145...  Val loss: 1.8735...  0.3632 sec/batch\n",
      "Epoch: 301/500...  Training Step: 66626...  Training loss: 0.8391...  Val loss: 1.8704...  0.3633 sec/batch\n",
      "Epoch: 301/500...  Training Step: 66651...  Training loss: 0.8143...  Val loss: 1.8855...  0.3638 sec/batch\n",
      "Epoch: 301/500...  Training Step: 66676...  Training loss: 0.8302...  Val loss: 1.8726...  0.3631 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301/500...  Training Step: 66701...  Training loss: 0.8253...  Val loss: 1.8841...  0.3626 sec/batch\n",
      "Epoch: 301/500...  Training Step: 66726...  Training loss: 0.8188...  Val loss: 1.8899...  0.3634 sec/batch\n",
      "Epoch: 301/500...  Training Step: 66751...  Training loss: 0.8418...  Val loss: 1.8684...  0.3632 sec/batch\n",
      "Epoch: 301/500...  Training Step: 66776...  Training loss: 0.8230...  Val loss: 1.8779...  0.3638 sec/batch\n",
      "Epoch: 301/500...  Training Step: 66801...  Training loss: 0.8327...  Val loss: 1.8876...  0.3632 sec/batch\n",
      "Epoch: 302/500...  Training Step: 66826...  Training loss: 0.8369...  Val loss: 1.8770...  0.3633 sec/batch\n",
      "Epoch: 302/500...  Training Step: 66851...  Training loss: 0.8370...  Val loss: 1.8680...  0.3634 sec/batch\n",
      "Epoch: 302/500...  Training Step: 66876...  Training loss: 0.8212...  Val loss: 1.8821...  0.3633 sec/batch\n",
      "Epoch: 302/500...  Training Step: 66901...  Training loss: 0.8129...  Val loss: 1.8814...  0.3632 sec/batch\n",
      "Epoch: 302/500...  Training Step: 66926...  Training loss: 0.8199...  Val loss: 1.8825...  0.3623 sec/batch\n",
      "Epoch: 302/500...  Training Step: 66951...  Training loss: 0.8216...  Val loss: 1.8791...  0.3633 sec/batch\n",
      "Epoch: 302/500...  Training Step: 66976...  Training loss: 0.8324...  Val loss: 1.8723...  0.3630 sec/batch\n",
      "Epoch: 302/500...  Training Step: 67001...  Training loss: 0.8299...  Val loss: 1.8897...  0.3637 sec/batch\n",
      "Epoch: 302/500...  Training Step: 67026...  Training loss: 0.8216...  Val loss: 1.8861...  0.3633 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67051...  Training loss: 0.8407...  Val loss: 1.8659...  0.3636 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67076...  Training loss: 0.8484...  Val loss: 1.9002...  0.3634 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67101...  Training loss: 0.8252...  Val loss: 1.8936...  0.3633 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67126...  Training loss: 0.8244...  Val loss: 1.8681...  0.3630 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67151...  Training loss: 0.8180...  Val loss: 1.8866...  0.3635 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67176...  Training loss: 0.8193...  Val loss: 1.8953...  0.3631 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67201...  Training loss: 0.8289...  Val loss: 1.8775...  0.3635 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67226...  Training loss: 0.8351...  Val loss: 1.8710...  0.3638 sec/batch\n",
      "Epoch: 303/500...  Training Step: 67251...  Training loss: 0.8342...  Val loss: 1.8936...  0.3634 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67276...  Training loss: 0.8286...  Val loss: 1.8584...  0.3629 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67301...  Training loss: 0.8269...  Val loss: 1.8799...  0.3631 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67326...  Training loss: 0.8285...  Val loss: 1.8749...  0.3635 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67351...  Training loss: 0.8441...  Val loss: 1.8690...  0.3631 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67376...  Training loss: 0.8391...  Val loss: 1.8653...  0.3637 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67401...  Training loss: 0.8228...  Val loss: 1.8915...  0.3634 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67426...  Training loss: 0.8273...  Val loss: 1.8641...  0.3639 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67451...  Training loss: 0.8148...  Val loss: 1.8894...  0.3630 sec/batch\n",
      "Epoch: 304/500...  Training Step: 67476...  Training loss: 0.8103...  Val loss: 1.8701...  0.3631 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67501...  Training loss: 0.8379...  Val loss: 1.8686...  0.3630 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67526...  Training loss: 0.8239...  Val loss: 1.8895...  0.3638 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67551...  Training loss: 0.8284...  Val loss: 1.8882...  0.3634 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67576...  Training loss: 0.8254...  Val loss: 1.8832...  0.3628 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67601...  Training loss: 0.8092...  Val loss: 1.8720...  0.3635 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67626...  Training loss: 0.8284...  Val loss: 1.8727...  0.3633 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67651...  Training loss: 0.8329...  Val loss: 1.8829...  0.3635 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67676...  Training loss: 0.8162...  Val loss: 1.8933...  0.3630 sec/batch\n",
      "Epoch: 305/500...  Training Step: 67701...  Training loss: 0.8402...  Val loss: 1.8878...  0.3631 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67726...  Training loss: 0.8089...  Val loss: 1.8734...  0.3634 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67751...  Training loss: 0.8188...  Val loss: 1.8851...  0.3631 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67776...  Training loss: 0.8280...  Val loss: 1.8783...  0.3632 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67801...  Training loss: 0.8275...  Val loss: 1.8793...  0.3628 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67826...  Training loss: 0.8315...  Val loss: 1.8757...  0.3638 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67851...  Training loss: 0.8189...  Val loss: 1.8771...  0.3636 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67876...  Training loss: 0.8323...  Val loss: 1.8758...  0.3631 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67901...  Training loss: 0.8228...  Val loss: 1.8870...  0.3633 sec/batch\n",
      "Epoch: 306/500...  Training Step: 67926...  Training loss: 0.8435...  Val loss: 1.8771...  0.3634 sec/batch\n",
      "Epoch: 307/500...  Training Step: 67951...  Training loss: 0.8347...  Val loss: 1.8794...  0.3630 sec/batch\n",
      "Epoch: 307/500...  Training Step: 67976...  Training loss: 0.8288...  Val loss: 1.8664...  0.3631 sec/batch\n",
      "Epoch: 307/500...  Training Step: 68001...  Training loss: 0.8210...  Val loss: 1.8768...  0.3633 sec/batch\n",
      "Epoch: 307/500...  Training Step: 68026...  Training loss: 0.8305...  Val loss: 1.8805...  0.3629 sec/batch\n",
      "Epoch: 307/500...  Training Step: 68051...  Training loss: 0.8306...  Val loss: 1.8921...  0.3633 sec/batch\n",
      "Epoch: 307/500...  Training Step: 68076...  Training loss: 0.8318...  Val loss: 1.8952...  0.3630 sec/batch\n",
      "Epoch: 307/500...  Training Step: 68101...  Training loss: 0.8236...  Val loss: 1.9048...  0.3633 sec/batch\n",
      "Epoch: 307/500...  Training Step: 68126...  Training loss: 0.8289...  Val loss: 1.9059...  0.3634 sec/batch\n",
      "Epoch: 307/500...  Training Step: 68151...  Training loss: 0.8392...  Val loss: 1.8900...  0.3635 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68176...  Training loss: 0.8139...  Val loss: 1.8717...  0.3644 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68201...  Training loss: 0.8158...  Val loss: 1.8865...  0.3636 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68226...  Training loss: 0.8236...  Val loss: 1.8961...  0.3633 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68251...  Training loss: 0.8103...  Val loss: 1.8891...  0.3637 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68276...  Training loss: 0.8343...  Val loss: 1.8701...  0.3641 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68301...  Training loss: 0.8414...  Val loss: 1.8522...  0.3634 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68326...  Training loss: 0.8295...  Val loss: 1.8773...  0.3638 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68351...  Training loss: 0.8318...  Val loss: 1.8962...  0.3634 sec/batch\n",
      "Epoch: 308/500...  Training Step: 68376...  Training loss: 0.8232...  Val loss: 1.8969...  0.3630 sec/batch\n",
      "Epoch: 309/500...  Training Step: 68401...  Training loss: 0.8035...  Val loss: 1.8811...  0.3635 sec/batch\n",
      "Epoch: 309/500...  Training Step: 68426...  Training loss: 0.8259...  Val loss: 1.8853...  0.3636 sec/batch\n",
      "Epoch: 309/500...  Training Step: 68451...  Training loss: 0.8433...  Val loss: 1.8785...  0.3632 sec/batch\n",
      "Epoch: 309/500...  Training Step: 68476...  Training loss: 0.8234...  Val loss: 1.8977...  0.3639 sec/batch\n",
      "Epoch: 309/500...  Training Step: 68501...  Training loss: 0.8062...  Val loss: 1.9075...  0.3632 sec/batch\n",
      "Epoch: 309/500...  Training Step: 68526...  Training loss: 0.8194...  Val loss: 1.8871...  0.3633 sec/batch\n",
      "Epoch: 309/500...  Training Step: 68551...  Training loss: 0.8238...  Val loss: 1.8705...  0.3635 sec/batch\n",
      "Epoch: 309/500...  Training Step: 68576...  Training loss: 0.8207...  Val loss: 1.9177...  0.3632 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 310/500...  Training Step: 68601...  Training loss: 0.8291...  Val loss: 1.8764...  0.3628 sec/batch\n",
      "Epoch: 310/500...  Training Step: 68626...  Training loss: 0.8178...  Val loss: 1.8764...  0.3632 sec/batch\n",
      "Epoch: 310/500...  Training Step: 68651...  Training loss: 0.7929...  Val loss: 1.8757...  0.3631 sec/batch\n",
      "Epoch: 310/500...  Training Step: 68676...  Training loss: 0.8081...  Val loss: 1.8964...  0.3632 sec/batch\n",
      "Epoch: 310/500...  Training Step: 68701...  Training loss: 0.8041...  Val loss: 1.8940...  0.3629 sec/batch\n",
      "Epoch: 310/500...  Training Step: 68726...  Training loss: 0.8182...  Val loss: 1.9078...  0.3633 sec/batch\n",
      "Epoch: 310/500...  Training Step: 68751...  Training loss: 0.8149...  Val loss: 1.8855...  0.3637 sec/batch\n",
      "Epoch: 310/500...  Training Step: 68776...  Training loss: 0.8157...  Val loss: 1.8923...  0.3637 sec/batch\n",
      "Epoch: 310/500...  Training Step: 68801...  Training loss: 0.8152...  Val loss: 1.9028...  0.3629 sec/batch\n",
      "Epoch 310/500 time:81.57478332519531...  finished at 2017-10-30 16:40:57\n",
      "Epoch: 311/500...  Training Step: 68826...  Training loss: 0.8115...  Val loss: 1.8772...  0.3632 sec/batch\n",
      "Epoch: 311/500...  Training Step: 68851...  Training loss: 0.8090...  Val loss: 1.8752...  0.3633 sec/batch\n",
      "Epoch: 311/500...  Training Step: 68876...  Training loss: 0.8077...  Val loss: 1.8873...  0.3639 sec/batch\n",
      "Epoch: 311/500...  Training Step: 68901...  Training loss: 0.8145...  Val loss: 1.8805...  0.3632 sec/batch\n",
      "Epoch: 311/500...  Training Step: 68926...  Training loss: 0.8132...  Val loss: 1.9057...  0.3636 sec/batch\n",
      "Epoch: 311/500...  Training Step: 68951...  Training loss: 0.8169...  Val loss: 1.8955...  0.3631 sec/batch\n",
      "Epoch: 311/500...  Training Step: 68976...  Training loss: 0.8141...  Val loss: 1.8950...  0.3632 sec/batch\n",
      "Epoch: 311/500...  Training Step: 69001...  Training loss: 0.8167...  Val loss: 1.8741...  0.3629 sec/batch\n",
      "Epoch: 311/500...  Training Step: 69026...  Training loss: 0.8096...  Val loss: 1.8983...  0.3634 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69051...  Training loss: 0.8205...  Val loss: 1.9023...  0.3632 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69076...  Training loss: 0.8138...  Val loss: 1.9105...  0.3630 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69101...  Training loss: 0.8221...  Val loss: 1.9199...  0.3636 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69126...  Training loss: 0.8300...  Val loss: 1.8795...  0.3633 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69151...  Training loss: 0.8097...  Val loss: 1.8775...  0.3635 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69176...  Training loss: 0.8057...  Val loss: 1.8939...  0.3635 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69201...  Training loss: 0.8314...  Val loss: 1.8987...  0.3631 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69226...  Training loss: 0.8259...  Val loss: 1.8894...  0.3638 sec/batch\n",
      "Epoch: 312/500...  Training Step: 69251...  Training loss: 0.8178...  Val loss: 1.8880...  0.3634 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69276...  Training loss: 0.8173...  Val loss: 1.8936...  0.3669 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69301...  Training loss: 0.8187...  Val loss: 1.8997...  0.3633 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69326...  Training loss: 0.8275...  Val loss: 1.8906...  0.3636 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69351...  Training loss: 0.8310...  Val loss: 1.8785...  0.3634 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69376...  Training loss: 0.8152...  Val loss: 1.8880...  0.3633 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69401...  Training loss: 0.8150...  Val loss: 1.9043...  0.3630 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69426...  Training loss: 0.8183...  Val loss: 1.8929...  0.3634 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69451...  Training loss: 0.8216...  Val loss: 1.8895...  0.3629 sec/batch\n",
      "Epoch: 313/500...  Training Step: 69476...  Training loss: 0.8122...  Val loss: 1.8912...  0.3633 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69501...  Training loss: 0.8343...  Val loss: 1.8890...  0.3633 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69526...  Training loss: 0.8021...  Val loss: 1.8891...  0.3633 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69551...  Training loss: 0.8313...  Val loss: 1.9125...  0.3639 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69576...  Training loss: 0.7962...  Val loss: 1.9069...  0.3635 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69601...  Training loss: 0.8317...  Val loss: 1.8875...  0.3636 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69626...  Training loss: 0.8156...  Val loss: 1.8878...  0.3635 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69651...  Training loss: 0.8149...  Val loss: 1.8939...  0.3634 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69676...  Training loss: 0.8343...  Val loss: 1.9102...  0.3630 sec/batch\n",
      "Epoch: 314/500...  Training Step: 69701...  Training loss: 0.8298...  Val loss: 1.9143...  0.3631 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69726...  Training loss: 0.8248...  Val loss: 1.8730...  0.3634 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69751...  Training loss: 0.8070...  Val loss: 1.9098...  0.3636 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69776...  Training loss: 0.8277...  Val loss: 1.9033...  0.3632 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69801...  Training loss: 0.8095...  Val loss: 1.8918...  0.3635 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69826...  Training loss: 0.8248...  Val loss: 1.8863...  0.3635 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69851...  Training loss: 0.8296...  Val loss: 1.8924...  0.3631 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69876...  Training loss: 0.8180...  Val loss: 1.9027...  0.3629 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69901...  Training loss: 0.7967...  Val loss: 1.9105...  0.3635 sec/batch\n",
      "Epoch: 315/500...  Training Step: 69926...  Training loss: 0.8315...  Val loss: 1.8896...  0.3634 sec/batch\n",
      "Epoch: 316/500...  Training Step: 69951...  Training loss: 0.8398...  Val loss: 1.8969...  0.3629 sec/batch\n",
      "Epoch: 316/500...  Training Step: 69976...  Training loss: 0.7941...  Val loss: 1.8971...  0.3631 sec/batch\n",
      "Epoch: 316/500...  Training Step: 70001...  Training loss: 0.8268...  Val loss: 1.9034...  0.3630 sec/batch\n",
      "Epoch: 316/500...  Training Step: 70026...  Training loss: 0.8154...  Val loss: 1.8947...  0.3641 sec/batch\n",
      "Epoch: 316/500...  Training Step: 70051...  Training loss: 0.8207...  Val loss: 1.8876...  0.3640 sec/batch\n",
      "Epoch: 316/500...  Training Step: 70076...  Training loss: 0.8228...  Val loss: 1.9188...  0.3636 sec/batch\n",
      "Epoch: 316/500...  Training Step: 70101...  Training loss: 0.8515...  Val loss: 1.9183...  0.3632 sec/batch\n",
      "Epoch: 316/500...  Training Step: 70126...  Training loss: 0.8179...  Val loss: 1.8855...  0.3632 sec/batch\n",
      "Epoch: 316/500...  Training Step: 70151...  Training loss: 0.8088...  Val loss: 1.9195...  0.3636 sec/batch\n",
      "Epoch: 317/500...  Training Step: 70176...  Training loss: 0.8184...  Val loss: 1.8984...  0.3632 sec/batch\n",
      "Epoch: 317/500...  Training Step: 70201...  Training loss: 0.8255...  Val loss: 1.9048...  0.3637 sec/batch\n",
      "Epoch: 317/500...  Training Step: 70226...  Training loss: 0.8150...  Val loss: 1.9122...  0.3631 sec/batch\n",
      "Epoch: 317/500...  Training Step: 70251...  Training loss: 0.8134...  Val loss: 1.9037...  0.3631 sec/batch\n",
      "Epoch: 317/500...  Training Step: 70276...  Training loss: 0.8069...  Val loss: 1.9008...  0.3634 sec/batch\n",
      "Epoch: 317/500...  Training Step: 70301...  Training loss: 0.8142...  Val loss: 1.8946...  0.3634 sec/batch\n",
      "Epoch: 317/500...  Training Step: 70326...  Training loss: 0.8274...  Val loss: 1.8980...  0.3627 sec/batch\n",
      "Epoch: 317/500...  Training Step: 70351...  Training loss: 0.8251...  Val loss: 1.9256...  0.3629 sec/batch\n",
      "Epoch: 318/500...  Training Step: 70376...  Training loss: 0.8045...  Val loss: 1.8954...  0.3627 sec/batch\n",
      "Epoch: 318/500...  Training Step: 70401...  Training loss: 0.8030...  Val loss: 1.8953...  0.3634 sec/batch\n",
      "Epoch: 318/500...  Training Step: 70426...  Training loss: 0.8205...  Val loss: 1.8797...  0.3632 sec/batch\n",
      "Epoch: 318/500...  Training Step: 70451...  Training loss: 0.8054...  Val loss: 1.9035...  0.3634 sec/batch\n",
      "Epoch: 318/500...  Training Step: 70476...  Training loss: 0.8196...  Val loss: 1.8888...  0.3636 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 318/500...  Training Step: 70501...  Training loss: 0.8139...  Val loss: 1.9183...  0.3637 sec/batch\n",
      "Epoch: 318/500...  Training Step: 70526...  Training loss: 0.8140...  Val loss: 1.8959...  0.3631 sec/batch\n",
      "Epoch: 318/500...  Training Step: 70551...  Training loss: 0.8075...  Val loss: 1.9016...  0.3636 sec/batch\n",
      "Epoch: 318/500...  Training Step: 70576...  Training loss: 0.7851...  Val loss: 1.9101...  0.3632 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70601...  Training loss: 0.8183...  Val loss: 1.9058...  0.3633 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70626...  Training loss: 0.8017...  Val loss: 1.8947...  0.3631 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70651...  Training loss: 0.7866...  Val loss: 1.9060...  0.3635 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70676...  Training loss: 0.8240...  Val loss: 1.8988...  0.3631 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70701...  Training loss: 0.8165...  Val loss: 1.8887...  0.3632 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70726...  Training loss: 0.8194...  Val loss: 1.9208...  0.3632 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70751...  Training loss: 0.8216...  Val loss: 1.8992...  0.3633 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70776...  Training loss: 0.8137...  Val loss: 1.8876...  0.3634 sec/batch\n",
      "Epoch: 319/500...  Training Step: 70801...  Training loss: 0.8037...  Val loss: 1.9080...  0.3637 sec/batch\n",
      "Epoch: 320/500...  Training Step: 70826...  Training loss: 0.8275...  Val loss: 1.8708...  0.3630 sec/batch\n",
      "Epoch: 320/500...  Training Step: 70851...  Training loss: 0.8064...  Val loss: 1.9026...  0.3636 sec/batch\n",
      "Epoch: 320/500...  Training Step: 70876...  Training loss: 0.8158...  Val loss: 1.9255...  0.3632 sec/batch\n",
      "Epoch: 320/500...  Training Step: 70901...  Training loss: 0.8105...  Val loss: 1.9065...  0.3633 sec/batch\n",
      "Epoch: 320/500...  Training Step: 70926...  Training loss: 0.8006...  Val loss: 1.9081...  0.3635 sec/batch\n",
      "Epoch: 320/500...  Training Step: 70951...  Training loss: 0.8094...  Val loss: 1.9119...  0.3633 sec/batch\n",
      "Epoch: 320/500...  Training Step: 70976...  Training loss: 0.8259...  Val loss: 1.9062...  0.3636 sec/batch\n",
      "Epoch: 320/500...  Training Step: 71001...  Training loss: 0.8064...  Val loss: 1.8837...  0.3631 sec/batch\n",
      "Epoch: 320/500...  Training Step: 71026...  Training loss: 0.8135...  Val loss: 1.9129...  0.3636 sec/batch\n",
      "Epoch 320/500 time:81.93251132965088...  finished at 2017-10-30 16:54:34\n",
      "Epoch: 321/500...  Training Step: 71051...  Training loss: 0.8100...  Val loss: 1.9077...  0.3630 sec/batch\n",
      "Epoch: 321/500...  Training Step: 71076...  Training loss: 0.8055...  Val loss: 1.8910...  0.3632 sec/batch\n",
      "Epoch: 321/500...  Training Step: 71101...  Training loss: 0.8155...  Val loss: 1.9212...  0.3632 sec/batch\n",
      "Epoch: 321/500...  Training Step: 71126...  Training loss: 0.8186...  Val loss: 1.8926...  0.3634 sec/batch\n",
      "Epoch: 321/500...  Training Step: 71151...  Training loss: 0.8242...  Val loss: 1.9025...  0.3632 sec/batch\n",
      "Epoch: 321/500...  Training Step: 71176...  Training loss: 0.7990...  Val loss: 1.8961...  0.3626 sec/batch\n",
      "Epoch: 321/500...  Training Step: 71201...  Training loss: 0.8213...  Val loss: 1.9063...  0.3637 sec/batch\n",
      "Epoch: 321/500...  Training Step: 71226...  Training loss: 0.8299...  Val loss: 1.9152...  0.3633 sec/batch\n",
      "Epoch: 321/500...  Training Step: 71251...  Training loss: 0.8018...  Val loss: 1.9111...  0.3630 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71276...  Training loss: 0.7977...  Val loss: 1.9010...  0.3630 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71301...  Training loss: 0.8188...  Val loss: 1.9099...  0.3632 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71326...  Training loss: 0.8182...  Val loss: 1.9057...  0.3631 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71351...  Training loss: 0.8018...  Val loss: 1.9117...  0.3630 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71376...  Training loss: 0.8010...  Val loss: 1.9111...  0.3634 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71401...  Training loss: 0.8310...  Val loss: 1.9041...  0.3632 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71426...  Training loss: 0.8184...  Val loss: 1.9328...  0.3635 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71451...  Training loss: 0.8146...  Val loss: 1.9007...  0.3630 sec/batch\n",
      "Epoch: 322/500...  Training Step: 71476...  Training loss: 0.8175...  Val loss: 1.9169...  0.3632 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71501...  Training loss: 0.8186...  Val loss: 1.8981...  0.3640 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71526...  Training loss: 0.8133...  Val loss: 1.8845...  0.3632 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71551...  Training loss: 0.8003...  Val loss: 1.9182...  0.3633 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71576...  Training loss: 0.8097...  Val loss: 1.8983...  0.3638 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71601...  Training loss: 0.8109...  Val loss: 1.9056...  0.3632 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71626...  Training loss: 0.8203...  Val loss: 1.8937...  0.3633 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71651...  Training loss: 0.8136...  Val loss: 1.9166...  0.3634 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71676...  Training loss: 0.8129...  Val loss: 1.9213...  0.3633 sec/batch\n",
      "Epoch: 323/500...  Training Step: 71701...  Training loss: 0.8085...  Val loss: 1.9227...  0.3629 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71726...  Training loss: 0.8191...  Val loss: 1.8636...  0.3633 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71751...  Training loss: 0.8044...  Val loss: 1.9016...  0.3634 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71776...  Training loss: 0.8134...  Val loss: 1.8993...  0.3630 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71801...  Training loss: 0.8018...  Val loss: 1.9092...  0.3632 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71826...  Training loss: 0.7942...  Val loss: 1.9059...  0.3637 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71851...  Training loss: 0.8183...  Val loss: 1.8987...  0.3635 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71876...  Training loss: 0.8061...  Val loss: 1.9157...  0.3634 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71901...  Training loss: 0.8089...  Val loss: 1.9060...  0.3632 sec/batch\n",
      "Epoch: 324/500...  Training Step: 71926...  Training loss: 0.8093...  Val loss: 1.9153...  0.3633 sec/batch\n",
      "Epoch: 325/500...  Training Step: 71951...  Training loss: 0.8058...  Val loss: 1.8958...  0.3636 sec/batch\n",
      "Epoch: 325/500...  Training Step: 71976...  Training loss: 0.7928...  Val loss: 1.9198...  0.3639 sec/batch\n",
      "Epoch: 325/500...  Training Step: 72001...  Training loss: 0.8007...  Val loss: 1.8856...  0.3635 sec/batch\n",
      "Epoch: 325/500...  Training Step: 72026...  Training loss: 0.8094...  Val loss: 1.9035...  0.3637 sec/batch\n",
      "Epoch: 325/500...  Training Step: 72051...  Training loss: 0.8087...  Val loss: 1.9264...  0.3633 sec/batch\n",
      "Epoch: 325/500...  Training Step: 72076...  Training loss: 0.8172...  Val loss: 1.9073...  0.3631 sec/batch\n",
      "Epoch: 325/500...  Training Step: 72101...  Training loss: 0.8203...  Val loss: 1.9063...  0.3639 sec/batch\n",
      "Epoch: 325/500...  Training Step: 72126...  Training loss: 0.8128...  Val loss: 1.9101...  0.3633 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72151...  Training loss: 0.8979...  Val loss: 1.9142...  0.3631 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72176...  Training loss: 0.8163...  Val loss: 1.9040...  0.3634 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72201...  Training loss: 0.7950...  Val loss: 1.9189...  0.3637 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72226...  Training loss: 0.8024...  Val loss: 1.8975...  0.3632 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72251...  Training loss: 0.8076...  Val loss: 1.9098...  0.3633 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72276...  Training loss: 0.8025...  Val loss: 1.9201...  0.3631 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72301...  Training loss: 0.8092...  Val loss: 1.9083...  0.3635 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72326...  Training loss: 0.8018...  Val loss: 1.9079...  0.3629 sec/batch\n",
      "Epoch: 326/500...  Training Step: 72351...  Training loss: 0.8148...  Val loss: 1.9275...  0.3633 sec/batch\n",
      "Epoch: 327/500...  Training Step: 72376...  Training loss: 0.8117...  Val loss: 1.9063...  0.3638 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 327/500...  Training Step: 72401...  Training loss: 0.8118...  Val loss: 1.9155...  0.3630 sec/batch\n",
      "Epoch: 327/500...  Training Step: 72426...  Training loss: 0.7993...  Val loss: 1.9189...  0.3637 sec/batch\n",
      "Epoch: 327/500...  Training Step: 72451...  Training loss: 0.7961...  Val loss: 1.9291...  0.3632 sec/batch\n",
      "Epoch: 327/500...  Training Step: 72476...  Training loss: 0.7991...  Val loss: 1.9280...  0.3622 sec/batch\n",
      "Epoch: 327/500...  Training Step: 72501...  Training loss: 0.7930...  Val loss: 1.9239...  0.3635 sec/batch\n",
      "Epoch: 327/500...  Training Step: 72526...  Training loss: 0.8095...  Val loss: 1.9180...  0.3635 sec/batch\n",
      "Epoch: 327/500...  Training Step: 72551...  Training loss: 0.8127...  Val loss: 1.9087...  0.3633 sec/batch\n",
      "Epoch: 327/500...  Training Step: 72576...  Training loss: 0.8070...  Val loss: 1.9244...  0.3641 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72601...  Training loss: 0.8237...  Val loss: 1.9091...  0.3640 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72626...  Training loss: 0.8259...  Val loss: 1.9265...  0.3637 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72651...  Training loss: 0.7932...  Val loss: 1.9146...  0.3638 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72676...  Training loss: 0.8014...  Val loss: 1.9117...  0.3634 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72701...  Training loss: 0.7933...  Val loss: 1.9172...  0.3636 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72726...  Training loss: 0.8196...  Val loss: 1.9328...  0.3636 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72751...  Training loss: 0.8105...  Val loss: 1.9235...  0.3634 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72776...  Training loss: 0.7979...  Val loss: 1.9060...  0.3632 sec/batch\n",
      "Epoch: 328/500...  Training Step: 72801...  Training loss: 0.8069...  Val loss: 1.9368...  0.3638 sec/batch\n",
      "Epoch: 329/500...  Training Step: 72826...  Training loss: 0.8032...  Val loss: 1.9084...  0.3633 sec/batch\n",
      "Epoch: 329/500...  Training Step: 72851...  Training loss: 0.8073...  Val loss: 1.9167...  0.3635 sec/batch\n",
      "Epoch: 329/500...  Training Step: 72876...  Training loss: 0.7948...  Val loss: 1.9252...  0.3631 sec/batch\n",
      "Epoch: 329/500...  Training Step: 72901...  Training loss: 0.8254...  Val loss: 1.9067...  0.3633 sec/batch\n",
      "Epoch: 329/500...  Training Step: 72926...  Training loss: 0.8066...  Val loss: 1.9122...  0.3634 sec/batch\n",
      "Epoch: 329/500...  Training Step: 72951...  Training loss: 0.8061...  Val loss: 1.9265...  0.3624 sec/batch\n",
      "Epoch: 329/500...  Training Step: 72976...  Training loss: 0.8164...  Val loss: 1.8969...  0.3634 sec/batch\n",
      "Epoch: 329/500...  Training Step: 73001...  Training loss: 0.8001...  Val loss: 1.9234...  0.3632 sec/batch\n",
      "Epoch: 329/500...  Training Step: 73026...  Training loss: 0.7913...  Val loss: 1.9120...  0.3633 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73051...  Training loss: 0.8105...  Val loss: 1.9047...  0.3635 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73076...  Training loss: 0.7971...  Val loss: 1.9251...  0.3635 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73101...  Training loss: 0.8003...  Val loss: 1.9316...  0.3634 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73126...  Training loss: 0.8073...  Val loss: 1.9209...  0.3637 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73151...  Training loss: 0.7784...  Val loss: 1.9137...  0.3628 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73176...  Training loss: 0.8091...  Val loss: 1.9139...  0.3631 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73201...  Training loss: 0.8079...  Val loss: 1.9086...  0.3631 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73226...  Training loss: 0.8122...  Val loss: 1.9168...  0.3638 sec/batch\n",
      "Epoch: 330/500...  Training Step: 73251...  Training loss: 0.8169...  Val loss: 1.9278...  0.3631 sec/batch\n",
      "Epoch 330/500 time:81.57103157043457...  finished at 2017-10-30 17:08:11\n",
      "Epoch: 331/500...  Training Step: 73276...  Training loss: 0.7796...  Val loss: 1.8999...  0.3636 sec/batch\n",
      "Epoch: 331/500...  Training Step: 73301...  Training loss: 0.8037...  Val loss: 1.9141...  0.3633 sec/batch\n",
      "Epoch: 331/500...  Training Step: 73326...  Training loss: 0.8024...  Val loss: 1.9279...  0.3631 sec/batch\n",
      "Epoch: 331/500...  Training Step: 73351...  Training loss: 0.7997...  Val loss: 1.9156...  0.3631 sec/batch\n",
      "Epoch: 331/500...  Training Step: 73376...  Training loss: 0.7991...  Val loss: 1.9135...  0.3632 sec/batch\n",
      "Epoch: 331/500...  Training Step: 73401...  Training loss: 0.7864...  Val loss: 1.9177...  0.3633 sec/batch\n",
      "Epoch: 331/500...  Training Step: 73426...  Training loss: 0.8142...  Val loss: 1.9022...  0.3635 sec/batch\n",
      "Epoch: 331/500...  Training Step: 73451...  Training loss: 0.8026...  Val loss: 1.9134...  0.3632 sec/batch\n",
      "Epoch: 331/500...  Training Step: 73476...  Training loss: 0.8018...  Val loss: 1.9185...  0.3632 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73501...  Training loss: 0.8155...  Val loss: 1.9066...  0.3629 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73526...  Training loss: 0.8035...  Val loss: 1.9113...  0.3635 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73551...  Training loss: 0.7930...  Val loss: 1.9192...  0.3636 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73576...  Training loss: 0.7918...  Val loss: 1.9110...  0.3634 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73601...  Training loss: 0.8124...  Val loss: 1.9265...  0.3635 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73626...  Training loss: 0.8209...  Val loss: 1.9232...  0.3633 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73651...  Training loss: 0.8013...  Val loss: 1.9328...  0.3631 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73676...  Training loss: 0.8054...  Val loss: 1.9341...  0.3631 sec/batch\n",
      "Epoch: 332/500...  Training Step: 73701...  Training loss: 0.8220...  Val loss: 1.9242...  0.3635 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73726...  Training loss: 0.7889...  Val loss: 1.8993...  0.3634 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73751...  Training loss: 0.7958...  Val loss: 1.9209...  0.3630 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73776...  Training loss: 0.8008...  Val loss: 1.9340...  0.3631 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73801...  Training loss: 0.7963...  Val loss: 1.9264...  0.3635 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73826...  Training loss: 0.8046...  Val loss: 1.9070...  0.3635 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73851...  Training loss: 0.8078...  Val loss: 1.8951...  0.3632 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73876...  Training loss: 0.8028...  Val loss: 1.9192...  0.3637 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73901...  Training loss: 0.8002...  Val loss: 1.9318...  0.3634 sec/batch\n",
      "Epoch: 333/500...  Training Step: 73926...  Training loss: 0.7950...  Val loss: 1.9196...  0.3630 sec/batch\n",
      "Epoch: 334/500...  Training Step: 73951...  Training loss: 0.7933...  Val loss: 1.9126...  0.3633 sec/batch\n",
      "Epoch: 334/500...  Training Step: 73976...  Training loss: 0.8185...  Val loss: 1.9251...  0.3638 sec/batch\n",
      "Epoch: 334/500...  Training Step: 74001...  Training loss: 0.8220...  Val loss: 1.9075...  0.3631 sec/batch\n",
      "Epoch: 334/500...  Training Step: 74026...  Training loss: 0.8014...  Val loss: 1.9305...  0.3636 sec/batch\n",
      "Epoch: 334/500...  Training Step: 74051...  Training loss: 0.7917...  Val loss: 1.9278...  0.3633 sec/batch\n",
      "Epoch: 334/500...  Training Step: 74076...  Training loss: 0.8093...  Val loss: 1.9197...  0.3635 sec/batch\n",
      "Epoch: 334/500...  Training Step: 74101...  Training loss: 0.8017...  Val loss: 1.9162...  0.3633 sec/batch\n",
      "Epoch: 334/500...  Training Step: 74126...  Training loss: 0.8066...  Val loss: 1.9361...  0.3642 sec/batch\n",
      "Epoch: 335/500...  Training Step: 74151...  Training loss: 0.8159...  Val loss: 1.9060...  0.3637 sec/batch\n",
      "Epoch: 335/500...  Training Step: 74176...  Training loss: 0.7912...  Val loss: 1.9082...  0.3634 sec/batch\n",
      "Epoch: 335/500...  Training Step: 74201...  Training loss: 0.7830...  Val loss: 1.9089...  0.3634 sec/batch\n",
      "Epoch: 335/500...  Training Step: 74226...  Training loss: 0.7864...  Val loss: 1.9379...  0.3637 sec/batch\n",
      "Epoch: 335/500...  Training Step: 74251...  Training loss: 0.7773...  Val loss: 1.9220...  0.3633 sec/batch\n",
      "Epoch: 335/500...  Training Step: 74276...  Training loss: 0.7942...  Val loss: 1.9320...  0.3637 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 335/500...  Training Step: 74301...  Training loss: 0.7929...  Val loss: 1.9125...  0.3630 sec/batch\n",
      "Epoch: 335/500...  Training Step: 74326...  Training loss: 0.7928...  Val loss: 1.9320...  0.3636 sec/batch\n",
      "Epoch: 335/500...  Training Step: 74351...  Training loss: 0.7892...  Val loss: 1.9317...  0.3635 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74376...  Training loss: 0.8043...  Val loss: 1.9042...  0.3636 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74401...  Training loss: 0.7886...  Val loss: 1.9088...  0.3633 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74426...  Training loss: 0.8110...  Val loss: 1.9133...  0.3636 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74451...  Training loss: 0.7856...  Val loss: 1.9111...  0.3633 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74476...  Training loss: 0.7822...  Val loss: 1.9321...  0.3636 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74501...  Training loss: 0.7960...  Val loss: 1.9247...  0.3634 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74526...  Training loss: 0.8027...  Val loss: 1.9335...  0.3634 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74551...  Training loss: 0.8024...  Val loss: 1.9087...  0.3638 sec/batch\n",
      "Epoch: 336/500...  Training Step: 74576...  Training loss: 0.7891...  Val loss: 1.9459...  0.3631 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74601...  Training loss: 0.7953...  Val loss: 1.9283...  0.3642 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74626...  Training loss: 0.8009...  Val loss: 1.9399...  0.3638 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74651...  Training loss: 0.8033...  Val loss: 1.9378...  0.3632 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74676...  Training loss: 0.8069...  Val loss: 1.9135...  0.3632 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74701...  Training loss: 0.7903...  Val loss: 1.9160...  0.3635 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74726...  Training loss: 0.7733...  Val loss: 1.9376...  0.3632 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74751...  Training loss: 0.8138...  Val loss: 1.9456...  0.3667 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74776...  Training loss: 0.8195...  Val loss: 1.9211...  0.3635 sec/batch\n",
      "Epoch: 337/500...  Training Step: 74801...  Training loss: 0.7925...  Val loss: 1.9257...  0.3634 sec/batch\n",
      "Epoch: 338/500...  Training Step: 74826...  Training loss: 0.7957...  Val loss: 1.9261...  0.3633 sec/batch\n",
      "Epoch: 338/500...  Training Step: 74851...  Training loss: 0.7921...  Val loss: 1.9144...  0.3631 sec/batch\n",
      "Epoch: 338/500...  Training Step: 74876...  Training loss: 0.8032...  Val loss: 1.9236...  0.3634 sec/batch\n",
      "Epoch: 338/500...  Training Step: 74901...  Training loss: 0.7928...  Val loss: 1.9180...  0.3633 sec/batch\n",
      "Epoch: 338/500...  Training Step: 74926...  Training loss: 0.7929...  Val loss: 1.9169...  0.3633 sec/batch\n",
      "Epoch: 338/500...  Training Step: 74951...  Training loss: 0.7784...  Val loss: 1.9491...  0.3631 sec/batch\n",
      "Epoch: 338/500...  Training Step: 74976...  Training loss: 0.8150...  Val loss: 1.9303...  0.3639 sec/batch\n",
      "Epoch: 338/500...  Training Step: 75001...  Training loss: 0.8008...  Val loss: 1.9196...  0.3636 sec/batch\n",
      "Epoch: 338/500...  Training Step: 75026...  Training loss: 0.7871...  Val loss: 1.9311...  0.3628 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75051...  Training loss: 0.8115...  Val loss: 1.9150...  0.3635 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75076...  Training loss: 0.7897...  Val loss: 1.9193...  0.3632 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75101...  Training loss: 0.8060...  Val loss: 1.9596...  0.3631 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75126...  Training loss: 0.7800...  Val loss: 1.9444...  0.3632 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75151...  Training loss: 0.8040...  Val loss: 1.9259...  0.3632 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75176...  Training loss: 0.7841...  Val loss: 1.9197...  0.3631 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75201...  Training loss: 0.7911...  Val loss: 1.9187...  0.3635 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75226...  Training loss: 0.8089...  Val loss: 1.9448...  0.3634 sec/batch\n",
      "Epoch: 339/500...  Training Step: 75251...  Training loss: 0.8108...  Val loss: 1.9540...  0.3633 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75276...  Training loss: 0.7942...  Val loss: 1.9075...  0.3635 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75301...  Training loss: 0.7919...  Val loss: 1.9370...  0.3630 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75326...  Training loss: 0.8056...  Val loss: 1.9385...  0.3634 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75351...  Training loss: 0.7839...  Val loss: 1.9162...  0.3633 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75376...  Training loss: 0.8051...  Val loss: 1.9224...  0.3635 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75401...  Training loss: 0.8076...  Val loss: 1.9258...  0.3637 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75426...  Training loss: 0.7959...  Val loss: 1.9338...  0.3630 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75451...  Training loss: 0.7909...  Val loss: 1.9408...  0.3636 sec/batch\n",
      "Epoch: 340/500...  Training Step: 75476...  Training loss: 0.8153...  Val loss: 1.9306...  0.3630 sec/batch\n",
      "Epoch 340/500 time:81.58417820930481...  finished at 2017-10-30 17:21:49\n",
      "Epoch: 341/500...  Training Step: 75501...  Training loss: 0.7959...  Val loss: 1.9307...  0.3637 sec/batch\n",
      "Epoch: 341/500...  Training Step: 75526...  Training loss: 0.7864...  Val loss: 1.9340...  0.3636 sec/batch\n",
      "Epoch: 341/500...  Training Step: 75551...  Training loss: 0.7992...  Val loss: 1.9474...  0.3636 sec/batch\n",
      "Epoch: 341/500...  Training Step: 75576...  Training loss: 0.7949...  Val loss: 1.9333...  0.3623 sec/batch\n",
      "Epoch: 341/500...  Training Step: 75601...  Training loss: 0.8001...  Val loss: 1.9245...  0.3630 sec/batch\n",
      "Epoch: 341/500...  Training Step: 75626...  Training loss: 0.8170...  Val loss: 1.9467...  0.3630 sec/batch\n",
      "Epoch: 341/500...  Training Step: 75651...  Training loss: 0.8282...  Val loss: 1.9425...  0.3637 sec/batch\n",
      "Epoch: 341/500...  Training Step: 75676...  Training loss: 0.8010...  Val loss: 1.9164...  0.3634 sec/batch\n",
      "Epoch: 341/500...  Training Step: 75701...  Training loss: 0.8065...  Val loss: 1.9507...  0.3633 sec/batch\n",
      "Epoch: 342/500...  Training Step: 75726...  Training loss: 0.7911...  Val loss: 1.9300...  0.3675 sec/batch\n",
      "Epoch: 342/500...  Training Step: 75751...  Training loss: 0.8027...  Val loss: 1.9449...  0.3640 sec/batch\n",
      "Epoch: 342/500...  Training Step: 75776...  Training loss: 0.7897...  Val loss: 1.9394...  0.3632 sec/batch\n",
      "Epoch: 342/500...  Training Step: 75801...  Training loss: 0.7950...  Val loss: 1.9267...  0.3635 sec/batch\n",
      "Epoch: 342/500...  Training Step: 75826...  Training loss: 0.7817...  Val loss: 1.9267...  0.3635 sec/batch\n",
      "Epoch: 342/500...  Training Step: 75851...  Training loss: 0.7995...  Val loss: 1.9151...  0.3632 sec/batch\n",
      "Epoch: 342/500...  Training Step: 75876...  Training loss: 0.8157...  Val loss: 1.9255...  0.3627 sec/batch\n",
      "Epoch: 342/500...  Training Step: 75901...  Training loss: 0.8024...  Val loss: 1.9426...  0.3633 sec/batch\n",
      "Epoch: 343/500...  Training Step: 75926...  Training loss: 0.7873...  Val loss: 1.9238...  0.3634 sec/batch\n",
      "Epoch: 343/500...  Training Step: 75951...  Training loss: 0.7888...  Val loss: 1.9156...  0.3631 sec/batch\n",
      "Epoch: 343/500...  Training Step: 75976...  Training loss: 0.7964...  Val loss: 1.9181...  0.3634 sec/batch\n",
      "Epoch: 343/500...  Training Step: 76001...  Training loss: 0.7698...  Val loss: 1.9330...  0.3635 sec/batch\n",
      "Epoch: 343/500...  Training Step: 76026...  Training loss: 0.8019...  Val loss: 1.9316...  0.3632 sec/batch\n",
      "Epoch: 343/500...  Training Step: 76051...  Training loss: 0.7929...  Val loss: 1.9563...  0.3635 sec/batch\n",
      "Epoch: 343/500...  Training Step: 76076...  Training loss: 0.7937...  Val loss: 1.9246...  0.3636 sec/batch\n",
      "Epoch: 343/500...  Training Step: 76101...  Training loss: 0.7833...  Val loss: 1.9396...  0.3631 sec/batch\n",
      "Epoch: 343/500...  Training Step: 76126...  Training loss: 0.7782...  Val loss: 1.9375...  0.3634 sec/batch\n",
      "Epoch: 344/500...  Training Step: 76151...  Training loss: 0.8169...  Val loss: 1.9435...  0.3635 sec/batch\n",
      "Epoch: 344/500...  Training Step: 76176...  Training loss: 0.7606...  Val loss: 1.9251...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 344/500...  Training Step: 76201...  Training loss: 0.7675...  Val loss: 1.9324...  0.3633 sec/batch\n",
      "Epoch: 344/500...  Training Step: 76226...  Training loss: 0.7964...  Val loss: 1.9313...  0.3637 sec/batch\n",
      "Epoch: 344/500...  Training Step: 76251...  Training loss: 0.7865...  Val loss: 1.9143...  0.3632 sec/batch\n",
      "Epoch: 344/500...  Training Step: 76276...  Training loss: 0.7814...  Val loss: 1.9454...  0.3635 sec/batch\n",
      "Epoch: 344/500...  Training Step: 76301...  Training loss: 0.7828...  Val loss: 1.9439...  0.3633 sec/batch\n",
      "Epoch: 344/500...  Training Step: 76326...  Training loss: 0.7886...  Val loss: 1.9229...  0.3633 sec/batch\n",
      "Epoch: 344/500...  Training Step: 76351...  Training loss: 0.7790...  Val loss: 1.9409...  0.3629 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76376...  Training loss: 0.7930...  Val loss: 1.9064...  0.3623 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76401...  Training loss: 0.7904...  Val loss: 1.9386...  0.3633 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76426...  Training loss: 0.7966...  Val loss: 1.9560...  0.3633 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76451...  Training loss: 0.7804...  Val loss: 1.9388...  0.3633 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76476...  Training loss: 0.7805...  Val loss: 1.9389...  0.3631 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76501...  Training loss: 0.7830...  Val loss: 1.9550...  0.3633 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76526...  Training loss: 0.7974...  Val loss: 1.9423...  0.3637 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76551...  Training loss: 0.7866...  Val loss: 1.9177...  0.3629 sec/batch\n",
      "Epoch: 345/500...  Training Step: 76576...  Training loss: 0.7883...  Val loss: 1.9453...  0.3636 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76601...  Training loss: 0.7882...  Val loss: 1.9435...  0.3639 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76626...  Training loss: 0.7775...  Val loss: 1.9406...  0.3629 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76651...  Training loss: 0.7901...  Val loss: 1.9515...  0.3633 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76676...  Training loss: 0.7927...  Val loss: 1.9328...  0.3634 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76701...  Training loss: 0.7982...  Val loss: 1.9453...  0.3634 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76726...  Training loss: 0.7788...  Val loss: 1.9402...  0.3635 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76751...  Training loss: 0.8060...  Val loss: 1.9423...  0.3637 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76776...  Training loss: 0.8055...  Val loss: 1.9415...  0.3634 sec/batch\n",
      "Epoch: 346/500...  Training Step: 76801...  Training loss: 0.7914...  Val loss: 1.9464...  0.3637 sec/batch\n",
      "Epoch: 347/500...  Training Step: 76826...  Training loss: 0.7777...  Val loss: 1.9298...  0.3634 sec/batch\n",
      "Epoch: 347/500...  Training Step: 76851...  Training loss: 0.7845...  Val loss: 1.9416...  0.3632 sec/batch\n",
      "Epoch: 347/500...  Training Step: 76876...  Training loss: 0.7969...  Val loss: 1.9471...  0.3631 sec/batch\n",
      "Epoch: 347/500...  Training Step: 76901...  Training loss: 0.7812...  Val loss: 1.9464...  0.3633 sec/batch\n",
      "Epoch: 347/500...  Training Step: 76926...  Training loss: 0.7771...  Val loss: 1.9412...  0.3638 sec/batch\n",
      "Epoch: 347/500...  Training Step: 76951...  Training loss: 0.8070...  Val loss: 1.9447...  0.3639 sec/batch\n",
      "Epoch: 347/500...  Training Step: 76976...  Training loss: 0.7917...  Val loss: 1.9472...  0.3635 sec/batch\n",
      "Epoch: 347/500...  Training Step: 77001...  Training loss: 0.7968...  Val loss: 1.9284...  0.3632 sec/batch\n",
      "Epoch: 347/500...  Training Step: 77026...  Training loss: 0.7864...  Val loss: 1.9535...  0.3622 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77051...  Training loss: 0.7934...  Val loss: 1.9270...  0.3630 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77076...  Training loss: 0.8049...  Val loss: 1.9254...  0.3634 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77101...  Training loss: 0.7749...  Val loss: 1.9662...  0.3640 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77126...  Training loss: 0.7804...  Val loss: 1.9296...  0.3635 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77151...  Training loss: 0.7972...  Val loss: 1.9435...  0.3632 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77176...  Training loss: 0.7854...  Val loss: 1.9210...  0.3620 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77201...  Training loss: 0.7879...  Val loss: 1.9444...  0.3635 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77226...  Training loss: 0.8036...  Val loss: 1.9400...  0.3635 sec/batch\n",
      "Epoch: 348/500...  Training Step: 77251...  Training loss: 0.7912...  Val loss: 1.9468...  0.3632 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77276...  Training loss: 0.7838...  Val loss: 1.8941...  0.3634 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77301...  Training loss: 0.7861...  Val loss: 1.9330...  0.3636 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77326...  Training loss: 0.7955...  Val loss: 1.9330...  0.3633 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77351...  Training loss: 0.7890...  Val loss: 1.9383...  0.3632 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77376...  Training loss: 0.7878...  Val loss: 1.9354...  0.3634 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77401...  Training loss: 0.7907...  Val loss: 1.9229...  0.3629 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77426...  Training loss: 0.7899...  Val loss: 1.9427...  0.3631 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77451...  Training loss: 0.7830...  Val loss: 1.9375...  0.3635 sec/batch\n",
      "Epoch: 349/500...  Training Step: 77476...  Training loss: 0.7788...  Val loss: 1.9464...  0.3631 sec/batch\n",
      "Epoch: 350/500...  Training Step: 77501...  Training loss: 0.7856...  Val loss: 1.9280...  0.3630 sec/batch\n",
      "Epoch: 350/500...  Training Step: 77526...  Training loss: 0.7786...  Val loss: 1.9411...  0.3631 sec/batch\n",
      "Epoch: 350/500...  Training Step: 77551...  Training loss: 0.7849...  Val loss: 1.9229...  0.3640 sec/batch\n",
      "Epoch: 350/500...  Training Step: 77576...  Training loss: 0.7761...  Val loss: 1.9313...  0.3632 sec/batch\n",
      "Epoch: 350/500...  Training Step: 77601...  Training loss: 0.7786...  Val loss: 1.9541...  0.3633 sec/batch\n",
      "Epoch: 350/500...  Training Step: 77626...  Training loss: 0.7980...  Val loss: 1.9344...  0.3637 sec/batch\n",
      "Epoch: 350/500...  Training Step: 77651...  Training loss: 0.8079...  Val loss: 1.9447...  0.3640 sec/batch\n",
      "Epoch: 350/500...  Training Step: 77676...  Training loss: 0.7901...  Val loss: 1.9373...  0.3636 sec/batch\n",
      "Epoch 350/500 time:81.82977819442749...  finished at 2017-10-30 17:35:26\n",
      "Epoch: 351/500...  Training Step: 77701...  Training loss: 0.8814...  Val loss: 1.9374...  0.3629 sec/batch\n",
      "Epoch: 351/500...  Training Step: 77726...  Training loss: 0.7939...  Val loss: 1.9308...  0.3633 sec/batch\n",
      "Epoch: 351/500...  Training Step: 77751...  Training loss: 0.7765...  Val loss: 1.9497...  0.3639 sec/batch\n",
      "Epoch: 351/500...  Training Step: 77776...  Training loss: 0.7874...  Val loss: 1.9338...  0.3634 sec/batch\n",
      "Epoch: 351/500...  Training Step: 77801...  Training loss: 0.7927...  Val loss: 1.9490...  0.3634 sec/batch\n",
      "Epoch: 351/500...  Training Step: 77826...  Training loss: 0.7801...  Val loss: 1.9576...  0.3626 sec/batch\n",
      "Epoch: 351/500...  Training Step: 77851...  Training loss: 0.7764...  Val loss: 1.9382...  0.3630 sec/batch\n",
      "Epoch: 351/500...  Training Step: 77876...  Training loss: 0.7831...  Val loss: 1.9453...  0.3636 sec/batch\n",
      "Epoch: 351/500...  Training Step: 77901...  Training loss: 0.7863...  Val loss: 1.9586...  0.3638 sec/batch\n",
      "Epoch: 352/500...  Training Step: 77926...  Training loss: 0.7914...  Val loss: 1.9448...  0.3636 sec/batch\n",
      "Epoch: 352/500...  Training Step: 77951...  Training loss: 0.7883...  Val loss: 1.9411...  0.3635 sec/batch\n",
      "Epoch: 352/500...  Training Step: 77976...  Training loss: 0.7699...  Val loss: 1.9400...  0.3631 sec/batch\n",
      "Epoch: 352/500...  Training Step: 78001...  Training loss: 0.7726...  Val loss: 1.9571...  0.3636 sec/batch\n",
      "Epoch: 352/500...  Training Step: 78026...  Training loss: 0.7840...  Val loss: 1.9502...  0.3634 sec/batch\n",
      "Epoch: 352/500...  Training Step: 78051...  Training loss: 0.7835...  Val loss: 1.9442...  0.3631 sec/batch\n",
      "Epoch: 352/500...  Training Step: 78076...  Training loss: 0.7816...  Val loss: 1.9482...  0.3636 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 352/500...  Training Step: 78101...  Training loss: 0.7899...  Val loss: 1.9477...  0.3632 sec/batch\n",
      "Epoch: 352/500...  Training Step: 78126...  Training loss: 0.7902...  Val loss: 1.9484...  0.3632 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78151...  Training loss: 0.8011...  Val loss: 1.9463...  0.3635 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78176...  Training loss: 0.8118...  Val loss: 1.9600...  0.3617 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78201...  Training loss: 0.7706...  Val loss: 1.9492...  0.3633 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78226...  Training loss: 0.7825...  Val loss: 1.9413...  0.3633 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78251...  Training loss: 0.7755...  Val loss: 1.9489...  0.3632 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78276...  Training loss: 0.7966...  Val loss: 1.9557...  0.3634 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78301...  Training loss: 0.7968...  Val loss: 1.9410...  0.3633 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78326...  Training loss: 0.7959...  Val loss: 1.9411...  0.3630 sec/batch\n",
      "Epoch: 353/500...  Training Step: 78351...  Training loss: 0.7849...  Val loss: 1.9601...  0.3630 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78376...  Training loss: 0.7706...  Val loss: 1.9293...  0.3636 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78401...  Training loss: 0.7943...  Val loss: 1.9532...  0.3630 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78426...  Training loss: 0.7854...  Val loss: 1.9321...  0.3634 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78451...  Training loss: 0.8006...  Val loss: 1.9234...  0.3635 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78476...  Training loss: 0.7831...  Val loss: 1.9340...  0.3636 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78501...  Training loss: 0.7909...  Val loss: 1.9649...  0.3632 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78526...  Training loss: 0.7829...  Val loss: 1.9275...  0.3634 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78551...  Training loss: 0.7824...  Val loss: 1.9529...  0.3638 sec/batch\n",
      "Epoch: 354/500...  Training Step: 78576...  Training loss: 0.7772...  Val loss: 1.9517...  0.3639 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78601...  Training loss: 0.7920...  Val loss: 1.9357...  0.3632 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78626...  Training loss: 0.7705...  Val loss: 1.9500...  0.3636 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78651...  Training loss: 0.7941...  Val loss: 1.9569...  0.3633 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78676...  Training loss: 0.7793...  Val loss: 1.9458...  0.3632 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78701...  Training loss: 0.7683...  Val loss: 1.9551...  0.3633 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78726...  Training loss: 0.7893...  Val loss: 1.9534...  0.3630 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78751...  Training loss: 0.7832...  Val loss: 1.9421...  0.3638 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78776...  Training loss: 0.7864...  Val loss: 1.9505...  0.3639 sec/batch\n",
      "Epoch: 355/500...  Training Step: 78801...  Training loss: 0.8068...  Val loss: 1.9707...  0.3633 sec/batch\n",
      "Epoch: 356/500...  Training Step: 78826...  Training loss: 0.7716...  Val loss: 1.9388...  0.3637 sec/batch\n",
      "Epoch: 356/500...  Training Step: 78851...  Training loss: 0.7712...  Val loss: 1.9489...  0.3637 sec/batch\n",
      "Epoch: 356/500...  Training Step: 78876...  Training loss: 0.7768...  Val loss: 1.9534...  0.3633 sec/batch\n",
      "Epoch: 356/500...  Training Step: 78901...  Training loss: 0.7839...  Val loss: 1.9399...  0.3638 sec/batch\n",
      "Epoch: 356/500...  Training Step: 78926...  Training loss: 0.7818...  Val loss: 1.9391...  0.3635 sec/batch\n",
      "Epoch: 356/500...  Training Step: 78951...  Training loss: 0.7749...  Val loss: 1.9477...  0.3636 sec/batch\n",
      "Epoch: 356/500...  Training Step: 78976...  Training loss: 0.7769...  Val loss: 1.9367...  0.3630 sec/batch\n",
      "Epoch: 356/500...  Training Step: 79001...  Training loss: 0.7844...  Val loss: 1.9362...  0.3629 sec/batch\n",
      "Epoch: 356/500...  Training Step: 79026...  Training loss: 0.7845...  Val loss: 1.9549...  0.3634 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79051...  Training loss: 0.7797...  Val loss: 1.9494...  0.3634 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79076...  Training loss: 0.7914...  Val loss: 1.9435...  0.3634 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79101...  Training loss: 0.7938...  Val loss: 1.9711...  0.3639 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79126...  Training loss: 0.7758...  Val loss: 1.9526...  0.3636 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79151...  Training loss: 0.7907...  Val loss: 1.9564...  0.3635 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79176...  Training loss: 0.7867...  Val loss: 1.9636...  0.3633 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79201...  Training loss: 0.7725...  Val loss: 1.9563...  0.3635 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79226...  Training loss: 0.7906...  Val loss: 1.9675...  0.3631 sec/batch\n",
      "Epoch: 357/500...  Training Step: 79251...  Training loss: 0.7969...  Val loss: 1.9605...  0.3636 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79276...  Training loss: 0.7622...  Val loss: 1.9354...  0.3633 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79301...  Training loss: 0.7803...  Val loss: 1.9562...  0.3635 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79326...  Training loss: 0.7818...  Val loss: 1.9745...  0.3634 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79351...  Training loss: 0.7670...  Val loss: 1.9526...  0.3632 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79376...  Training loss: 0.7952...  Val loss: 1.9351...  0.3631 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79401...  Training loss: 0.7988...  Val loss: 1.9254...  0.3634 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79426...  Training loss: 0.7879...  Val loss: 1.9524...  0.3636 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79451...  Training loss: 0.7890...  Val loss: 1.9580...  0.3641 sec/batch\n",
      "Epoch: 358/500...  Training Step: 79476...  Training loss: 0.7847...  Val loss: 1.9510...  0.3633 sec/batch\n",
      "Epoch: 359/500...  Training Step: 79501...  Training loss: 0.7819...  Val loss: 1.9307...  0.3632 sec/batch\n",
      "Epoch: 359/500...  Training Step: 79526...  Training loss: 0.7838...  Val loss: 1.9524...  0.3636 sec/batch\n",
      "Epoch: 359/500...  Training Step: 79551...  Training loss: 0.8021...  Val loss: 1.9472...  0.3635 sec/batch\n",
      "Epoch: 359/500...  Training Step: 79576...  Training loss: 0.7877...  Val loss: 1.9609...  0.3632 sec/batch\n",
      "Epoch: 359/500...  Training Step: 79601...  Training loss: 0.7769...  Val loss: 1.9687...  0.3631 sec/batch\n",
      "Epoch: 359/500...  Training Step: 79626...  Training loss: 0.7753...  Val loss: 1.9488...  0.3633 sec/batch\n",
      "Epoch: 359/500...  Training Step: 79651...  Training loss: 0.7832...  Val loss: 1.9580...  0.3626 sec/batch\n",
      "Epoch: 359/500...  Training Step: 79676...  Training loss: 0.7749...  Val loss: 1.9677...  0.3634 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79701...  Training loss: 0.7920...  Val loss: 1.9428...  0.3634 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79726...  Training loss: 0.7849...  Val loss: 1.9409...  0.3638 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79751...  Training loss: 0.7463...  Val loss: 1.9396...  0.3633 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79776...  Training loss: 0.7622...  Val loss: 1.9734...  0.3634 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79801...  Training loss: 0.7687...  Val loss: 1.9506...  0.3634 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79826...  Training loss: 0.7804...  Val loss: 1.9724...  0.3635 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79851...  Training loss: 0.7652...  Val loss: 1.9431...  0.3634 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79876...  Training loss: 0.7862...  Val loss: 1.9810...  0.3633 sec/batch\n",
      "Epoch: 360/500...  Training Step: 79901...  Training loss: 0.7763...  Val loss: 1.9634...  0.3631 sec/batch\n",
      "Epoch 360/500 time:81.58099937438965...  finished at 2017-10-30 17:49:03\n",
      "Epoch: 361/500...  Training Step: 79926...  Training loss: 0.7880...  Val loss: 1.9415...  0.3633 sec/batch\n",
      "Epoch: 361/500...  Training Step: 79951...  Training loss: 0.7541...  Val loss: 1.9327...  0.3631 sec/batch\n",
      "Epoch: 361/500...  Training Step: 79976...  Training loss: 0.7865...  Val loss: 1.9557...  0.3637 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 361/500...  Training Step: 80001...  Training loss: 0.7761...  Val loss: 1.9477...  0.3635 sec/batch\n",
      "Epoch: 361/500...  Training Step: 80026...  Training loss: 0.7713...  Val loss: 1.9739...  0.3637 sec/batch\n",
      "Epoch: 361/500...  Training Step: 80051...  Training loss: 0.7690...  Val loss: 1.9628...  0.3635 sec/batch\n",
      "Epoch: 361/500...  Training Step: 80076...  Training loss: 0.7888...  Val loss: 1.9688...  0.3638 sec/batch\n",
      "Epoch: 361/500...  Training Step: 80101...  Training loss: 0.7829...  Val loss: 1.9419...  0.3634 sec/batch\n",
      "Epoch: 361/500...  Training Step: 80126...  Training loss: 0.7723...  Val loss: 1.9699...  0.3637 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80151...  Training loss: 0.7774...  Val loss: 1.9699...  0.3632 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80176...  Training loss: 0.7807...  Val loss: 1.9697...  0.3633 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80201...  Training loss: 0.7855...  Val loss: 1.9787...  0.3635 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80226...  Training loss: 0.7870...  Val loss: 1.9552...  0.3633 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80251...  Training loss: 0.7722...  Val loss: 1.9490...  0.3632 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80276...  Training loss: 0.7742...  Val loss: 1.9837...  0.3634 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80301...  Training loss: 0.7866...  Val loss: 1.9807...  0.3628 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80326...  Training loss: 0.7991...  Val loss: 1.9492...  0.3631 sec/batch\n",
      "Epoch: 362/500...  Training Step: 80351...  Training loss: 0.7792...  Val loss: 1.9681...  0.3631 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80376...  Training loss: 0.7700...  Val loss: 1.9612...  0.3633 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80401...  Training loss: 0.7662...  Val loss: 1.9534...  0.3633 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80426...  Training loss: 0.7756...  Val loss: 1.9495...  0.3634 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80451...  Training loss: 0.7759...  Val loss: 1.9389...  0.3633 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80476...  Training loss: 0.7805...  Val loss: 1.9491...  0.3635 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80501...  Training loss: 0.7728...  Val loss: 1.9798...  0.3633 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80526...  Training loss: 0.7886...  Val loss: 1.9576...  0.3638 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80551...  Training loss: 0.7936...  Val loss: 1.9509...  0.3633 sec/batch\n",
      "Epoch: 363/500...  Training Step: 80576...  Training loss: 0.7685...  Val loss: 1.9599...  0.3632 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80601...  Training loss: 0.7975...  Val loss: 1.9648...  0.3636 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80626...  Training loss: 0.7715...  Val loss: 1.9586...  0.3632 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80651...  Training loss: 0.7929...  Val loss: 1.9776...  0.3634 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80676...  Training loss: 0.7628...  Val loss: 1.9734...  0.3634 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80701...  Training loss: 0.7878...  Val loss: 1.9530...  0.3632 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80726...  Training loss: 0.7678...  Val loss: 1.9668...  0.3631 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80751...  Training loss: 0.7688...  Val loss: 1.9534...  0.3629 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80776...  Training loss: 0.7919...  Val loss: 1.9701...  0.3639 sec/batch\n",
      "Epoch: 364/500...  Training Step: 80801...  Training loss: 0.7962...  Val loss: 1.9862...  0.3632 sec/batch\n",
      "Epoch: 365/500...  Training Step: 80826...  Training loss: 0.7775...  Val loss: 1.9329...  0.3633 sec/batch\n",
      "Epoch: 365/500...  Training Step: 80851...  Training loss: 0.7570...  Val loss: 1.9702...  0.3629 sec/batch\n",
      "Epoch: 365/500...  Training Step: 80876...  Training loss: 0.7960...  Val loss: 1.9758...  0.3634 sec/batch\n",
      "Epoch: 365/500...  Training Step: 80901...  Training loss: 0.7533...  Val loss: 1.9462...  0.3631 sec/batch\n",
      "Epoch: 365/500...  Training Step: 80926...  Training loss: 0.7787...  Val loss: 1.9487...  0.3633 sec/batch\n",
      "Epoch: 365/500...  Training Step: 80951...  Training loss: 0.7882...  Val loss: 1.9608...  0.3631 sec/batch\n",
      "Epoch: 365/500...  Training Step: 80976...  Training loss: 0.7757...  Val loss: 1.9557...  0.3632 sec/batch\n",
      "Epoch: 365/500...  Training Step: 81001...  Training loss: 0.7683...  Val loss: 1.9719...  0.3632 sec/batch\n",
      "Epoch: 365/500...  Training Step: 81026...  Training loss: 0.7884...  Val loss: 1.9581...  0.3631 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81051...  Training loss: 0.7852...  Val loss: 1.9712...  0.3642 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81076...  Training loss: 0.7603...  Val loss: 1.9678...  0.3630 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81101...  Training loss: 0.7938...  Val loss: 1.9770...  0.3630 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81126...  Training loss: 0.7799...  Val loss: 1.9586...  0.3632 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81151...  Training loss: 0.7717...  Val loss: 1.9452...  0.3633 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81176...  Training loss: 0.7847...  Val loss: 1.9742...  0.3636 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81201...  Training loss: 0.8167...  Val loss: 1.9759...  0.3632 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81226...  Training loss: 0.7890...  Val loss: 1.9553...  0.3630 sec/batch\n",
      "Epoch: 366/500...  Training Step: 81251...  Training loss: 0.7824...  Val loss: 1.9771...  0.3631 sec/batch\n",
      "Epoch: 367/500...  Training Step: 81276...  Training loss: 0.7767...  Val loss: 1.9697...  0.3630 sec/batch\n",
      "Epoch: 367/500...  Training Step: 81301...  Training loss: 0.7838...  Val loss: 1.9845...  0.3630 sec/batch\n",
      "Epoch: 367/500...  Training Step: 81326...  Training loss: 0.7794...  Val loss: 1.9831...  0.3628 sec/batch\n",
      "Epoch: 367/500...  Training Step: 81351...  Training loss: 0.7747...  Val loss: 1.9651...  0.3635 sec/batch\n",
      "Epoch: 367/500...  Training Step: 81376...  Training loss: 0.7534...  Val loss: 1.9629...  0.3632 sec/batch\n",
      "Epoch: 367/500...  Training Step: 81401...  Training loss: 0.7852...  Val loss: 1.9525...  0.3631 sec/batch\n",
      "Epoch: 367/500...  Training Step: 81426...  Training loss: 0.7925...  Val loss: 1.9587...  0.3633 sec/batch\n",
      "Epoch: 367/500...  Training Step: 81451...  Training loss: 0.7845...  Val loss: 1.9678...  0.3634 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81476...  Training loss: 0.7708...  Val loss: 1.9608...  0.3632 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81501...  Training loss: 0.7845...  Val loss: 1.9483...  0.3633 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81526...  Training loss: 0.7862...  Val loss: 1.9497...  0.3638 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81551...  Training loss: 0.7647...  Val loss: 1.9776...  0.3627 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81576...  Training loss: 0.7831...  Val loss: 1.9670...  0.3633 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81601...  Training loss: 0.7907...  Val loss: 1.9861...  0.3634 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81626...  Training loss: 0.7724...  Val loss: 1.9534...  0.3629 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81651...  Training loss: 0.7718...  Val loss: 1.9803...  0.3631 sec/batch\n",
      "Epoch: 368/500...  Training Step: 81676...  Training loss: 0.7572...  Val loss: 1.9693...  0.3634 sec/batch\n",
      "Epoch: 369/500...  Training Step: 81701...  Training loss: 0.7924...  Val loss: 1.9702...  0.3634 sec/batch\n",
      "Epoch: 369/500...  Training Step: 81726...  Training loss: 0.7611...  Val loss: 1.9516...  0.3633 sec/batch\n",
      "Epoch: 369/500...  Training Step: 81751...  Training loss: 0.7571...  Val loss: 1.9608...  0.3634 sec/batch\n",
      "Epoch: 369/500...  Training Step: 81776...  Training loss: 0.7784...  Val loss: 1.9697...  0.3634 sec/batch\n",
      "Epoch: 369/500...  Training Step: 81801...  Training loss: 0.7758...  Val loss: 1.9556...  0.3631 sec/batch\n",
      "Epoch: 369/500...  Training Step: 81826...  Training loss: 0.7610...  Val loss: 1.9842...  0.3633 sec/batch\n",
      "Epoch: 369/500...  Training Step: 81851...  Training loss: 0.7677...  Val loss: 1.9691...  0.3636 sec/batch\n",
      "Epoch: 369/500...  Training Step: 81876...  Training loss: 0.7694...  Val loss: 1.9580...  0.3631 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 369/500...  Training Step: 81901...  Training loss: 0.7631...  Val loss: 1.9655...  0.3635 sec/batch\n",
      "Epoch: 370/500...  Training Step: 81926...  Training loss: 0.7659...  Val loss: 1.9246...  0.3637 sec/batch\n",
      "Epoch: 370/500...  Training Step: 81951...  Training loss: 0.7731...  Val loss: 1.9672...  0.3631 sec/batch\n",
      "Epoch: 370/500...  Training Step: 81976...  Training loss: 0.7683...  Val loss: 1.9777...  0.3631 sec/batch\n",
      "Epoch: 370/500...  Training Step: 82001...  Training loss: 0.7652...  Val loss: 1.9740...  0.3633 sec/batch\n",
      "Epoch: 370/500...  Training Step: 82026...  Training loss: 0.7670...  Val loss: 1.9819...  0.3634 sec/batch\n",
      "Epoch: 370/500...  Training Step: 82051...  Training loss: 0.7808...  Val loss: 1.9897...  0.3638 sec/batch\n",
      "Epoch: 370/500...  Training Step: 82076...  Training loss: 0.7842...  Val loss: 1.9678...  0.3640 sec/batch\n",
      "Epoch: 370/500...  Training Step: 82101...  Training loss: 0.7792...  Val loss: 1.9508...  0.3631 sec/batch\n",
      "Epoch: 370/500...  Training Step: 82126...  Training loss: 0.7815...  Val loss: 1.9757...  0.3634 sec/batch\n",
      "Epoch 370/500 time:81.93950891494751...  finished at 2017-10-30 18:02:41\n",
      "Epoch: 371/500...  Training Step: 82151...  Training loss: 0.7633...  Val loss: 1.9683...  0.3636 sec/batch\n",
      "Epoch: 371/500...  Training Step: 82176...  Training loss: 0.7678...  Val loss: 1.9638...  0.3632 sec/batch\n",
      "Epoch: 371/500...  Training Step: 82201...  Training loss: 0.7671...  Val loss: 1.9700...  0.3629 sec/batch\n",
      "Epoch: 371/500...  Training Step: 82226...  Training loss: 0.7603...  Val loss: 1.9553...  0.3634 sec/batch\n",
      "Epoch: 371/500...  Training Step: 82251...  Training loss: 0.7892...  Val loss: 1.9789...  0.3636 sec/batch\n",
      "Epoch: 371/500...  Training Step: 82276...  Training loss: 0.7633...  Val loss: 1.9780...  0.3637 sec/batch\n",
      "Epoch: 371/500...  Training Step: 82301...  Training loss: 0.7795...  Val loss: 1.9698...  0.3630 sec/batch\n",
      "Epoch: 371/500...  Training Step: 82326...  Training loss: 0.8010...  Val loss: 1.9764...  0.3636 sec/batch\n",
      "Epoch: 371/500...  Training Step: 82351...  Training loss: 0.7775...  Val loss: 1.9770...  0.3635 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82376...  Training loss: 0.7720...  Val loss: 1.9699...  0.3635 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82401...  Training loss: 0.7628...  Val loss: 1.9733...  0.3637 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82426...  Training loss: 0.7832...  Val loss: 1.9710...  0.3635 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82451...  Training loss: 0.7816...  Val loss: 1.9707...  0.3633 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82476...  Training loss: 0.7712...  Val loss: 1.9714...  0.3633 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82501...  Training loss: 0.7859...  Val loss: 1.9826...  0.3625 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82526...  Training loss: 0.7703...  Val loss: 1.9776...  0.3632 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82551...  Training loss: 0.7846...  Val loss: 1.9534...  0.3631 sec/batch\n",
      "Epoch: 372/500...  Training Step: 82576...  Training loss: 0.7713...  Val loss: 1.9729...  0.3632 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82601...  Training loss: 0.7766...  Val loss: 1.9597...  0.3634 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82626...  Training loss: 0.7785...  Val loss: 1.9539...  0.3638 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82651...  Training loss: 0.7620...  Val loss: 1.9862...  0.3635 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82676...  Training loss: 0.7682...  Val loss: 1.9533...  0.3633 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82701...  Training loss: 0.7720...  Val loss: 1.9633...  0.3635 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82726...  Training loss: 0.7682...  Val loss: 1.9597...  0.3638 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82751...  Training loss: 0.7626...  Val loss: 1.9670...  0.3632 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82776...  Training loss: 0.7814...  Val loss: 1.9612...  0.3632 sec/batch\n",
      "Epoch: 373/500...  Training Step: 82801...  Training loss: 0.7698...  Val loss: 1.9817...  0.3636 sec/batch\n",
      "Epoch: 374/500...  Training Step: 82826...  Training loss: 0.7747...  Val loss: 1.9341...  0.3634 sec/batch\n",
      "Epoch: 374/500...  Training Step: 82851...  Training loss: 0.7561...  Val loss: 1.9772...  0.3635 sec/batch\n",
      "Epoch: 374/500...  Training Step: 82876...  Training loss: 0.7699...  Val loss: 1.9784...  0.3629 sec/batch\n",
      "Epoch: 374/500...  Training Step: 82901...  Training loss: 0.7676...  Val loss: 1.9695...  0.3642 sec/batch\n",
      "Epoch: 374/500...  Training Step: 82926...  Training loss: 0.7596...  Val loss: 1.9641...  0.3636 sec/batch\n",
      "Epoch: 374/500...  Training Step: 82951...  Training loss: 0.7676...  Val loss: 1.9602...  0.3639 sec/batch\n",
      "Epoch: 374/500...  Training Step: 82976...  Training loss: 0.7728...  Val loss: 1.9717...  0.3633 sec/batch\n",
      "Epoch: 374/500...  Training Step: 83001...  Training loss: 0.7689...  Val loss: 1.9666...  0.3631 sec/batch\n",
      "Epoch: 374/500...  Training Step: 83026...  Training loss: 0.7688...  Val loss: 1.9759...  0.3633 sec/batch\n",
      "Epoch: 375/500...  Training Step: 83051...  Training loss: 0.7746...  Val loss: 1.9558...  0.3633 sec/batch\n",
      "Epoch: 375/500...  Training Step: 83076...  Training loss: 0.7565...  Val loss: 1.9795...  0.3636 sec/batch\n",
      "Epoch: 375/500...  Training Step: 83101...  Training loss: 0.7571...  Val loss: 1.9566...  0.3636 sec/batch\n",
      "Epoch: 375/500...  Training Step: 83126...  Training loss: 0.7685...  Val loss: 1.9597...  0.3635 sec/batch\n",
      "Epoch: 375/500...  Training Step: 83151...  Training loss: 0.7569...  Val loss: 1.9846...  0.3637 sec/batch\n",
      "Epoch: 375/500...  Training Step: 83176...  Training loss: 0.7805...  Val loss: 1.9586...  0.3631 sec/batch\n",
      "Epoch: 375/500...  Training Step: 83201...  Training loss: 0.7910...  Val loss: 1.9738...  0.3631 sec/batch\n",
      "Epoch: 375/500...  Training Step: 83226...  Training loss: 0.7850...  Val loss: 1.9719...  0.3637 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83251...  Training loss: 0.8589...  Val loss: 1.9734...  0.3633 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83276...  Training loss: 0.7815...  Val loss: 1.9619...  0.3632 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83301...  Training loss: 0.7468...  Val loss: 1.9824...  0.3633 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83326...  Training loss: 0.7546...  Val loss: 1.9703...  0.3634 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83351...  Training loss: 0.7702...  Val loss: 1.9709...  0.3637 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83376...  Training loss: 0.7713...  Val loss: 1.9721...  0.3633 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83401...  Training loss: 0.7687...  Val loss: 1.9747...  0.3636 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83426...  Training loss: 0.7675...  Val loss: 1.9689...  0.3634 sec/batch\n",
      "Epoch: 376/500...  Training Step: 83451...  Training loss: 0.7724...  Val loss: 1.9892...  0.3636 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83476...  Training loss: 0.7737...  Val loss: 1.9814...  0.3634 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83501...  Training loss: 0.7815...  Val loss: 1.9722...  0.3624 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83526...  Training loss: 0.7596...  Val loss: 1.9688...  0.3632 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83551...  Training loss: 0.7528...  Val loss: 1.9935...  0.3630 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83576...  Training loss: 0.7576...  Val loss: 1.9931...  0.3633 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83601...  Training loss: 0.7705...  Val loss: 1.9835...  0.3628 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83626...  Training loss: 0.7710...  Val loss: 1.9704...  0.3634 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83651...  Training loss: 0.7687...  Val loss: 1.9810...  0.3635 sec/batch\n",
      "Epoch: 377/500...  Training Step: 83676...  Training loss: 0.7678...  Val loss: 1.9709...  0.3632 sec/batch\n",
      "Epoch: 378/500...  Training Step: 83701...  Training loss: 0.7881...  Val loss: 1.9782...  0.3635 sec/batch\n",
      "Epoch: 378/500...  Training Step: 83726...  Training loss: 0.7797...  Val loss: 1.9866...  0.3635 sec/batch\n",
      "Epoch: 378/500...  Training Step: 83751...  Training loss: 0.7620...  Val loss: 1.9804...  0.3635 sec/batch\n",
      "Epoch: 378/500...  Training Step: 83776...  Training loss: 0.7565...  Val loss: 1.9734...  0.3637 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 378/500...  Training Step: 83801...  Training loss: 0.7517...  Val loss: 1.9825...  0.3640 sec/batch\n",
      "Epoch: 378/500...  Training Step: 83826...  Training loss: 0.7635...  Val loss: 1.9926...  0.3633 sec/batch\n",
      "Epoch: 378/500...  Training Step: 83851...  Training loss: 0.7826...  Val loss: 1.9761...  0.3640 sec/batch\n",
      "Epoch: 378/500...  Training Step: 83876...  Training loss: 0.7786...  Val loss: 1.9760...  0.3632 sec/batch\n",
      "Epoch: 378/500...  Training Step: 83901...  Training loss: 0.7667...  Val loss: 1.9906...  0.3621 sec/batch\n",
      "Epoch: 379/500...  Training Step: 83926...  Training loss: 0.7551...  Val loss: 1.9734...  0.3632 sec/batch\n",
      "Epoch: 379/500...  Training Step: 83951...  Training loss: 0.7687...  Val loss: 1.9831...  0.3637 sec/batch\n",
      "Epoch: 379/500...  Training Step: 83976...  Training loss: 0.7626...  Val loss: 1.9742...  0.3631 sec/batch\n",
      "Epoch: 379/500...  Training Step: 84001...  Training loss: 0.7792...  Val loss: 1.9707...  0.3637 sec/batch\n",
      "Epoch: 379/500...  Training Step: 84026...  Training loss: 0.7693...  Val loss: 1.9674...  0.3633 sec/batch\n",
      "Epoch: 379/500...  Training Step: 84051...  Training loss: 0.7731...  Val loss: 1.9986...  0.3638 sec/batch\n",
      "Epoch: 379/500...  Training Step: 84076...  Training loss: 0.7750...  Val loss: 1.9647...  0.3630 sec/batch\n",
      "Epoch: 379/500...  Training Step: 84101...  Training loss: 0.7609...  Val loss: 1.9875...  0.3631 sec/batch\n",
      "Epoch: 379/500...  Training Step: 84126...  Training loss: 0.7613...  Val loss: 1.9769...  0.3632 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84151...  Training loss: 0.7806...  Val loss: 1.9712...  0.3635 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84176...  Training loss: 0.7560...  Val loss: 1.9820...  0.3633 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84201...  Training loss: 0.7690...  Val loss: 1.9890...  0.3636 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84226...  Training loss: 0.7678...  Val loss: 1.9770...  0.3633 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84251...  Training loss: 0.7591...  Val loss: 1.9873...  0.3634 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84276...  Training loss: 0.7664...  Val loss: 1.9825...  0.3638 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84301...  Training loss: 0.7627...  Val loss: 1.9662...  0.3637 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84326...  Training loss: 0.7609...  Val loss: 1.9733...  0.3632 sec/batch\n",
      "Epoch: 380/500...  Training Step: 84351...  Training loss: 0.7845...  Val loss: 1.9859...  0.3643 sec/batch\n",
      "Epoch 380/500 time:81.6049165725708...  finished at 2017-10-30 18:16:18\n",
      "Epoch: 381/500...  Training Step: 84376...  Training loss: 0.7451...  Val loss: 1.9524...  0.3632 sec/batch\n",
      "Epoch: 381/500...  Training Step: 84401...  Training loss: 0.7581...  Val loss: 1.9740...  0.3633 sec/batch\n",
      "Epoch: 381/500...  Training Step: 84426...  Training loss: 0.7713...  Val loss: 1.9827...  0.3631 sec/batch\n",
      "Epoch: 381/500...  Training Step: 84451...  Training loss: 0.7558...  Val loss: 1.9699...  0.3633 sec/batch\n",
      "Epoch: 381/500...  Training Step: 84476...  Training loss: 0.7562...  Val loss: 1.9653...  0.3629 sec/batch\n",
      "Epoch: 381/500...  Training Step: 84501...  Training loss: 0.7601...  Val loss: 1.9791...  0.3633 sec/batch\n",
      "Epoch: 381/500...  Training Step: 84526...  Training loss: 0.7663...  Val loss: 1.9714...  0.3631 sec/batch\n",
      "Epoch: 381/500...  Training Step: 84551...  Training loss: 0.7697...  Val loss: 1.9642...  0.3628 sec/batch\n",
      "Epoch: 381/500...  Training Step: 84576...  Training loss: 0.7744...  Val loss: 1.9710...  0.3636 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84601...  Training loss: 0.7786...  Val loss: 1.9810...  0.3633 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84626...  Training loss: 0.7782...  Val loss: 1.9666...  0.3633 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84651...  Training loss: 0.7683...  Val loss: 2.0092...  0.3631 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84676...  Training loss: 0.7569...  Val loss: 1.9741...  0.3638 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84701...  Training loss: 0.7770...  Val loss: 2.0076...  0.3639 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84726...  Training loss: 0.7751...  Val loss: 1.9865...  0.3631 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84751...  Training loss: 0.7551...  Val loss: 1.9888...  0.3633 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84776...  Training loss: 0.7625...  Val loss: 1.9930...  0.3637 sec/batch\n",
      "Epoch: 382/500...  Training Step: 84801...  Training loss: 0.7908...  Val loss: 1.9951...  0.3632 sec/batch\n",
      "Epoch: 383/500...  Training Step: 84826...  Training loss: 0.7586...  Val loss: 1.9616...  0.3634 sec/batch\n",
      "Epoch: 383/500...  Training Step: 84851...  Training loss: 0.7717...  Val loss: 1.9859...  0.3633 sec/batch\n",
      "Epoch: 383/500...  Training Step: 84876...  Training loss: 0.7572...  Val loss: 2.0100...  0.3630 sec/batch\n",
      "Epoch: 383/500...  Training Step: 84901...  Training loss: 0.7582...  Val loss: 1.9788...  0.3631 sec/batch\n",
      "Epoch: 383/500...  Training Step: 84926...  Training loss: 0.7715...  Val loss: 1.9686...  0.3631 sec/batch\n",
      "Epoch: 383/500...  Training Step: 84951...  Training loss: 0.7719...  Val loss: 1.9515...  0.3640 sec/batch\n",
      "Epoch: 383/500...  Training Step: 84976...  Training loss: 0.7714...  Val loss: 1.9706...  0.3639 sec/batch\n",
      "Epoch: 383/500...  Training Step: 85001...  Training loss: 0.7693...  Val loss: 1.9908...  0.3629 sec/batch\n",
      "Epoch: 383/500...  Training Step: 85026...  Training loss: 0.7639...  Val loss: 1.9882...  0.3628 sec/batch\n",
      "Epoch: 384/500...  Training Step: 85051...  Training loss: 0.7533...  Val loss: 1.9635...  0.3631 sec/batch\n",
      "Epoch: 384/500...  Training Step: 85076...  Training loss: 0.7688...  Val loss: 1.9913...  0.3631 sec/batch\n",
      "Epoch: 384/500...  Training Step: 85101...  Training loss: 0.7827...  Val loss: 1.9825...  0.3634 sec/batch\n",
      "Epoch: 384/500...  Training Step: 85126...  Training loss: 0.7640...  Val loss: 1.9838...  0.3632 sec/batch\n",
      "Epoch: 384/500...  Training Step: 85151...  Training loss: 0.7665...  Val loss: 1.9955...  0.3633 sec/batch\n",
      "Epoch: 384/500...  Training Step: 85176...  Training loss: 0.7737...  Val loss: 1.9737...  0.3635 sec/batch\n",
      "Epoch: 384/500...  Training Step: 85201...  Training loss: 0.7616...  Val loss: 1.9771...  0.3642 sec/batch\n",
      "Epoch: 384/500...  Training Step: 85226...  Training loss: 0.7644...  Val loss: 1.9991...  0.3633 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85251...  Training loss: 0.7654...  Val loss: 1.9685...  0.3633 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85276...  Training loss: 0.7629...  Val loss: 1.9680...  0.3637 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85301...  Training loss: 0.7411...  Val loss: 1.9802...  0.3637 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85326...  Training loss: 0.7498...  Val loss: 1.9950...  0.3634 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85351...  Training loss: 0.7548...  Val loss: 1.9840...  0.3633 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85376...  Training loss: 0.7577...  Val loss: 1.9931...  0.3632 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85401...  Training loss: 0.7592...  Val loss: 1.9673...  0.3631 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85426...  Training loss: 0.7663...  Val loss: 2.0132...  0.3635 sec/batch\n",
      "Epoch: 385/500...  Training Step: 85451...  Training loss: 0.7755...  Val loss: 1.9929...  0.3632 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85476...  Training loss: 0.7667...  Val loss: 1.9781...  0.3633 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85501...  Training loss: 0.7453...  Val loss: 1.9704...  0.3632 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85526...  Training loss: 0.7748...  Val loss: 1.9882...  0.3632 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85551...  Training loss: 0.7536...  Val loss: 1.9858...  0.3629 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85576...  Training loss: 0.7470...  Val loss: 1.9974...  0.3631 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85601...  Training loss: 0.7473...  Val loss: 1.9916...  0.3634 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85626...  Training loss: 0.7655...  Val loss: 1.9832...  0.3632 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85651...  Training loss: 0.7633...  Val loss: 1.9769...  0.3634 sec/batch\n",
      "Epoch: 386/500...  Training Step: 85676...  Training loss: 0.7638...  Val loss: 1.9950...  0.3634 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 387/500...  Training Step: 85701...  Training loss: 0.7629...  Val loss: 2.0085...  0.3635 sec/batch\n",
      "Epoch: 387/500...  Training Step: 85726...  Training loss: 0.7524...  Val loss: 1.9992...  0.3634 sec/batch\n",
      "Epoch: 387/500...  Training Step: 85751...  Training loss: 0.7736...  Val loss: 2.0121...  0.3637 sec/batch\n",
      "Epoch: 387/500...  Training Step: 85776...  Training loss: 0.7746...  Val loss: 1.9854...  0.3635 sec/batch\n",
      "Epoch: 387/500...  Training Step: 85801...  Training loss: 0.7589...  Val loss: 1.9776...  0.3633 sec/batch\n",
      "Epoch: 387/500...  Training Step: 85826...  Training loss: 0.7478...  Val loss: 2.0124...  0.3637 sec/batch\n",
      "Epoch: 387/500...  Training Step: 85851...  Training loss: 0.7662...  Val loss: 2.0072...  0.3634 sec/batch\n",
      "Epoch: 387/500...  Training Step: 85876...  Training loss: 0.7789...  Val loss: 1.9863...  0.3632 sec/batch\n",
      "Epoch: 387/500...  Training Step: 85901...  Training loss: 0.7610...  Val loss: 1.9953...  0.3630 sec/batch\n",
      "Epoch: 388/500...  Training Step: 85926...  Training loss: 0.7601...  Val loss: 1.9945...  0.3629 sec/batch\n",
      "Epoch: 388/500...  Training Step: 85951...  Training loss: 0.7526...  Val loss: 1.9823...  0.3631 sec/batch\n",
      "Epoch: 388/500...  Training Step: 85976...  Training loss: 0.7504...  Val loss: 1.9674...  0.3627 sec/batch\n",
      "Epoch: 388/500...  Training Step: 86001...  Training loss: 0.7757...  Val loss: 1.9802...  0.3633 sec/batch\n",
      "Epoch: 388/500...  Training Step: 86026...  Training loss: 0.7631...  Val loss: 1.9785...  0.3631 sec/batch\n",
      "Epoch: 388/500...  Training Step: 86051...  Training loss: 0.7426...  Val loss: 2.0234...  0.3635 sec/batch\n",
      "Epoch: 388/500...  Training Step: 86076...  Training loss: 0.7727...  Val loss: 1.9899...  0.3630 sec/batch\n",
      "Epoch: 388/500...  Training Step: 86101...  Training loss: 0.7584...  Val loss: 1.9916...  0.3633 sec/batch\n",
      "Epoch: 388/500...  Training Step: 86126...  Training loss: 0.7550...  Val loss: 1.9945...  0.3638 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86151...  Training loss: 0.7854...  Val loss: 2.0000...  0.3636 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86176...  Training loss: 0.7396...  Val loss: 1.9933...  0.3635 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86201...  Training loss: 0.7542...  Val loss: 2.0245...  0.3633 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86226...  Training loss: 0.7449...  Val loss: 2.0102...  0.3633 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86251...  Training loss: 0.7619...  Val loss: 1.9915...  0.3630 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86276...  Training loss: 0.7470...  Val loss: 1.9920...  0.3638 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86301...  Training loss: 0.7497...  Val loss: 1.9821...  0.3630 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86326...  Training loss: 0.7711...  Val loss: 2.0146...  0.3634 sec/batch\n",
      "Epoch: 389/500...  Training Step: 86351...  Training loss: 0.7819...  Val loss: 2.0123...  0.3635 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86376...  Training loss: 0.7570...  Val loss: 1.9687...  0.3639 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86401...  Training loss: 0.7557...  Val loss: 1.9868...  0.3632 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86426...  Training loss: 0.7687...  Val loss: 2.0078...  0.3633 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86451...  Training loss: 0.7488...  Val loss: 1.9729...  0.3635 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86476...  Training loss: 0.7652...  Val loss: 1.9906...  0.3633 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86501...  Training loss: 0.7604...  Val loss: 1.9987...  0.3632 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86526...  Training loss: 0.7645...  Val loss: 1.9886...  0.3632 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86551...  Training loss: 0.7503...  Val loss: 1.9982...  0.3632 sec/batch\n",
      "Epoch: 390/500...  Training Step: 86576...  Training loss: 0.7720...  Val loss: 1.9941...  0.3624 sec/batch\n",
      "Epoch 390/500 time:81.91893219947815...  finished at 2017-10-30 18:29:55\n",
      "Epoch: 391/500...  Training Step: 86601...  Training loss: 0.7643...  Val loss: 2.0005...  0.3634 sec/batch\n",
      "Epoch: 391/500...  Training Step: 86626...  Training loss: 0.7522...  Val loss: 1.9863...  0.3635 sec/batch\n",
      "Epoch: 391/500...  Training Step: 86651...  Training loss: 0.7651...  Val loss: 2.0129...  0.3667 sec/batch\n",
      "Epoch: 391/500...  Training Step: 86676...  Training loss: 0.7632...  Val loss: 1.9982...  0.3633 sec/batch\n",
      "Epoch: 391/500...  Training Step: 86701...  Training loss: 0.7657...  Val loss: 1.9770...  0.3631 sec/batch\n",
      "Epoch: 391/500...  Training Step: 86726...  Training loss: 0.7625...  Val loss: 2.0188...  0.3630 sec/batch\n",
      "Epoch: 391/500...  Training Step: 86751...  Training loss: 0.7811...  Val loss: 2.0035...  0.3635 sec/batch\n",
      "Epoch: 391/500...  Training Step: 86776...  Training loss: 0.7692...  Val loss: 1.9694...  0.3631 sec/batch\n",
      "Epoch: 391/500...  Training Step: 86801...  Training loss: 0.7676...  Val loss: 2.0111...  0.3634 sec/batch\n",
      "Epoch: 392/500...  Training Step: 86826...  Training loss: 0.7598...  Val loss: 2.0026...  0.3632 sec/batch\n",
      "Epoch: 392/500...  Training Step: 86851...  Training loss: 0.7577...  Val loss: 2.0030...  0.3632 sec/batch\n",
      "Epoch: 392/500...  Training Step: 86876...  Training loss: 0.7617...  Val loss: 2.0163...  0.3632 sec/batch\n",
      "Epoch: 392/500...  Training Step: 86901...  Training loss: 0.7537...  Val loss: 1.9915...  0.3633 sec/batch\n",
      "Epoch: 392/500...  Training Step: 86926...  Training loss: 0.7497...  Val loss: 2.0010...  0.3637 sec/batch\n",
      "Epoch: 392/500...  Training Step: 86951...  Training loss: 0.7584...  Val loss: 1.9835...  0.3630 sec/batch\n",
      "Epoch: 392/500...  Training Step: 86976...  Training loss: 0.7619...  Val loss: 1.9927...  0.3633 sec/batch\n",
      "Epoch: 392/500...  Training Step: 87001...  Training loss: 0.7682...  Val loss: 1.9976...  0.3638 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87026...  Training loss: 0.7508...  Val loss: 1.9937...  0.3631 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87051...  Training loss: 0.7585...  Val loss: 1.9876...  0.3632 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87076...  Training loss: 0.7562...  Val loss: 1.9836...  0.3637 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87101...  Training loss: 0.7566...  Val loss: 2.0188...  0.3632 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87126...  Training loss: 0.7636...  Val loss: 1.9980...  0.3634 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87151...  Training loss: 0.7616...  Val loss: 2.0133...  0.3636 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87176...  Training loss: 0.7551...  Val loss: 1.9867...  0.3634 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87201...  Training loss: 0.7549...  Val loss: 2.0138...  0.3617 sec/batch\n",
      "Epoch: 393/500...  Training Step: 87226...  Training loss: 0.7411...  Val loss: 1.9936...  0.3634 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87251...  Training loss: 0.7657...  Val loss: 1.9997...  0.3629 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87276...  Training loss: 0.7370...  Val loss: 1.9944...  0.3635 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87301...  Training loss: 0.7322...  Val loss: 2.0036...  0.3636 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87326...  Training loss: 0.7617...  Val loss: 2.0085...  0.3630 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87351...  Training loss: 0.7614...  Val loss: 1.9819...  0.3637 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87376...  Training loss: 0.7484...  Val loss: 2.0132...  0.3629 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87401...  Training loss: 0.7599...  Val loss: 2.0101...  0.3633 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87426...  Training loss: 0.7582...  Val loss: 1.9904...  0.3633 sec/batch\n",
      "Epoch: 394/500...  Training Step: 87451...  Training loss: 0.7520...  Val loss: 1.9920...  0.3635 sec/batch\n",
      "Epoch: 395/500...  Training Step: 87476...  Training loss: 0.7605...  Val loss: 1.9765...  0.3630 sec/batch\n",
      "Epoch: 395/500...  Training Step: 87501...  Training loss: 0.7573...  Val loss: 1.9909...  0.3633 sec/batch\n",
      "Epoch: 395/500...  Training Step: 87526...  Training loss: 0.7620...  Val loss: 2.0098...  0.3635 sec/batch\n",
      "Epoch: 395/500...  Training Step: 87551...  Training loss: 0.7503...  Val loss: 1.9973...  0.3633 sec/batch\n",
      "Epoch: 395/500...  Training Step: 87576...  Training loss: 0.7488...  Val loss: 2.0010...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 395/500...  Training Step: 87601...  Training loss: 0.7491...  Val loss: 2.0199...  0.3632 sec/batch\n",
      "Epoch: 395/500...  Training Step: 87626...  Training loss: 0.7636...  Val loss: 1.9921...  0.3631 sec/batch\n",
      "Epoch: 395/500...  Training Step: 87651...  Training loss: 0.7586...  Val loss: 1.9816...  0.3635 sec/batch\n",
      "Epoch: 395/500...  Training Step: 87676...  Training loss: 0.7558...  Val loss: 2.0036...  0.3636 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87701...  Training loss: 0.7519...  Val loss: 2.0092...  0.3628 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87726...  Training loss: 0.7437...  Val loss: 1.9962...  0.3638 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87751...  Training loss: 0.7516...  Val loss: 1.9999...  0.3627 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87776...  Training loss: 0.7618...  Val loss: 1.9920...  0.3636 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87801...  Training loss: 0.7797...  Val loss: 2.0021...  0.3631 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87826...  Training loss: 0.7476...  Val loss: 1.9984...  0.3632 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87851...  Training loss: 0.7754...  Val loss: 1.9874...  0.3635 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87876...  Training loss: 0.7701...  Val loss: 2.0039...  0.3632 sec/batch\n",
      "Epoch: 396/500...  Training Step: 87901...  Training loss: 0.7607...  Val loss: 2.0140...  0.3632 sec/batch\n",
      "Epoch: 397/500...  Training Step: 87926...  Training loss: 0.7481...  Val loss: 2.0065...  0.3630 sec/batch\n",
      "Epoch: 397/500...  Training Step: 87951...  Training loss: 0.7397...  Val loss: 2.0077...  0.3637 sec/batch\n",
      "Epoch: 397/500...  Training Step: 87976...  Training loss: 0.7555...  Val loss: 1.9964...  0.3634 sec/batch\n",
      "Epoch: 397/500...  Training Step: 88001...  Training loss: 0.7591...  Val loss: 2.0122...  0.3637 sec/batch\n",
      "Epoch: 397/500...  Training Step: 88026...  Training loss: 0.7607...  Val loss: 2.0118...  0.3635 sec/batch\n",
      "Epoch: 397/500...  Training Step: 88051...  Training loss: 0.7513...  Val loss: 2.0146...  0.3626 sec/batch\n",
      "Epoch: 397/500...  Training Step: 88076...  Training loss: 0.7602...  Val loss: 2.0109...  0.3635 sec/batch\n",
      "Epoch: 397/500...  Training Step: 88101...  Training loss: 0.7635...  Val loss: 1.9857...  0.3635 sec/batch\n",
      "Epoch: 397/500...  Training Step: 88126...  Training loss: 0.7424...  Val loss: 2.0131...  0.3630 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88151...  Training loss: 0.7559...  Val loss: 1.9890...  0.3632 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88176...  Training loss: 0.7554...  Val loss: 1.9801...  0.3633 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88201...  Training loss: 0.7390...  Val loss: 2.0200...  0.3635 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88226...  Training loss: 0.7657...  Val loss: 1.9847...  0.3638 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88251...  Training loss: 0.7597...  Val loss: 1.9988...  0.3630 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88276...  Training loss: 0.7561...  Val loss: 2.0028...  0.3630 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88301...  Training loss: 0.7428...  Val loss: 2.0056...  0.3634 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88326...  Training loss: 0.7633...  Val loss: 1.9794...  0.3633 sec/batch\n",
      "Epoch: 398/500...  Training Step: 88351...  Training loss: 0.7517...  Val loss: 2.0162...  0.3637 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88376...  Training loss: 0.7557...  Val loss: 1.9667...  0.3634 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88401...  Training loss: 0.7420...  Val loss: 1.9969...  0.3636 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88426...  Training loss: 0.7491...  Val loss: 2.0038...  0.3633 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88451...  Training loss: 0.7534...  Val loss: 2.0050...  0.3634 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88476...  Training loss: 0.7403...  Val loss: 1.9910...  0.3633 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88501...  Training loss: 0.7563...  Val loss: 1.9937...  0.3631 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88526...  Training loss: 0.7388...  Val loss: 2.0077...  0.3629 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88551...  Training loss: 0.7507...  Val loss: 1.9949...  0.3630 sec/batch\n",
      "Epoch: 399/500...  Training Step: 88576...  Training loss: 0.7438...  Val loss: 2.0139...  0.3632 sec/batch\n",
      "Epoch: 400/500...  Training Step: 88601...  Training loss: 0.7584...  Val loss: 1.9947...  0.3630 sec/batch\n",
      "Epoch: 400/500...  Training Step: 88626...  Training loss: 0.7484...  Val loss: 2.0078...  0.3631 sec/batch\n",
      "Epoch: 400/500...  Training Step: 88651...  Training loss: 0.7482...  Val loss: 1.9860...  0.3635 sec/batch\n",
      "Epoch: 400/500...  Training Step: 88676...  Training loss: 0.7492...  Val loss: 1.9913...  0.3634 sec/batch\n",
      "Epoch: 400/500...  Training Step: 88701...  Training loss: 0.7483...  Val loss: 2.0191...  0.3638 sec/batch\n",
      "Epoch: 400/500...  Training Step: 88726...  Training loss: 0.7619...  Val loss: 1.9978...  0.3631 sec/batch\n",
      "Epoch: 400/500...  Training Step: 88751...  Training loss: 0.7739...  Val loss: 2.0135...  0.3626 sec/batch\n",
      "Epoch: 400/500...  Training Step: 88776...  Training loss: 0.7641...  Val loss: 1.9983...  0.3636 sec/batch\n",
      "Epoch 400/500 time:81.47621083259583...  finished at 2017-10-30 18:43:32\n",
      "Epoch: 401/500...  Training Step: 88801...  Training loss: 0.8396...  Val loss: 2.0106...  0.3634 sec/batch\n",
      "Epoch: 401/500...  Training Step: 88826...  Training loss: 0.7623...  Val loss: 2.0071...  0.3637 sec/batch\n",
      "Epoch: 401/500...  Training Step: 88851...  Training loss: 0.7355...  Val loss: 2.0172...  0.3631 sec/batch\n",
      "Epoch: 401/500...  Training Step: 88876...  Training loss: 0.7494...  Val loss: 2.0153...  0.3634 sec/batch\n",
      "Epoch: 401/500...  Training Step: 88901...  Training loss: 0.7578...  Val loss: 2.0070...  0.3633 sec/batch\n",
      "Epoch: 401/500...  Training Step: 88926...  Training loss: 0.7435...  Val loss: 2.0064...  0.3636 sec/batch\n",
      "Epoch: 401/500...  Training Step: 88951...  Training loss: 0.7507...  Val loss: 2.0081...  0.3634 sec/batch\n",
      "Epoch: 401/500...  Training Step: 88976...  Training loss: 0.7423...  Val loss: 2.0144...  0.3632 sec/batch\n",
      "Epoch: 401/500...  Training Step: 89001...  Training loss: 0.7624...  Val loss: 2.0136...  0.3633 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89026...  Training loss: 0.7639...  Val loss: 2.0102...  0.3635 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89051...  Training loss: 0.7521...  Val loss: 2.0063...  0.3629 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89076...  Training loss: 0.7499...  Val loss: 2.0139...  0.3632 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89101...  Training loss: 0.7349...  Val loss: 2.0305...  0.3635 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89126...  Training loss: 0.7375...  Val loss: 2.0301...  0.3632 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89151...  Training loss: 0.7397...  Val loss: 2.0070...  0.3633 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89176...  Training loss: 0.7488...  Val loss: 2.0009...  0.3629 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89201...  Training loss: 0.7568...  Val loss: 2.0244...  0.3633 sec/batch\n",
      "Epoch: 402/500...  Training Step: 89226...  Training loss: 0.7517...  Val loss: 2.0014...  0.3622 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89251...  Training loss: 0.7734...  Val loss: 2.0138...  0.3632 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89276...  Training loss: 0.7643...  Val loss: 2.0220...  0.3635 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89301...  Training loss: 0.7356...  Val loss: 2.0098...  0.3626 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89326...  Training loss: 0.7430...  Val loss: 2.0039...  0.3632 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89351...  Training loss: 0.7356...  Val loss: 2.0143...  0.3633 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89376...  Training loss: 0.7575...  Val loss: 2.0122...  0.3631 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89401...  Training loss: 0.7532...  Val loss: 2.0000...  0.3631 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89426...  Training loss: 0.7624...  Val loss: 2.0164...  0.3631 sec/batch\n",
      "Epoch: 403/500...  Training Step: 89451...  Training loss: 0.7459...  Val loss: 2.0167...  0.3637 sec/batch\n",
      "Epoch: 404/500...  Training Step: 89476...  Training loss: 0.7375...  Val loss: 2.0045...  0.3637 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 404/500...  Training Step: 89501...  Training loss: 0.7472...  Val loss: 2.0167...  0.3632 sec/batch\n",
      "Epoch: 404/500...  Training Step: 89526...  Training loss: 0.7523...  Val loss: 1.9968...  0.3637 sec/batch\n",
      "Epoch: 404/500...  Training Step: 89551...  Training loss: 0.7585...  Val loss: 2.0006...  0.3634 sec/batch\n",
      "Epoch: 404/500...  Training Step: 89576...  Training loss: 0.7554...  Val loss: 2.0024...  0.3629 sec/batch\n",
      "Epoch: 404/500...  Training Step: 89601...  Training loss: 0.7529...  Val loss: 2.0244...  0.3636 sec/batch\n",
      "Epoch: 404/500...  Training Step: 89626...  Training loss: 0.7613...  Val loss: 1.9912...  0.3630 sec/batch\n",
      "Epoch: 404/500...  Training Step: 89651...  Training loss: 0.7389...  Val loss: 2.0190...  0.3634 sec/batch\n",
      "Epoch: 404/500...  Training Step: 89676...  Training loss: 0.7458...  Val loss: 1.9981...  0.3633 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89701...  Training loss: 0.7540...  Val loss: 2.0033...  0.3630 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89726...  Training loss: 0.7497...  Val loss: 2.0238...  0.3635 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89751...  Training loss: 0.7516...  Val loss: 2.0162...  0.3629 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89776...  Training loss: 0.7547...  Val loss: 2.0208...  0.3632 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89801...  Training loss: 0.7324...  Val loss: 2.0185...  0.3634 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89826...  Training loss: 0.7498...  Val loss: 2.0258...  0.3634 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89851...  Training loss: 0.7582...  Val loss: 1.9979...  0.3632 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89876...  Training loss: 0.7535...  Val loss: 1.9981...  0.3635 sec/batch\n",
      "Epoch: 405/500...  Training Step: 89901...  Training loss: 0.7513...  Val loss: 2.0183...  0.3633 sec/batch\n",
      "Epoch: 406/500...  Training Step: 89926...  Training loss: 0.7348...  Val loss: 1.9862...  0.3635 sec/batch\n",
      "Epoch: 406/500...  Training Step: 89951...  Training loss: 0.7367...  Val loss: 2.0014...  0.3635 sec/batch\n",
      "Epoch: 406/500...  Training Step: 89976...  Training loss: 0.7530...  Val loss: 2.0083...  0.3629 sec/batch\n",
      "Epoch: 406/500...  Training Step: 90001...  Training loss: 0.7419...  Val loss: 1.9889...  0.3632 sec/batch\n",
      "Epoch: 406/500...  Training Step: 90026...  Training loss: 0.7463...  Val loss: 2.0095...  0.3631 sec/batch\n",
      "Epoch: 406/500...  Training Step: 90051...  Training loss: 0.7407...  Val loss: 2.0191...  0.3627 sec/batch\n",
      "Epoch: 406/500...  Training Step: 90076...  Training loss: 0.7534...  Val loss: 2.0108...  0.3632 sec/batch\n",
      "Epoch: 406/500...  Training Step: 90101...  Training loss: 0.7497...  Val loss: 2.0017...  0.3633 sec/batch\n",
      "Epoch: 406/500...  Training Step: 90126...  Training loss: 0.7560...  Val loss: 2.0019...  0.3624 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90151...  Training loss: 0.7598...  Val loss: 2.0044...  0.3630 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90176...  Training loss: 0.7449...  Val loss: 1.9990...  0.3633 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90201...  Training loss: 0.7562...  Val loss: 2.0299...  0.3632 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90226...  Training loss: 0.7483...  Val loss: 2.0024...  0.3628 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90251...  Training loss: 0.7495...  Val loss: 2.0225...  0.3635 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90276...  Training loss: 0.7658...  Val loss: 2.0256...  0.3630 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90301...  Training loss: 0.7425...  Val loss: 2.0208...  0.3631 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90326...  Training loss: 0.7497...  Val loss: 2.0267...  0.3635 sec/batch\n",
      "Epoch: 407/500...  Training Step: 90351...  Training loss: 0.7697...  Val loss: 2.0350...  0.3635 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90376...  Training loss: 0.7350...  Val loss: 2.0030...  0.3631 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90401...  Training loss: 0.7440...  Val loss: 2.0199...  0.3636 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90426...  Training loss: 0.7412...  Val loss: 2.0448...  0.3635 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90451...  Training loss: 0.7236...  Val loss: 2.0004...  0.3638 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90476...  Training loss: 0.7449...  Val loss: 1.9986...  0.3634 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90501...  Training loss: 0.7618...  Val loss: 1.9787...  0.3636 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90526...  Training loss: 0.7449...  Val loss: 2.0139...  0.3725 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90551...  Training loss: 0.7605...  Val loss: 2.0225...  0.3684 sec/batch\n",
      "Epoch: 408/500...  Training Step: 90576...  Training loss: 0.7526...  Val loss: 2.0173...  0.3636 sec/batch\n",
      "Epoch: 409/500...  Training Step: 90601...  Training loss: 0.7364...  Val loss: 1.9993...  0.3634 sec/batch\n",
      "Epoch: 409/500...  Training Step: 90626...  Training loss: 0.7516...  Val loss: 2.0170...  0.3633 sec/batch\n",
      "Epoch: 409/500...  Training Step: 90651...  Training loss: 0.7678...  Val loss: 2.0056...  0.3632 sec/batch\n",
      "Epoch: 409/500...  Training Step: 90676...  Training loss: 0.7389...  Val loss: 2.0100...  0.3637 sec/batch\n",
      "Epoch: 409/500...  Training Step: 90701...  Training loss: 0.7360...  Val loss: 2.0307...  0.3631 sec/batch\n",
      "Epoch: 409/500...  Training Step: 90726...  Training loss: 0.7589...  Val loss: 2.0113...  0.3629 sec/batch\n",
      "Epoch: 409/500...  Training Step: 90751...  Training loss: 0.7437...  Val loss: 2.0136...  0.3636 sec/batch\n",
      "Epoch: 409/500...  Training Step: 90776...  Training loss: 0.7560...  Val loss: 2.0392...  0.3631 sec/batch\n",
      "Epoch: 410/500...  Training Step: 90801...  Training loss: 0.7624...  Val loss: 2.0120...  0.3636 sec/batch\n",
      "Epoch: 410/500...  Training Step: 90826...  Training loss: 0.7381...  Val loss: 2.0051...  0.3631 sec/batch\n",
      "Epoch: 410/500...  Training Step: 90851...  Training loss: 0.7219...  Val loss: 2.0150...  0.3637 sec/batch\n",
      "Epoch: 410/500...  Training Step: 90876...  Training loss: 0.7277...  Val loss: 2.0423...  0.3632 sec/batch\n",
      "Epoch: 410/500...  Training Step: 90901...  Training loss: 0.7308...  Val loss: 2.0164...  0.3632 sec/batch\n",
      "Epoch: 410/500...  Training Step: 90926...  Training loss: 0.7437...  Val loss: 2.0357...  0.3637 sec/batch\n",
      "Epoch: 410/500...  Training Step: 90951...  Training loss: 0.7344...  Val loss: 1.9944...  0.3636 sec/batch\n",
      "Epoch: 410/500...  Training Step: 90976...  Training loss: 0.7348...  Val loss: 2.0413...  0.3630 sec/batch\n",
      "Epoch: 410/500...  Training Step: 91001...  Training loss: 0.7487...  Val loss: 2.0195...  0.3632 sec/batch\n",
      "Epoch 410/500 time:81.98620533943176...  finished at 2017-10-30 18:57:10\n",
      "Epoch: 411/500...  Training Step: 91026...  Training loss: 0.7545...  Val loss: 2.0002...  0.3638 sec/batch\n",
      "Epoch: 411/500...  Training Step: 91051...  Training loss: 0.7216...  Val loss: 1.9972...  0.3637 sec/batch\n",
      "Epoch: 411/500...  Training Step: 91076...  Training loss: 0.7453...  Val loss: 2.0099...  0.3630 sec/batch\n",
      "Epoch: 411/500...  Training Step: 91101...  Training loss: 0.7279...  Val loss: 2.0239...  0.3629 sec/batch\n",
      "Epoch: 411/500...  Training Step: 91126...  Training loss: 0.7386...  Val loss: 2.0321...  0.3634 sec/batch\n",
      "Epoch: 411/500...  Training Step: 91151...  Training loss: 0.7418...  Val loss: 2.0208...  0.3635 sec/batch\n",
      "Epoch: 411/500...  Training Step: 91176...  Training loss: 0.7410...  Val loss: 2.0092...  0.3630 sec/batch\n",
      "Epoch: 411/500...  Training Step: 91201...  Training loss: 0.7582...  Val loss: 2.0228...  0.3634 sec/batch\n",
      "Epoch: 411/500...  Training Step: 91226...  Training loss: 0.7485...  Val loss: 2.0269...  0.3634 sec/batch\n",
      "Epoch: 412/500...  Training Step: 91251...  Training loss: 0.7361...  Val loss: 2.0394...  0.3633 sec/batch\n",
      "Epoch: 412/500...  Training Step: 91276...  Training loss: 0.7347...  Val loss: 2.0376...  0.3632 sec/batch\n",
      "Epoch: 412/500...  Training Step: 91301...  Training loss: 0.7476...  Val loss: 2.0324...  0.3632 sec/batch\n",
      "Epoch: 412/500...  Training Step: 91326...  Training loss: 0.7526...  Val loss: 2.0114...  0.3629 sec/batch\n",
      "Epoch: 412/500...  Training Step: 91351...  Training loss: 0.7365...  Val loss: 2.0153...  0.3635 sec/batch\n",
      "Epoch: 412/500...  Training Step: 91376...  Training loss: 0.7132...  Val loss: 2.0258...  0.3635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 412/500...  Training Step: 91401...  Training loss: 0.7506...  Val loss: 2.0399...  0.3639 sec/batch\n",
      "Epoch: 412/500...  Training Step: 91426...  Training loss: 0.7672...  Val loss: 2.0199...  0.3636 sec/batch\n",
      "Epoch: 412/500...  Training Step: 91451...  Training loss: 0.7318...  Val loss: 2.0301...  0.3635 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91476...  Training loss: 0.7420...  Val loss: 2.0354...  0.3634 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91501...  Training loss: 0.7353...  Val loss: 2.0091...  0.3634 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91526...  Training loss: 0.7392...  Val loss: 2.0143...  0.3636 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91551...  Training loss: 0.7395...  Val loss: 2.0077...  0.3633 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91576...  Training loss: 0.7442...  Val loss: 2.0193...  0.3632 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91601...  Training loss: 0.7399...  Val loss: 2.0437...  0.3635 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91626...  Training loss: 0.7465...  Val loss: 2.0234...  0.3634 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91651...  Training loss: 0.7574...  Val loss: 2.0101...  0.3630 sec/batch\n",
      "Epoch: 413/500...  Training Step: 91676...  Training loss: 0.7436...  Val loss: 2.0198...  0.3632 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91701...  Training loss: 0.7594...  Val loss: 2.0102...  0.3634 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91726...  Training loss: 0.7357...  Val loss: 2.0101...  0.3629 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91751...  Training loss: 0.7513...  Val loss: 2.0497...  0.3631 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91776...  Training loss: 0.7135...  Val loss: 2.0341...  0.3635 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91801...  Training loss: 0.7541...  Val loss: 2.0227...  0.3634 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91826...  Training loss: 0.7448...  Val loss: 2.0304...  0.3629 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91851...  Training loss: 0.7312...  Val loss: 2.0067...  0.3636 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91876...  Training loss: 0.7628...  Val loss: 2.0376...  0.3636 sec/batch\n",
      "Epoch: 414/500...  Training Step: 91901...  Training loss: 0.7562...  Val loss: 2.0521...  0.3627 sec/batch\n",
      "Epoch: 415/500...  Training Step: 91926...  Training loss: 0.7318...  Val loss: 2.0015...  0.3630 sec/batch\n",
      "Epoch: 415/500...  Training Step: 91951...  Training loss: 0.7284...  Val loss: 2.0287...  0.3629 sec/batch\n",
      "Epoch: 415/500...  Training Step: 91976...  Training loss: 0.7443...  Val loss: 2.0416...  0.3637 sec/batch\n",
      "Epoch: 415/500...  Training Step: 92001...  Training loss: 0.7293...  Val loss: 2.0013...  0.3631 sec/batch\n",
      "Epoch: 415/500...  Training Step: 92026...  Training loss: 0.7389...  Val loss: 2.0267...  0.3635 sec/batch\n",
      "Epoch: 415/500...  Training Step: 92051...  Training loss: 0.7487...  Val loss: 2.0314...  0.3631 sec/batch\n",
      "Epoch: 415/500...  Training Step: 92076...  Training loss: 0.7488...  Val loss: 2.0284...  0.3637 sec/batch\n",
      "Epoch: 415/500...  Training Step: 92101...  Training loss: 0.7352...  Val loss: 2.0292...  0.3630 sec/batch\n",
      "Epoch: 415/500...  Training Step: 92126...  Training loss: 0.7506...  Val loss: 2.0129...  0.3632 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92151...  Training loss: 0.7485...  Val loss: 2.0070...  0.3639 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92176...  Training loss: 0.7287...  Val loss: 2.0161...  0.3634 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92201...  Training loss: 0.7534...  Val loss: 2.0376...  0.3638 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92226...  Training loss: 0.7422...  Val loss: 2.0205...  0.3630 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92251...  Training loss: 0.7440...  Val loss: 2.0038...  0.3633 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92276...  Training loss: 0.7430...  Val loss: 2.0463...  0.3633 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92301...  Training loss: 0.7693...  Val loss: 2.0421...  0.3634 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92326...  Training loss: 0.7565...  Val loss: 2.0000...  0.3632 sec/batch\n",
      "Epoch: 416/500...  Training Step: 92351...  Training loss: 0.7475...  Val loss: 2.0409...  0.3633 sec/batch\n",
      "Epoch: 417/500...  Training Step: 92376...  Training loss: 0.7438...  Val loss: 2.0286...  0.3634 sec/batch\n",
      "Epoch: 417/500...  Training Step: 92401...  Training loss: 0.7402...  Val loss: 2.0431...  0.3634 sec/batch\n",
      "Epoch: 417/500...  Training Step: 92426...  Training loss: 0.7386...  Val loss: 2.0531...  0.3633 sec/batch\n",
      "Epoch: 417/500...  Training Step: 92451...  Training loss: 0.7473...  Val loss: 2.0265...  0.3635 sec/batch\n",
      "Epoch: 417/500...  Training Step: 92476...  Training loss: 0.7294...  Val loss: 2.0224...  0.3639 sec/batch\n",
      "Epoch: 417/500...  Training Step: 92501...  Training loss: 0.7479...  Val loss: 2.0209...  0.3633 sec/batch\n",
      "Epoch: 417/500...  Training Step: 92526...  Training loss: 0.7532...  Val loss: 2.0194...  0.3640 sec/batch\n",
      "Epoch: 417/500...  Training Step: 92551...  Training loss: 0.7478...  Val loss: 2.0272...  0.3636 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92576...  Training loss: 0.7318...  Val loss: 2.0277...  0.3628 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92601...  Training loss: 0.7443...  Val loss: 2.0082...  0.3633 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92626...  Training loss: 0.7487...  Val loss: 2.0162...  0.3635 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92651...  Training loss: 0.7408...  Val loss: 2.0615...  0.3636 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92676...  Training loss: 0.7562...  Val loss: 2.0196...  0.3633 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92701...  Training loss: 0.7382...  Val loss: 2.0478...  0.3631 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92726...  Training loss: 0.7363...  Val loss: 2.0226...  0.3632 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92751...  Training loss: 0.7425...  Val loss: 2.0290...  0.3636 sec/batch\n",
      "Epoch: 418/500...  Training Step: 92776...  Training loss: 0.7191...  Val loss: 2.0240...  0.3633 sec/batch\n",
      "Epoch: 419/500...  Training Step: 92801...  Training loss: 0.7598...  Val loss: 2.0387...  0.3635 sec/batch\n",
      "Epoch: 419/500...  Training Step: 92826...  Training loss: 0.7308...  Val loss: 2.0234...  0.3633 sec/batch\n",
      "Epoch: 419/500...  Training Step: 92851...  Training loss: 0.7269...  Val loss: 2.0267...  0.3632 sec/batch\n",
      "Epoch: 419/500...  Training Step: 92876...  Training loss: 0.7482...  Val loss: 2.0385...  0.3635 sec/batch\n",
      "Epoch: 419/500...  Training Step: 92901...  Training loss: 0.7426...  Val loss: 2.0136...  0.3632 sec/batch\n",
      "Epoch: 419/500...  Training Step: 92926...  Training loss: 0.7289...  Val loss: 2.0433...  0.3632 sec/batch\n",
      "Epoch: 419/500...  Training Step: 92951...  Training loss: 0.7470...  Val loss: 2.0373...  0.3631 sec/batch\n",
      "Epoch: 419/500...  Training Step: 92976...  Training loss: 0.7331...  Val loss: 2.0297...  0.3635 sec/batch\n",
      "Epoch: 419/500...  Training Step: 93001...  Training loss: 0.7442...  Val loss: 2.0269...  0.3632 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93026...  Training loss: 0.7444...  Val loss: 2.0038...  0.3635 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93051...  Training loss: 0.7378...  Val loss: 2.0151...  0.3635 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93076...  Training loss: 0.7370...  Val loss: 2.0442...  0.3635 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93101...  Training loss: 0.7356...  Val loss: 2.0316...  0.3621 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93126...  Training loss: 0.7351...  Val loss: 2.0354...  0.3631 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93151...  Training loss: 0.7412...  Val loss: 2.0579...  0.3636 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93176...  Training loss: 0.7590...  Val loss: 2.0137...  0.3633 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93201...  Training loss: 0.7532...  Val loss: 2.0209...  0.3633 sec/batch\n",
      "Epoch: 420/500...  Training Step: 93226...  Training loss: 0.7413...  Val loss: 2.0339...  0.3634 sec/batch\n",
      "Epoch 420/500 time:81.58001136779785...  finished at 2017-10-30 19:10:47\n",
      "Epoch: 421/500...  Training Step: 93251...  Training loss: 0.7389...  Val loss: 2.0436...  0.3634 sec/batch\n",
      "Epoch: 421/500...  Training Step: 93276...  Training loss: 0.7335...  Val loss: 2.0400...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 421/500...  Training Step: 93301...  Training loss: 0.7386...  Val loss: 2.0398...  0.3634 sec/batch\n",
      "Epoch: 421/500...  Training Step: 93326...  Training loss: 0.7312...  Val loss: 2.0162...  0.3635 sec/batch\n",
      "Epoch: 421/500...  Training Step: 93351...  Training loss: 0.7594...  Val loss: 2.0420...  0.3634 sec/batch\n",
      "Epoch: 421/500...  Training Step: 93376...  Training loss: 0.7291...  Val loss: 2.0312...  0.3636 sec/batch\n",
      "Epoch: 421/500...  Training Step: 93401...  Training loss: 0.7513...  Val loss: 2.0202...  0.3634 sec/batch\n",
      "Epoch: 421/500...  Training Step: 93426...  Training loss: 0.7613...  Val loss: 2.0386...  0.3638 sec/batch\n",
      "Epoch: 421/500...  Training Step: 93451...  Training loss: 0.7292...  Val loss: 2.0199...  0.3633 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93476...  Training loss: 0.7304...  Val loss: 2.0345...  0.3632 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93501...  Training loss: 0.7346...  Val loss: 2.0352...  0.3632 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93526...  Training loss: 0.7469...  Val loss: 2.0182...  0.3631 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93551...  Training loss: 0.7489...  Val loss: 2.0418...  0.3630 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93576...  Training loss: 0.7382...  Val loss: 2.0463...  0.3632 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93601...  Training loss: 0.7450...  Val loss: 2.0423...  0.3634 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93626...  Training loss: 0.7385...  Val loss: 2.0463...  0.3633 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93651...  Training loss: 0.7462...  Val loss: 2.0241...  0.3633 sec/batch\n",
      "Epoch: 422/500...  Training Step: 93676...  Training loss: 0.7408...  Val loss: 2.0409...  0.3632 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93701...  Training loss: 0.7412...  Val loss: 2.0247...  0.3633 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93726...  Training loss: 0.7392...  Val loss: 2.0187...  0.3633 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93751...  Training loss: 0.7298...  Val loss: 2.0422...  0.3629 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93776...  Training loss: 0.7363...  Val loss: 2.0137...  0.3634 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93801...  Training loss: 0.7499...  Val loss: 2.0351...  0.3631 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93826...  Training loss: 0.7268...  Val loss: 2.0306...  0.3634 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93851...  Training loss: 0.7350...  Val loss: 2.0425...  0.3635 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93876...  Training loss: 0.7443...  Val loss: 2.0250...  0.3633 sec/batch\n",
      "Epoch: 423/500...  Training Step: 93901...  Training loss: 0.7325...  Val loss: 2.0317...  0.3637 sec/batch\n",
      "Epoch: 424/500...  Training Step: 93926...  Training loss: 0.7354...  Val loss: 1.9902...  0.3635 sec/batch\n",
      "Epoch: 424/500...  Training Step: 93951...  Training loss: 0.7318...  Val loss: 2.0244...  0.3633 sec/batch\n",
      "Epoch: 424/500...  Training Step: 93976...  Training loss: 0.7380...  Val loss: 2.0375...  0.3632 sec/batch\n",
      "Epoch: 424/500...  Training Step: 94001...  Training loss: 0.7303...  Val loss: 2.0266...  0.3630 sec/batch\n",
      "Epoch: 424/500...  Training Step: 94026...  Training loss: 0.7311...  Val loss: 2.0228...  0.3630 sec/batch\n",
      "Epoch: 424/500...  Training Step: 94051...  Training loss: 0.7309...  Val loss: 2.0372...  0.3635 sec/batch\n",
      "Epoch: 424/500...  Training Step: 94076...  Training loss: 0.7279...  Val loss: 2.0294...  0.3637 sec/batch\n",
      "Epoch: 424/500...  Training Step: 94101...  Training loss: 0.7338...  Val loss: 2.0098...  0.3639 sec/batch\n",
      "Epoch: 424/500...  Training Step: 94126...  Training loss: 0.7368...  Val loss: 2.0270...  0.3630 sec/batch\n",
      "Epoch: 425/500...  Training Step: 94151...  Training loss: 0.7311...  Val loss: 2.0026...  0.3637 sec/batch\n",
      "Epoch: 425/500...  Training Step: 94176...  Training loss: 0.7305...  Val loss: 2.0326...  0.3638 sec/batch\n",
      "Epoch: 425/500...  Training Step: 94201...  Training loss: 0.7221...  Val loss: 2.0241...  0.3629 sec/batch\n",
      "Epoch: 425/500...  Training Step: 94226...  Training loss: 0.7324...  Val loss: 2.0168...  0.3635 sec/batch\n",
      "Epoch: 425/500...  Training Step: 94251...  Training loss: 0.7228...  Val loss: 2.0343...  0.3646 sec/batch\n",
      "Epoch: 425/500...  Training Step: 94276...  Training loss: 0.7380...  Val loss: 2.0355...  0.3642 sec/batch\n",
      "Epoch: 425/500...  Training Step: 94301...  Training loss: 0.7508...  Val loss: 2.0345...  0.3633 sec/batch\n",
      "Epoch: 425/500...  Training Step: 94326...  Training loss: 0.7476...  Val loss: 2.0348...  0.3634 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94351...  Training loss: 0.8201...  Val loss: 2.0476...  0.3632 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94376...  Training loss: 0.7352...  Val loss: 2.0289...  0.3637 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94401...  Training loss: 0.7282...  Val loss: 2.0359...  0.3633 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94426...  Training loss: 0.7192...  Val loss: 2.0528...  0.3630 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94451...  Training loss: 0.7400...  Val loss: 2.0374...  0.3633 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94476...  Training loss: 0.7277...  Val loss: 2.0365...  0.3633 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94501...  Training loss: 0.7361...  Val loss: 2.0372...  0.3632 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94526...  Training loss: 0.7255...  Val loss: 2.0394...  0.3631 sec/batch\n",
      "Epoch: 426/500...  Training Step: 94551...  Training loss: 0.7384...  Val loss: 2.0351...  0.3633 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94576...  Training loss: 0.7495...  Val loss: 2.0448...  0.3636 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94601...  Training loss: 0.7278...  Val loss: 2.0302...  0.3633 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94626...  Training loss: 0.7205...  Val loss: 2.0392...  0.3631 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94651...  Training loss: 0.7262...  Val loss: 2.0642...  0.3634 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94676...  Training loss: 0.7306...  Val loss: 2.0516...  0.3631 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94701...  Training loss: 0.7326...  Val loss: 2.0356...  0.3631 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94726...  Training loss: 0.7441...  Val loss: 2.0345...  0.3638 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94751...  Training loss: 0.7439...  Val loss: 2.0370...  0.3632 sec/batch\n",
      "Epoch: 427/500...  Training Step: 94776...  Training loss: 0.7390...  Val loss: 2.0260...  0.3631 sec/batch\n",
      "Epoch: 428/500...  Training Step: 94801...  Training loss: 0.7495...  Val loss: 2.0451...  0.3632 sec/batch\n",
      "Epoch: 428/500...  Training Step: 94826...  Training loss: 0.7419...  Val loss: 2.0378...  0.3638 sec/batch\n",
      "Epoch: 428/500...  Training Step: 94851...  Training loss: 0.7244...  Val loss: 2.0442...  0.3628 sec/batch\n",
      "Epoch: 428/500...  Training Step: 94876...  Training loss: 0.7378...  Val loss: 2.0331...  0.3634 sec/batch\n",
      "Epoch: 428/500...  Training Step: 94901...  Training loss: 0.7155...  Val loss: 2.0535...  0.3635 sec/batch\n",
      "Epoch: 428/500...  Training Step: 94926...  Training loss: 0.7452...  Val loss: 2.0399...  0.3634 sec/batch\n",
      "Epoch: 428/500...  Training Step: 94951...  Training loss: 0.7392...  Val loss: 2.0184...  0.3635 sec/batch\n",
      "Epoch: 428/500...  Training Step: 94976...  Training loss: 0.7519...  Val loss: 2.0401...  0.3635 sec/batch\n",
      "Epoch: 428/500...  Training Step: 95001...  Training loss: 0.7387...  Val loss: 2.0423...  0.3633 sec/batch\n",
      "Epoch: 429/500...  Training Step: 95026...  Training loss: 0.7200...  Val loss: 2.0438...  0.3631 sec/batch\n",
      "Epoch: 429/500...  Training Step: 95051...  Training loss: 0.7418...  Val loss: 2.0400...  0.3629 sec/batch\n",
      "Epoch: 429/500...  Training Step: 95076...  Training loss: 0.7329...  Val loss: 2.0213...  0.3633 sec/batch\n",
      "Epoch: 429/500...  Training Step: 95101...  Training loss: 0.7311...  Val loss: 2.0301...  0.3633 sec/batch\n",
      "Epoch: 429/500...  Training Step: 95126...  Training loss: 0.7416...  Val loss: 2.0282...  0.3636 sec/batch\n",
      "Epoch: 429/500...  Training Step: 95151...  Training loss: 0.7323...  Val loss: 2.0463...  0.3630 sec/batch\n",
      "Epoch: 429/500...  Training Step: 95176...  Training loss: 0.7419...  Val loss: 2.0042...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 429/500...  Training Step: 95201...  Training loss: 0.7333...  Val loss: 2.0454...  0.3636 sec/batch\n",
      "Epoch: 429/500...  Training Step: 95226...  Training loss: 0.7243...  Val loss: 2.0251...  0.3630 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95251...  Training loss: 0.7370...  Val loss: 2.0369...  0.3638 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95276...  Training loss: 0.7195...  Val loss: 2.0466...  0.3633 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95301...  Training loss: 0.7402...  Val loss: 2.0366...  0.3634 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95326...  Training loss: 0.7292...  Val loss: 2.0333...  0.3636 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95351...  Training loss: 0.7182...  Val loss: 2.0387...  0.3640 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95376...  Training loss: 0.7378...  Val loss: 2.0423...  0.3636 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95401...  Training loss: 0.7435...  Val loss: 2.0134...  0.3634 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95426...  Training loss: 0.7346...  Val loss: 2.0226...  0.3622 sec/batch\n",
      "Epoch: 430/500...  Training Step: 95451...  Training loss: 0.7418...  Val loss: 2.0396...  0.3631 sec/batch\n",
      "Epoch 430/500 time:81.57489490509033...  finished at 2017-10-30 19:24:25\n",
      "Epoch: 431/500...  Training Step: 95476...  Training loss: 0.7205...  Val loss: 2.0334...  0.3633 sec/batch\n",
      "Epoch: 431/500...  Training Step: 95501...  Training loss: 0.7250...  Val loss: 2.0199...  0.3632 sec/batch\n",
      "Epoch: 431/500...  Training Step: 95526...  Training loss: 0.7400...  Val loss: 2.0426...  0.3635 sec/batch\n",
      "Epoch: 431/500...  Training Step: 95551...  Training loss: 0.7402...  Val loss: 2.0235...  0.3630 sec/batch\n",
      "Epoch: 431/500...  Training Step: 95576...  Training loss: 0.7352...  Val loss: 2.0341...  0.3645 sec/batch\n",
      "Epoch: 431/500...  Training Step: 95601...  Training loss: 0.7155...  Val loss: 2.0496...  0.3632 sec/batch\n",
      "Epoch: 431/500...  Training Step: 95626...  Training loss: 0.7435...  Val loss: 2.0312...  0.3634 sec/batch\n",
      "Epoch: 431/500...  Training Step: 95651...  Training loss: 0.7302...  Val loss: 2.0157...  0.3630 sec/batch\n",
      "Epoch: 431/500...  Training Step: 95676...  Training loss: 0.7329...  Val loss: 2.0167...  0.3634 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95701...  Training loss: 0.7310...  Val loss: 2.0360...  0.3644 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95726...  Training loss: 0.7319...  Val loss: 2.0285...  0.3634 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95751...  Training loss: 0.7416...  Val loss: 2.0642...  0.3634 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95776...  Training loss: 0.7337...  Val loss: 2.0412...  0.3634 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95801...  Training loss: 0.7305...  Val loss: 2.0682...  0.3634 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95826...  Training loss: 0.7484...  Val loss: 2.0576...  0.3645 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95851...  Training loss: 0.7300...  Val loss: 2.0544...  0.3636 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95876...  Training loss: 0.7345...  Val loss: 2.0505...  0.3635 sec/batch\n",
      "Epoch: 432/500...  Training Step: 95901...  Training loss: 0.7593...  Val loss: 2.0480...  0.3628 sec/batch\n",
      "Epoch: 433/500...  Training Step: 95926...  Training loss: 0.7157...  Val loss: 2.0397...  0.3632 sec/batch\n",
      "Epoch: 433/500...  Training Step: 95951...  Training loss: 0.7404...  Val loss: 2.0436...  0.3648 sec/batch\n",
      "Epoch: 433/500...  Training Step: 95976...  Training loss: 0.7172...  Val loss: 2.0639...  0.3633 sec/batch\n",
      "Epoch: 433/500...  Training Step: 96001...  Training loss: 0.7212...  Val loss: 2.0277...  0.3635 sec/batch\n",
      "Epoch: 433/500...  Training Step: 96026...  Training loss: 0.7371...  Val loss: 2.0239...  0.3655 sec/batch\n",
      "Epoch: 433/500...  Training Step: 96051...  Training loss: 0.7377...  Val loss: 2.0193...  0.3637 sec/batch\n",
      "Epoch: 433/500...  Training Step: 96076...  Training loss: 0.7164...  Val loss: 2.0307...  0.3633 sec/batch\n",
      "Epoch: 433/500...  Training Step: 96101...  Training loss: 0.7376...  Val loss: 2.0438...  0.3631 sec/batch\n",
      "Epoch: 433/500...  Training Step: 96126...  Training loss: 0.7358...  Val loss: 2.0448...  0.3639 sec/batch\n",
      "Epoch: 434/500...  Training Step: 96151...  Training loss: 0.7240...  Val loss: 2.0094...  0.3670 sec/batch\n",
      "Epoch: 434/500...  Training Step: 96176...  Training loss: 0.7367...  Val loss: 2.0448...  0.3635 sec/batch\n",
      "Epoch: 434/500...  Training Step: 96201...  Training loss: 0.7575...  Val loss: 2.0460...  0.3635 sec/batch\n",
      "Epoch: 434/500...  Training Step: 96226...  Training loss: 0.7187...  Val loss: 2.0424...  0.3632 sec/batch\n",
      "Epoch: 434/500...  Training Step: 96251...  Training loss: 0.7412...  Val loss: 2.0701...  0.3627 sec/batch\n",
      "Epoch: 434/500...  Training Step: 96276...  Training loss: 0.7272...  Val loss: 2.0380...  0.3664 sec/batch\n",
      "Epoch: 434/500...  Training Step: 96301...  Training loss: 0.7309...  Val loss: 2.0358...  0.3635 sec/batch\n",
      "Epoch: 434/500...  Training Step: 96326...  Training loss: 0.7369...  Val loss: 2.0652...  0.3634 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96351...  Training loss: 0.7494...  Val loss: 2.0289...  0.3638 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96376...  Training loss: 0.7335...  Val loss: 2.0285...  0.3637 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96401...  Training loss: 0.7013...  Val loss: 2.0284...  0.3660 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96426...  Training loss: 0.7231...  Val loss: 2.0558...  0.3629 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96451...  Training loss: 0.7223...  Val loss: 2.0386...  0.3638 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96476...  Training loss: 0.7203...  Val loss: 2.0521...  0.3641 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96501...  Training loss: 0.7278...  Val loss: 2.0212...  0.3633 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96526...  Training loss: 0.7269...  Val loss: 2.0538...  0.3633 sec/batch\n",
      "Epoch: 435/500...  Training Step: 96551...  Training loss: 0.7381...  Val loss: 2.0456...  0.3635 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96576...  Training loss: 0.7416...  Val loss: 2.0211...  0.3636 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96601...  Training loss: 0.7175...  Val loss: 2.0210...  0.3672 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96626...  Training loss: 0.7394...  Val loss: 2.0375...  0.3634 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96651...  Training loss: 0.7099...  Val loss: 2.0550...  0.3634 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96676...  Training loss: 0.7441...  Val loss: 2.0641...  0.3637 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96701...  Training loss: 0.7267...  Val loss: 2.0350...  0.3639 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96726...  Training loss: 0.7374...  Val loss: 2.0252...  0.3664 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96751...  Training loss: 0.7456...  Val loss: 2.0422...  0.3635 sec/batch\n",
      "Epoch: 436/500...  Training Step: 96776...  Training loss: 0.7338...  Val loss: 2.0536...  0.3638 sec/batch\n",
      "Epoch: 437/500...  Training Step: 96801...  Training loss: 0.7305...  Val loss: 2.0699...  0.3634 sec/batch\n",
      "Epoch: 437/500...  Training Step: 96826...  Training loss: 0.7239...  Val loss: 2.0601...  0.3634 sec/batch\n",
      "Epoch: 437/500...  Training Step: 96851...  Training loss: 0.7385...  Val loss: 2.0636...  0.3659 sec/batch\n",
      "Epoch: 437/500...  Training Step: 96876...  Training loss: 0.7397...  Val loss: 2.0392...  0.3632 sec/batch\n",
      "Epoch: 437/500...  Training Step: 96901...  Training loss: 0.7062...  Val loss: 2.0380...  0.3637 sec/batch\n",
      "Epoch: 437/500...  Training Step: 96926...  Training loss: 0.7055...  Val loss: 2.0571...  0.3632 sec/batch\n",
      "Epoch: 437/500...  Training Step: 96951...  Training loss: 0.7391...  Val loss: 2.0595...  0.3635 sec/batch\n",
      "Epoch: 437/500...  Training Step: 96976...  Training loss: 0.7606...  Val loss: 2.0459...  0.3670 sec/batch\n",
      "Epoch: 437/500...  Training Step: 97001...  Training loss: 0.7298...  Val loss: 2.0493...  0.3634 sec/batch\n",
      "Epoch: 438/500...  Training Step: 97026...  Training loss: 0.7178...  Val loss: 2.0601...  0.3631 sec/batch\n",
      "Epoch: 438/500...  Training Step: 97051...  Training loss: 0.7187...  Val loss: 2.0260...  0.3666 sec/batch\n",
      "Epoch: 438/500...  Training Step: 97076...  Training loss: 0.7303...  Val loss: 2.0337...  0.3629 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 438/500...  Training Step: 97101...  Training loss: 0.7336...  Val loss: 2.0433...  0.3633 sec/batch\n",
      "Epoch: 438/500...  Training Step: 97126...  Training loss: 0.7342...  Val loss: 2.0391...  0.3633 sec/batch\n",
      "Epoch: 438/500...  Training Step: 97151...  Training loss: 0.7050...  Val loss: 2.0643...  0.3636 sec/batch\n",
      "Epoch: 438/500...  Training Step: 97176...  Training loss: 0.7439...  Val loss: 2.0459...  0.3666 sec/batch\n",
      "Epoch: 438/500...  Training Step: 97201...  Training loss: 0.7376...  Val loss: 2.0387...  0.3631 sec/batch\n",
      "Epoch: 438/500...  Training Step: 97226...  Training loss: 0.7196...  Val loss: 2.0379...  0.3629 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97251...  Training loss: 0.7410...  Val loss: 2.0342...  0.3634 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97276...  Training loss: 0.7160...  Val loss: 2.0492...  0.3637 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97301...  Training loss: 0.7239...  Val loss: 2.0649...  0.3662 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97326...  Training loss: 0.7068...  Val loss: 2.0708...  0.3634 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97351...  Training loss: 0.7313...  Val loss: 2.0580...  0.3631 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97376...  Training loss: 0.7163...  Val loss: 2.0498...  0.3634 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97401...  Training loss: 0.7127...  Val loss: 2.0403...  0.3630 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97426...  Training loss: 0.7427...  Val loss: 2.0624...  0.3664 sec/batch\n",
      "Epoch: 439/500...  Training Step: 97451...  Training loss: 0.7408...  Val loss: 2.0685...  0.3635 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97476...  Training loss: 0.7270...  Val loss: 2.0277...  0.3638 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97501...  Training loss: 0.7189...  Val loss: 2.0433...  0.3630 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97526...  Training loss: 0.7342...  Val loss: 2.0554...  0.3630 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97551...  Training loss: 0.7135...  Val loss: 2.0277...  0.3635 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97576...  Training loss: 0.7467...  Val loss: 2.0368...  0.3631 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97601...  Training loss: 0.7356...  Val loss: 2.0580...  0.3630 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97626...  Training loss: 0.7333...  Val loss: 2.0373...  0.3655 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97651...  Training loss: 0.7164...  Val loss: 2.0456...  0.3636 sec/batch\n",
      "Epoch: 440/500...  Training Step: 97676...  Training loss: 0.7434...  Val loss: 2.0356...  0.3637 sec/batch\n",
      "Epoch 440/500 time:82.0708270072937...  finished at 2017-10-30 19:38:04\n",
      "Epoch: 441/500...  Training Step: 97701...  Training loss: 0.7263...  Val loss: 2.0428...  0.3633 sec/batch\n",
      "Epoch: 441/500...  Training Step: 97726...  Training loss: 0.7194...  Val loss: 2.0360...  0.3632 sec/batch\n",
      "Epoch: 441/500...  Training Step: 97751...  Training loss: 0.7373...  Val loss: 2.0577...  0.3660 sec/batch\n",
      "Epoch: 441/500...  Training Step: 97776...  Training loss: 0.7213...  Val loss: 2.0384...  0.3640 sec/batch\n",
      "Epoch: 441/500...  Training Step: 97801...  Training loss: 0.7343...  Val loss: 2.0391...  0.3626 sec/batch\n",
      "Epoch: 441/500...  Training Step: 97826...  Training loss: 0.7253...  Val loss: 2.0748...  0.3631 sec/batch\n",
      "Epoch: 441/500...  Training Step: 97851...  Training loss: 0.7480...  Val loss: 2.0509...  0.3635 sec/batch\n",
      "Epoch: 441/500...  Training Step: 97876...  Training loss: 0.7289...  Val loss: 2.0244...  0.3659 sec/batch\n",
      "Epoch: 441/500...  Training Step: 97901...  Training loss: 0.7346...  Val loss: 2.0862...  0.3634 sec/batch\n",
      "Epoch: 442/500...  Training Step: 97926...  Training loss: 0.7350...  Val loss: 2.0512...  0.3633 sec/batch\n",
      "Epoch: 442/500...  Training Step: 97951...  Training loss: 0.7303...  Val loss: 2.0578...  0.3629 sec/batch\n",
      "Epoch: 442/500...  Training Step: 97976...  Training loss: 0.7162...  Val loss: 2.0791...  0.3634 sec/batch\n",
      "Epoch: 442/500...  Training Step: 98001...  Training loss: 0.7211...  Val loss: 2.0543...  0.3659 sec/batch\n",
      "Epoch: 442/500...  Training Step: 98026...  Training loss: 0.7079...  Val loss: 2.0610...  0.3636 sec/batch\n",
      "Epoch: 442/500...  Training Step: 98051...  Training loss: 0.7333...  Val loss: 2.0474...  0.3638 sec/batch\n",
      "Epoch: 442/500...  Training Step: 98076...  Training loss: 0.7419...  Val loss: 2.0457...  0.3628 sec/batch\n",
      "Epoch: 442/500...  Training Step: 98101...  Training loss: 0.7313...  Val loss: 2.0556...  0.3635 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98126...  Training loss: 0.7143...  Val loss: 2.0492...  0.3634 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98151...  Training loss: 0.7179...  Val loss: 2.0347...  0.3635 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98176...  Training loss: 0.7365...  Val loss: 2.0406...  0.3635 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98201...  Training loss: 0.7149...  Val loss: 2.0793...  0.3631 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98226...  Training loss: 0.7298...  Val loss: 2.0572...  0.3631 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98251...  Training loss: 0.7163...  Val loss: 2.0732...  0.3634 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98276...  Training loss: 0.7284...  Val loss: 2.0519...  0.3633 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98301...  Training loss: 0.7056...  Val loss: 2.0577...  0.3635 sec/batch\n",
      "Epoch: 443/500...  Training Step: 98326...  Training loss: 0.7111...  Val loss: 2.0446...  0.3628 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98351...  Training loss: 0.7294...  Val loss: 2.0676...  0.3631 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98376...  Training loss: 0.7048...  Val loss: 2.0383...  0.3629 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98401...  Training loss: 0.6996...  Val loss: 2.0572...  0.3630 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98426...  Training loss: 0.7210...  Val loss: 2.0770...  0.3633 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98451...  Training loss: 0.7353...  Val loss: 2.0434...  0.3635 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98476...  Training loss: 0.7151...  Val loss: 2.0661...  0.3637 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98501...  Training loss: 0.7188...  Val loss: 2.0527...  0.3634 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98526...  Training loss: 0.7130...  Val loss: 2.0600...  0.3632 sec/batch\n",
      "Epoch: 444/500...  Training Step: 98551...  Training loss: 0.7266...  Val loss: 2.0491...  0.3627 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98576...  Training loss: 0.7403...  Val loss: 2.0457...  0.3631 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98601...  Training loss: 0.7161...  Val loss: 2.0287...  0.3638 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98626...  Training loss: 0.7187...  Val loss: 2.0754...  0.3633 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98651...  Training loss: 0.7067...  Val loss: 2.0760...  0.3629 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98676...  Training loss: 0.7108...  Val loss: 2.0613...  0.3634 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98701...  Training loss: 0.7311...  Val loss: 2.0710...  0.3633 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98726...  Training loss: 0.7334...  Val loss: 2.0451...  0.3634 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98751...  Training loss: 0.7281...  Val loss: 2.0489...  0.3629 sec/batch\n",
      "Epoch: 445/500...  Training Step: 98776...  Training loss: 0.7359...  Val loss: 2.0544...  0.3630 sec/batch\n",
      "Epoch: 446/500...  Training Step: 98801...  Training loss: 0.7282...  Val loss: 2.0751...  0.3635 sec/batch\n",
      "Epoch: 446/500...  Training Step: 98826...  Training loss: 0.7212...  Val loss: 2.0487...  0.3637 sec/batch\n",
      "Epoch: 446/500...  Training Step: 98851...  Training loss: 0.7175...  Val loss: 2.0704...  0.3633 sec/batch\n",
      "Epoch: 446/500...  Training Step: 98876...  Training loss: 0.7339...  Val loss: 2.0546...  0.3631 sec/batch\n",
      "Epoch: 446/500...  Training Step: 98901...  Training loss: 0.7488...  Val loss: 2.0695...  0.3630 sec/batch\n",
      "Epoch: 446/500...  Training Step: 98926...  Training loss: 0.7116...  Val loss: 2.0639...  0.3633 sec/batch\n",
      "Epoch: 446/500...  Training Step: 98951...  Training loss: 0.7487...  Val loss: 2.0368...  0.3633 sec/batch\n",
      "Epoch: 446/500...  Training Step: 98976...  Training loss: 0.7460...  Val loss: 2.0702...  0.3634 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 446/500...  Training Step: 99001...  Training loss: 0.7269...  Val loss: 2.0489...  0.3630 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99026...  Training loss: 0.7286...  Val loss: 2.0714...  0.3631 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99051...  Training loss: 0.7183...  Val loss: 2.0571...  0.3633 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99076...  Training loss: 0.7295...  Val loss: 2.0349...  0.3638 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99101...  Training loss: 0.7227...  Val loss: 2.0633...  0.3634 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99126...  Training loss: 0.7117...  Val loss: 2.0674...  0.3635 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99151...  Training loss: 0.7321...  Val loss: 2.0635...  0.3633 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99176...  Training loss: 0.7201...  Val loss: 2.0618...  0.3635 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99201...  Training loss: 0.7344...  Val loss: 2.0501...  0.3635 sec/batch\n",
      "Epoch: 447/500...  Training Step: 99226...  Training loss: 0.7214...  Val loss: 2.0563...  0.3638 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99251...  Training loss: 0.7279...  Val loss: 2.0571...  0.3639 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99276...  Training loss: 0.7380...  Val loss: 2.0390...  0.3635 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99301...  Training loss: 0.7113...  Val loss: 2.0801...  0.3636 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99326...  Training loss: 0.7193...  Val loss: 2.0376...  0.3631 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99351...  Training loss: 0.7122...  Val loss: 2.0593...  0.3631 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99376...  Training loss: 0.7196...  Val loss: 2.0703...  0.3639 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99401...  Training loss: 0.7101...  Val loss: 2.0507...  0.3628 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99426...  Training loss: 0.7252...  Val loss: 2.0502...  0.3636 sec/batch\n",
      "Epoch: 448/500...  Training Step: 99451...  Training loss: 0.7188...  Val loss: 2.0601...  0.3631 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99476...  Training loss: 0.7294...  Val loss: 2.0214...  0.3635 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99501...  Training loss: 0.7105...  Val loss: 2.0453...  0.3637 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99526...  Training loss: 0.7138...  Val loss: 2.0606...  0.3631 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99551...  Training loss: 0.7114...  Val loss: 2.0546...  0.3636 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99576...  Training loss: 0.7148...  Val loss: 2.0435...  0.3631 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99601...  Training loss: 0.7184...  Val loss: 2.0658...  0.3630 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99626...  Training loss: 0.7159...  Val loss: 2.0585...  0.3633 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99651...  Training loss: 0.7071...  Val loss: 2.0423...  0.3635 sec/batch\n",
      "Epoch: 449/500...  Training Step: 99676...  Training loss: 0.7116...  Val loss: 2.0525...  0.3635 sec/batch\n",
      "Epoch: 450/500...  Training Step: 99701...  Training loss: 0.7195...  Val loss: 2.0451...  0.3633 sec/batch\n",
      "Epoch: 450/500...  Training Step: 99726...  Training loss: 0.7148...  Val loss: 2.0748...  0.3631 sec/batch\n",
      "Epoch: 450/500...  Training Step: 99751...  Training loss: 0.7127...  Val loss: 2.0565...  0.3633 sec/batch\n",
      "Epoch: 450/500...  Training Step: 99776...  Training loss: 0.7059...  Val loss: 2.0342...  0.3633 sec/batch\n",
      "Epoch: 450/500...  Training Step: 99801...  Training loss: 0.7188...  Val loss: 2.0663...  0.3634 sec/batch\n",
      "Epoch: 450/500...  Training Step: 99826...  Training loss: 0.7224...  Val loss: 2.0599...  0.3629 sec/batch\n",
      "Epoch: 450/500...  Training Step: 99851...  Training loss: 0.7408...  Val loss: 2.0647...  0.3637 sec/batch\n",
      "Epoch: 450/500...  Training Step: 99876...  Training loss: 0.7291...  Val loss: 2.0570...  0.3633 sec/batch\n",
      "Epoch 450/500 time:81.5025749206543...  finished at 2017-10-30 19:51:41\n",
      "Epoch: 451/500...  Training Step: 99901...  Training loss: 0.8034...  Val loss: 2.0646...  0.3631 sec/batch\n",
      "Epoch: 451/500...  Training Step: 99926...  Training loss: 0.7305...  Val loss: 2.0713...  0.3639 sec/batch\n",
      "Epoch: 451/500...  Training Step: 99951...  Training loss: 0.7074...  Val loss: 2.0655...  0.3646 sec/batch\n",
      "Epoch: 451/500...  Training Step: 99976...  Training loss: 0.7179...  Val loss: 2.0775...  0.3632 sec/batch\n",
      "Epoch: 451/500...  Training Step: 100001...  Training loss: 0.7161...  Val loss: 2.0581...  0.3633 sec/batch\n",
      "Epoch: 451/500...  Training Step: 100026...  Training loss: 0.7153...  Val loss: 2.0576...  0.3637 sec/batch\n",
      "Epoch: 451/500...  Training Step: 100051...  Training loss: 0.7178...  Val loss: 2.0688...  0.3636 sec/batch\n",
      "Epoch: 451/500...  Training Step: 100076...  Training loss: 0.7095...  Val loss: 2.0637...  0.3621 sec/batch\n",
      "Epoch: 451/500...  Training Step: 100101...  Training loss: 0.7246...  Val loss: 2.0721...  0.3634 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100126...  Training loss: 0.7443...  Val loss: 2.0655...  0.3631 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100151...  Training loss: 0.7129...  Val loss: 2.0631...  0.3632 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100176...  Training loss: 0.7178...  Val loss: 2.0564...  0.3629 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100201...  Training loss: 0.7097...  Val loss: 2.0967...  0.3636 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100226...  Training loss: 0.7097...  Val loss: 2.0756...  0.3632 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100251...  Training loss: 0.7156...  Val loss: 2.0536...  0.3627 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100276...  Training loss: 0.7142...  Val loss: 2.0586...  0.3635 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100301...  Training loss: 0.7226...  Val loss: 2.0699...  0.3637 sec/batch\n",
      "Epoch: 452/500...  Training Step: 100326...  Training loss: 0.7257...  Val loss: 2.0579...  0.3639 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100351...  Training loss: 0.7410...  Val loss: 2.0773...  0.3635 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100376...  Training loss: 0.7466...  Val loss: 2.0678...  0.3630 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100401...  Training loss: 0.7157...  Val loss: 2.0761...  0.3633 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100426...  Training loss: 0.7144...  Val loss: 2.0830...  0.3633 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100451...  Training loss: 0.6987...  Val loss: 2.0756...  0.3633 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100476...  Training loss: 0.7220...  Val loss: 2.0713...  0.3630 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100501...  Training loss: 0.7094...  Val loss: 2.0575...  0.3636 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100526...  Training loss: 0.7193...  Val loss: 2.0742...  0.3634 sec/batch\n",
      "Epoch: 453/500...  Training Step: 100551...  Training loss: 0.7244...  Val loss: 2.0666...  0.3635 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100576...  Training loss: 0.7072...  Val loss: 2.0717...  0.3632 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100601...  Training loss: 0.7285...  Val loss: 2.0676...  0.3634 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100626...  Training loss: 0.7236...  Val loss: 2.0629...  0.3630 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100651...  Training loss: 0.7205...  Val loss: 2.0560...  0.3634 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100676...  Training loss: 0.7313...  Val loss: 2.0462...  0.3636 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100701...  Training loss: 0.7202...  Val loss: 2.0772...  0.3635 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100726...  Training loss: 0.7351...  Val loss: 2.0345...  0.3628 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100751...  Training loss: 0.7111...  Val loss: 2.0814...  0.3629 sec/batch\n",
      "Epoch: 454/500...  Training Step: 100776...  Training loss: 0.7111...  Val loss: 2.0463...  0.3633 sec/batch\n",
      "Epoch: 455/500...  Training Step: 100801...  Training loss: 0.7223...  Val loss: 2.0598...  0.3638 sec/batch\n",
      "Epoch: 455/500...  Training Step: 100826...  Training loss: 0.7134...  Val loss: 2.0683...  0.3629 sec/batch\n",
      "Epoch: 455/500...  Training Step: 100851...  Training loss: 0.7181...  Val loss: 2.0599...  0.3628 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 455/500...  Training Step: 100876...  Training loss: 0.7288...  Val loss: 2.0589...  0.3630 sec/batch\n",
      "Epoch: 455/500...  Training Step: 100901...  Training loss: 0.6959...  Val loss: 2.0618...  0.3619 sec/batch\n",
      "Epoch: 455/500...  Training Step: 100926...  Training loss: 0.7212...  Val loss: 2.0759...  0.3636 sec/batch\n",
      "Epoch: 455/500...  Training Step: 100951...  Training loss: 0.7242...  Val loss: 2.0591...  0.3635 sec/batch\n",
      "Epoch: 455/500...  Training Step: 100976...  Training loss: 0.7229...  Val loss: 2.0623...  0.3634 sec/batch\n",
      "Epoch: 455/500...  Training Step: 101001...  Training loss: 0.7407...  Val loss: 2.0774...  0.3636 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101026...  Training loss: 0.6986...  Val loss: 2.0493...  0.3634 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101051...  Training loss: 0.7118...  Val loss: 2.0566...  0.3633 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101076...  Training loss: 0.7210...  Val loss: 2.0641...  0.3636 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101101...  Training loss: 0.7160...  Val loss: 2.0572...  0.3634 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101126...  Training loss: 0.7186...  Val loss: 2.0659...  0.3637 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101151...  Training loss: 0.7045...  Val loss: 2.0715...  0.3636 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101176...  Training loss: 0.7302...  Val loss: 2.0492...  0.3635 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101201...  Training loss: 0.7187...  Val loss: 2.0508...  0.3632 sec/batch\n",
      "Epoch: 456/500...  Training Step: 101226...  Training loss: 0.7254...  Val loss: 2.0514...  0.3630 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101251...  Training loss: 0.7276...  Val loss: 2.0668...  0.3632 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101276...  Training loss: 0.7192...  Val loss: 2.0638...  0.3634 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101301...  Training loss: 0.7240...  Val loss: 2.0714...  0.3639 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101326...  Training loss: 0.7248...  Val loss: 2.0622...  0.3637 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101351...  Training loss: 0.7164...  Val loss: 2.0814...  0.3746 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101376...  Training loss: 0.7291...  Val loss: 2.0865...  0.3635 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101401...  Training loss: 0.7203...  Val loss: 2.0735...  0.3632 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101426...  Training loss: 0.7282...  Val loss: 2.0709...  0.3637 sec/batch\n",
      "Epoch: 457/500...  Training Step: 101451...  Training loss: 0.7392...  Val loss: 2.0765...  0.3631 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101476...  Training loss: 0.6917...  Val loss: 2.0583...  0.3814 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101501...  Training loss: 0.7095...  Val loss: 2.0667...  0.3636 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101526...  Training loss: 0.7163...  Val loss: 2.0814...  0.3631 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101551...  Training loss: 0.7092...  Val loss: 2.0525...  0.3642 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101576...  Training loss: 0.7150...  Val loss: 2.0619...  0.3636 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101601...  Training loss: 0.7318...  Val loss: 2.0455...  0.3637 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101626...  Training loss: 0.7225...  Val loss: 2.0587...  0.3629 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101651...  Training loss: 0.7214...  Val loss: 2.0553...  0.3636 sec/batch\n",
      "Epoch: 458/500...  Training Step: 101676...  Training loss: 0.7339...  Val loss: 2.0644...  0.3634 sec/batch\n",
      "Epoch: 459/500...  Training Step: 101701...  Training loss: 0.7017...  Val loss: 2.0490...  0.3633 sec/batch\n",
      "Epoch: 459/500...  Training Step: 101726...  Training loss: 0.7172...  Val loss: 2.0683...  0.3634 sec/batch\n",
      "Epoch: 459/500...  Training Step: 101751...  Training loss: 0.7334...  Val loss: 2.0723...  0.3634 sec/batch\n",
      "Epoch: 459/500...  Training Step: 101776...  Training loss: 0.7072...  Val loss: 2.0606...  0.3634 sec/batch\n",
      "Epoch: 459/500...  Training Step: 101801...  Training loss: 0.7052...  Val loss: 2.0895...  0.3631 sec/batch\n",
      "Epoch: 459/500...  Training Step: 101826...  Training loss: 0.7282...  Val loss: 2.0680...  0.3639 sec/batch\n",
      "Epoch: 459/500...  Training Step: 101851...  Training loss: 0.7151...  Val loss: 2.0618...  0.3630 sec/batch\n",
      "Epoch: 459/500...  Training Step: 101876...  Training loss: 0.7166...  Val loss: 2.0885...  0.3631 sec/batch\n",
      "Epoch: 460/500...  Training Step: 101901...  Training loss: 0.7262...  Val loss: 2.0452...  0.3634 sec/batch\n",
      "Epoch: 460/500...  Training Step: 101926...  Training loss: 0.7071...  Val loss: 2.0639...  0.3634 sec/batch\n",
      "Epoch: 460/500...  Training Step: 101951...  Training loss: 0.6901...  Val loss: 2.0613...  0.3661 sec/batch\n",
      "Epoch: 460/500...  Training Step: 101976...  Training loss: 0.7044...  Val loss: 2.0915...  0.3632 sec/batch\n",
      "Epoch: 460/500...  Training Step: 102001...  Training loss: 0.7041...  Val loss: 2.0579...  0.3631 sec/batch\n",
      "Epoch: 460/500...  Training Step: 102026...  Training loss: 0.7020...  Val loss: 2.0687...  0.3634 sec/batch\n",
      "Epoch: 460/500...  Training Step: 102051...  Training loss: 0.7211...  Val loss: 2.0547...  0.3634 sec/batch\n",
      "Epoch: 460/500...  Training Step: 102076...  Training loss: 0.7101...  Val loss: 2.0857...  0.3636 sec/batch\n",
      "Epoch: 460/500...  Training Step: 102101...  Training loss: 0.7278...  Val loss: 2.0701...  0.3810 sec/batch\n",
      "Epoch 460/500 time:82.3343493938446...  finished at 2017-10-30 20:05:19\n",
      "Epoch: 461/500...  Training Step: 102126...  Training loss: 0.7241...  Val loss: 2.0511...  0.3635 sec/batch\n",
      "Epoch: 461/500...  Training Step: 102151...  Training loss: 0.7074...  Val loss: 2.0514...  0.3634 sec/batch\n",
      "Epoch: 461/500...  Training Step: 102176...  Training loss: 0.7197...  Val loss: 2.0657...  0.3633 sec/batch\n",
      "Epoch: 461/500...  Training Step: 102201...  Training loss: 0.7092...  Val loss: 2.0809...  0.3811 sec/batch\n",
      "Epoch: 461/500...  Training Step: 102226...  Training loss: 0.7084...  Val loss: 2.0887...  0.3635 sec/batch\n",
      "Epoch: 461/500...  Training Step: 102251...  Training loss: 0.7158...  Val loss: 2.0515...  0.3630 sec/batch\n",
      "Epoch: 461/500...  Training Step: 102276...  Training loss: 0.7171...  Val loss: 2.0611...  0.3634 sec/batch\n",
      "Epoch: 461/500...  Training Step: 102301...  Training loss: 0.7115...  Val loss: 2.0726...  0.3671 sec/batch\n",
      "Epoch: 461/500...  Training Step: 102326...  Training loss: 0.7142...  Val loss: 2.0720...  0.3631 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102351...  Training loss: 0.7162...  Val loss: 2.0920...  0.3635 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102376...  Training loss: 0.7091...  Val loss: 2.0963...  0.3631 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102401...  Training loss: 0.7216...  Val loss: 2.0935...  0.3630 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102426...  Training loss: 0.7268...  Val loss: 2.0802...  0.3665 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102451...  Training loss: 0.7085...  Val loss: 2.0700...  0.3632 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102476...  Training loss: 0.6957...  Val loss: 2.0759...  0.3633 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102501...  Training loss: 0.7253...  Val loss: 2.0802...  0.3632 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102526...  Training loss: 0.7435...  Val loss: 2.0703...  0.3630 sec/batch\n",
      "Epoch: 462/500...  Training Step: 102551...  Training loss: 0.7144...  Val loss: 2.0737...  0.3630 sec/batch\n",
      "Epoch: 463/500...  Training Step: 102576...  Training loss: 0.7188...  Val loss: 2.0943...  0.3636 sec/batch\n",
      "Epoch: 463/500...  Training Step: 102601...  Training loss: 0.7105...  Val loss: 2.0622...  0.3634 sec/batch\n",
      "Epoch: 463/500...  Training Step: 102626...  Training loss: 0.7234...  Val loss: 2.0618...  0.3664 sec/batch\n",
      "Epoch: 463/500...  Training Step: 102651...  Training loss: 0.7218...  Val loss: 2.0691...  0.3635 sec/batch\n",
      "Epoch: 463/500...  Training Step: 102676...  Training loss: 0.7136...  Val loss: 2.0643...  0.3638 sec/batch\n",
      "Epoch: 463/500...  Training Step: 102701...  Training loss: 0.6938...  Val loss: 2.0930...  0.3635 sec/batch\n",
      "Epoch: 463/500...  Training Step: 102726...  Training loss: 0.7287...  Val loss: 2.0624...  0.3633 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 463/500...  Training Step: 102751...  Training loss: 0.7196...  Val loss: 2.0744...  0.3663 sec/batch\n",
      "Epoch: 463/500...  Training Step: 102776...  Training loss: 0.7133...  Val loss: 2.0731...  0.3632 sec/batch\n",
      "Epoch: 464/500...  Training Step: 102801...  Training loss: 0.7283...  Val loss: 2.0749...  0.3625 sec/batch\n",
      "Epoch: 464/500...  Training Step: 102826...  Training loss: 0.6997...  Val loss: 2.0759...  0.3637 sec/batch\n",
      "Epoch: 464/500...  Training Step: 102851...  Training loss: 0.7175...  Val loss: 2.0803...  0.3632 sec/batch\n",
      "Epoch: 464/500...  Training Step: 102876...  Training loss: 0.6847...  Val loss: 2.0900...  0.3665 sec/batch\n",
      "Epoch: 464/500...  Training Step: 102901...  Training loss: 0.7301...  Val loss: 2.0712...  0.3633 sec/batch\n",
      "Epoch: 464/500...  Training Step: 102926...  Training loss: 0.7127...  Val loss: 2.0785...  0.3632 sec/batch\n",
      "Epoch: 464/500...  Training Step: 102951...  Training loss: 0.7059...  Val loss: 2.0604...  0.3628 sec/batch\n",
      "Epoch: 464/500...  Training Step: 102976...  Training loss: 0.7347...  Val loss: 2.1044...  0.3635 sec/batch\n",
      "Epoch: 464/500...  Training Step: 103001...  Training loss: 0.7195...  Val loss: 2.0866...  0.3652 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103026...  Training loss: 0.7129...  Val loss: 2.0498...  0.3623 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103051...  Training loss: 0.6968...  Val loss: 2.0761...  0.3654 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103076...  Training loss: 0.7134...  Val loss: 2.0890...  0.3652 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103101...  Training loss: 0.7058...  Val loss: 2.0595...  0.3634 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103126...  Training loss: 0.7085...  Val loss: 2.0765...  0.3631 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103151...  Training loss: 0.7134...  Val loss: 2.0820...  0.3633 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103176...  Training loss: 0.7114...  Val loss: 2.0557...  0.3628 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103201...  Training loss: 0.7132...  Val loss: 2.0693...  0.3665 sec/batch\n",
      "Epoch: 465/500...  Training Step: 103226...  Training loss: 0.7265...  Val loss: 2.0623...  0.3630 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103251...  Training loss: 0.7185...  Val loss: 2.0751...  0.3637 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103276...  Training loss: 0.7008...  Val loss: 2.0729...  0.3634 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103301...  Training loss: 0.7178...  Val loss: 2.0864...  0.3632 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103326...  Training loss: 0.7207...  Val loss: 2.0720...  0.3653 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103351...  Training loss: 0.7096...  Val loss: 2.0715...  0.3635 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103376...  Training loss: 0.7144...  Val loss: 2.1051...  0.3633 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103401...  Training loss: 0.7441...  Val loss: 2.0841...  0.3634 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103426...  Training loss: 0.7146...  Val loss: 2.0525...  0.3633 sec/batch\n",
      "Epoch: 466/500...  Training Step: 103451...  Training loss: 0.7149...  Val loss: 2.0923...  0.3633 sec/batch\n",
      "Epoch: 467/500...  Training Step: 103476...  Training loss: 0.7163...  Val loss: 2.0787...  0.3653 sec/batch\n",
      "Epoch: 467/500...  Training Step: 103501...  Training loss: 0.7133...  Val loss: 2.0858...  0.3630 sec/batch\n",
      "Epoch: 467/500...  Training Step: 103526...  Training loss: 0.7080...  Val loss: 2.1087...  0.3631 sec/batch\n",
      "Epoch: 467/500...  Training Step: 103551...  Training loss: 0.7169...  Val loss: 2.0674...  0.3636 sec/batch\n",
      "Epoch: 467/500...  Training Step: 103576...  Training loss: 0.7062...  Val loss: 2.0857...  0.3656 sec/batch\n",
      "Epoch: 467/500...  Training Step: 103601...  Training loss: 0.7199...  Val loss: 2.0655...  0.3633 sec/batch\n",
      "Epoch: 467/500...  Training Step: 103626...  Training loss: 0.7223...  Val loss: 2.0616...  0.3635 sec/batch\n",
      "Epoch: 467/500...  Training Step: 103651...  Training loss: 0.7067...  Val loss: 2.0777...  0.3631 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103676...  Training loss: 0.7106...  Val loss: 2.0769...  0.3630 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103701...  Training loss: 0.7211...  Val loss: 2.0710...  0.3625 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103726...  Training loss: 0.7139...  Val loss: 2.0624...  0.3632 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103751...  Training loss: 0.7031...  Val loss: 2.1059...  0.3662 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103776...  Training loss: 0.7302...  Val loss: 2.0626...  0.3635 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103801...  Training loss: 0.7096...  Val loss: 2.0903...  0.3631 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103826...  Training loss: 0.7157...  Val loss: 2.0888...  0.3631 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103851...  Training loss: 0.7028...  Val loss: 2.0805...  0.3629 sec/batch\n",
      "Epoch: 468/500...  Training Step: 103876...  Training loss: 0.6885...  Val loss: 2.0627...  0.3664 sec/batch\n",
      "Epoch: 469/500...  Training Step: 103901...  Training loss: 0.7227...  Val loss: 2.0855...  0.3630 sec/batch\n",
      "Epoch: 469/500...  Training Step: 103926...  Training loss: 0.6860...  Val loss: 2.0760...  0.3636 sec/batch\n",
      "Epoch: 469/500...  Training Step: 103951...  Training loss: 0.6931...  Val loss: 2.0775...  0.3635 sec/batch\n",
      "Epoch: 469/500...  Training Step: 103976...  Training loss: 0.7046...  Val loss: 2.0950...  0.3631 sec/batch\n",
      "Epoch: 469/500...  Training Step: 104001...  Training loss: 0.7130...  Val loss: 2.0494...  0.3662 sec/batch\n",
      "Epoch: 469/500...  Training Step: 104026...  Training loss: 0.7069...  Val loss: 2.0794...  0.3637 sec/batch\n",
      "Epoch: 469/500...  Training Step: 104051...  Training loss: 0.7073...  Val loss: 2.0867...  0.3634 sec/batch\n",
      "Epoch: 469/500...  Training Step: 104076...  Training loss: 0.7177...  Val loss: 2.0766...  0.3642 sec/batch\n",
      "Epoch: 469/500...  Training Step: 104101...  Training loss: 0.7041...  Val loss: 2.0756...  0.3632 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104126...  Training loss: 0.7316...  Val loss: 2.0562...  0.3642 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104151...  Training loss: 0.7124...  Val loss: 2.0621...  0.3639 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104176...  Training loss: 0.7124...  Val loss: 2.1053...  0.3636 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104201...  Training loss: 0.7187...  Val loss: 2.0880...  0.3666 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104226...  Training loss: 0.7025...  Val loss: 2.0769...  0.3634 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104251...  Training loss: 0.7012...  Val loss: 2.0807...  0.3636 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104276...  Training loss: 0.7128...  Val loss: 2.0804...  0.3634 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104301...  Training loss: 0.7165...  Val loss: 2.0790...  0.3635 sec/batch\n",
      "Epoch: 470/500...  Training Step: 104326...  Training loss: 0.7212...  Val loss: 2.0823...  0.3659 sec/batch\n",
      "Epoch 470/500 time:81.71480560302734...  finished at 2017-10-30 20:18:58\n",
      "Epoch: 471/500...  Training Step: 104351...  Training loss: 0.7166...  Val loss: 2.0918...  0.3636 sec/batch\n",
      "Epoch: 471/500...  Training Step: 104376...  Training loss: 0.7097...  Val loss: 2.0711...  0.3637 sec/batch\n",
      "Epoch: 471/500...  Training Step: 104401...  Training loss: 0.6908...  Val loss: 2.0912...  0.3630 sec/batch\n",
      "Epoch: 471/500...  Training Step: 104426...  Training loss: 0.7114...  Val loss: 2.0685...  0.3632 sec/batch\n",
      "Epoch: 471/500...  Training Step: 104451...  Training loss: 0.7261...  Val loss: 2.0870...  0.3661 sec/batch\n",
      "Epoch: 471/500...  Training Step: 104476...  Training loss: 0.6976...  Val loss: 2.0731...  0.3630 sec/batch\n",
      "Epoch: 471/500...  Training Step: 104501...  Training loss: 0.7265...  Val loss: 2.0737...  0.3640 sec/batch\n",
      "Epoch: 471/500...  Training Step: 104526...  Training loss: 0.7320...  Val loss: 2.0924...  0.3649 sec/batch\n",
      "Epoch: 471/500...  Training Step: 104551...  Training loss: 0.7167...  Val loss: 2.0759...  0.3632 sec/batch\n",
      "Epoch: 472/500...  Training Step: 104576...  Training loss: 0.7029...  Val loss: 2.0986...  0.3637 sec/batch\n",
      "Epoch: 472/500...  Training Step: 104601...  Training loss: 0.6919...  Val loss: 2.0834...  0.3635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 472/500...  Training Step: 104626...  Training loss: 0.7189...  Val loss: 2.0704...  0.3634 sec/batch\n",
      "Epoch: 472/500...  Training Step: 104651...  Training loss: 0.7019...  Val loss: 2.0857...  0.3654 sec/batch\n",
      "Epoch: 472/500...  Training Step: 104676...  Training loss: 0.7033...  Val loss: 2.0985...  0.3634 sec/batch\n",
      "Epoch: 472/500...  Training Step: 104701...  Training loss: 0.7230...  Val loss: 2.0861...  0.3637 sec/batch\n",
      "Epoch: 472/500...  Training Step: 104726...  Training loss: 0.7020...  Val loss: 2.0896...  0.3637 sec/batch\n",
      "Epoch: 472/500...  Training Step: 104751...  Training loss: 0.7185...  Val loss: 2.0883...  0.3634 sec/batch\n",
      "Epoch: 472/500...  Training Step: 104776...  Training loss: 0.6964...  Val loss: 2.0768...  0.3664 sec/batch\n",
      "Epoch: 473/500...  Training Step: 104801...  Training loss: 0.7161...  Val loss: 2.0914...  0.3634 sec/batch\n",
      "Epoch: 473/500...  Training Step: 104826...  Training loss: 0.7195...  Val loss: 2.0762...  0.3643 sec/batch\n",
      "Epoch: 473/500...  Training Step: 104851...  Training loss: 0.6892...  Val loss: 2.1058...  0.3632 sec/batch\n",
      "Epoch: 473/500...  Training Step: 104876...  Training loss: 0.7057...  Val loss: 2.0818...  0.3637 sec/batch\n",
      "Epoch: 473/500...  Training Step: 104901...  Training loss: 0.7076...  Val loss: 2.0855...  0.3659 sec/batch\n",
      "Epoch: 473/500...  Training Step: 104926...  Training loss: 0.6976...  Val loss: 2.0886...  0.3639 sec/batch\n",
      "Epoch: 473/500...  Training Step: 104951...  Training loss: 0.6969...  Val loss: 2.0818...  0.3641 sec/batch\n",
      "Epoch: 473/500...  Training Step: 104976...  Training loss: 0.7274...  Val loss: 2.0792...  0.3634 sec/batch\n",
      "Epoch: 473/500...  Training Step: 105001...  Training loss: 0.6971...  Val loss: 2.0937...  0.3632 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105026...  Training loss: 0.7179...  Val loss: 2.0520...  0.3640 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105051...  Training loss: 0.6976...  Val loss: 2.0680...  0.3633 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105076...  Training loss: 0.7227...  Val loss: 2.0910...  0.3627 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105101...  Training loss: 0.7077...  Val loss: 2.0758...  0.3655 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105126...  Training loss: 0.6991...  Val loss: 2.0904...  0.3631 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105151...  Training loss: 0.7152...  Val loss: 2.0946...  0.3639 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105176...  Training loss: 0.6976...  Val loss: 2.0786...  0.3630 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105201...  Training loss: 0.7033...  Val loss: 2.0745...  0.3639 sec/batch\n",
      "Epoch: 474/500...  Training Step: 105226...  Training loss: 0.6865...  Val loss: 2.0804...  0.3659 sec/batch\n",
      "Epoch: 475/500...  Training Step: 105251...  Training loss: 0.7124...  Val loss: 2.0701...  0.3633 sec/batch\n",
      "Epoch: 475/500...  Training Step: 105276...  Training loss: 0.6957...  Val loss: 2.0991...  0.3640 sec/batch\n",
      "Epoch: 475/500...  Training Step: 105301...  Training loss: 0.6827...  Val loss: 2.0700...  0.3635 sec/batch\n",
      "Epoch: 475/500...  Training Step: 105326...  Training loss: 0.7059...  Val loss: 2.0580...  0.3633 sec/batch\n",
      "Epoch: 475/500...  Training Step: 105351...  Training loss: 0.7008...  Val loss: 2.0892...  0.3651 sec/batch\n",
      "Epoch: 475/500...  Training Step: 105376...  Training loss: 0.7037...  Val loss: 2.0883...  0.3631 sec/batch\n",
      "Epoch: 475/500...  Training Step: 105401...  Training loss: 0.7278...  Val loss: 2.0913...  0.3639 sec/batch\n",
      "Epoch: 475/500...  Training Step: 105426...  Training loss: 0.7183...  Val loss: 2.0716...  0.3638 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105451...  Training loss: 0.7834...  Val loss: 2.0912...  0.3629 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105476...  Training loss: 0.7260...  Val loss: 2.0903...  0.3653 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105501...  Training loss: 0.7008...  Val loss: 2.0786...  0.3633 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105526...  Training loss: 0.6917...  Val loss: 2.0934...  0.3634 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105551...  Training loss: 0.7166...  Val loss: 2.0889...  0.3640 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105576...  Training loss: 0.7022...  Val loss: 2.0791...  0.3632 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105601...  Training loss: 0.7012...  Val loss: 2.0999...  0.3641 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105626...  Training loss: 0.6843...  Val loss: 2.0982...  0.3632 sec/batch\n",
      "Epoch: 476/500...  Training Step: 105651...  Training loss: 0.7170...  Val loss: 2.0947...  0.3633 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105676...  Training loss: 0.7226...  Val loss: 2.0902...  0.3650 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105701...  Training loss: 0.6995...  Val loss: 2.0919...  0.3632 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105726...  Training loss: 0.6925...  Val loss: 2.0818...  0.3624 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105751...  Training loss: 0.6896...  Val loss: 2.1169...  0.3628 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105776...  Training loss: 0.7034...  Val loss: 2.0942...  0.3633 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105801...  Training loss: 0.7101...  Val loss: 2.0930...  0.3635 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105826...  Training loss: 0.7062...  Val loss: 2.0862...  0.3633 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105851...  Training loss: 0.7040...  Val loss: 2.0884...  0.3638 sec/batch\n",
      "Epoch: 477/500...  Training Step: 105876...  Training loss: 0.6991...  Val loss: 2.0786...  0.3634 sec/batch\n",
      "Epoch: 478/500...  Training Step: 105901...  Training loss: 0.7216...  Val loss: 2.0937...  0.3631 sec/batch\n",
      "Epoch: 478/500...  Training Step: 105926...  Training loss: 0.7254...  Val loss: 2.0708...  0.3633 sec/batch\n",
      "Epoch: 478/500...  Training Step: 105951...  Training loss: 0.6962...  Val loss: 2.0932...  0.3633 sec/batch\n",
      "Epoch: 478/500...  Training Step: 105976...  Training loss: 0.7011...  Val loss: 2.0883...  0.3647 sec/batch\n",
      "Epoch: 478/500...  Training Step: 106001...  Training loss: 0.6835...  Val loss: 2.0974...  0.3638 sec/batch\n",
      "Epoch: 478/500...  Training Step: 106026...  Training loss: 0.7256...  Val loss: 2.0992...  0.3632 sec/batch\n",
      "Epoch: 478/500...  Training Step: 106051...  Training loss: 0.7014...  Val loss: 2.0936...  0.3670 sec/batch\n",
      "Epoch: 478/500...  Training Step: 106076...  Training loss: 0.7183...  Val loss: 2.0951...  0.3631 sec/batch\n",
      "Epoch: 478/500...  Training Step: 106101...  Training loss: 0.7083...  Val loss: 2.0813...  0.3636 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106126...  Training loss: 0.7090...  Val loss: 2.0783...  0.3634 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106151...  Training loss: 0.7116...  Val loss: 2.0883...  0.3634 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106176...  Training loss: 0.6983...  Val loss: 2.0854...  0.3669 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106201...  Training loss: 0.7246...  Val loss: 2.0791...  0.3630 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106226...  Training loss: 0.7181...  Val loss: 2.0759...  0.3635 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106251...  Training loss: 0.7075...  Val loss: 2.0950...  0.3634 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106276...  Training loss: 0.7119...  Val loss: 2.0587...  0.3636 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106301...  Training loss: 0.7050...  Val loss: 2.1083...  0.3651 sec/batch\n",
      "Epoch: 479/500...  Training Step: 106326...  Training loss: 0.6893...  Val loss: 2.0796...  0.3637 sec/batch\n",
      "Epoch: 480/500...  Training Step: 106351...  Training loss: 0.7065...  Val loss: 2.0730...  0.3634 sec/batch\n",
      "Epoch: 480/500...  Training Step: 106376...  Training loss: 0.6896...  Val loss: 2.0920...  0.3634 sec/batch\n",
      "Epoch: 480/500...  Training Step: 106401...  Training loss: 0.7059...  Val loss: 2.0976...  0.3634 sec/batch\n",
      "Epoch: 480/500...  Training Step: 106426...  Training loss: 0.7019...  Val loss: 2.0853...  0.3656 sec/batch\n",
      "Epoch: 480/500...  Training Step: 106451...  Training loss: 0.6953...  Val loss: 2.0912...  0.3633 sec/batch\n",
      "Epoch: 480/500...  Training Step: 106476...  Training loss: 0.7044...  Val loss: 2.0951...  0.3632 sec/batch\n",
      "Epoch: 480/500...  Training Step: 106501...  Training loss: 0.7115...  Val loss: 2.0780...  0.3628 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480/500...  Training Step: 106526...  Training loss: 0.7120...  Val loss: 2.0855...  0.3632 sec/batch\n",
      "Epoch: 480/500...  Training Step: 106551...  Training loss: 0.7205...  Val loss: 2.0969...  0.3632 sec/batch\n",
      "Epoch 480/500 time:82.05109572410583...  finished at 2017-10-30 20:32:37\n",
      "Epoch: 481/500...  Training Step: 106576...  Training loss: 0.6880...  Val loss: 2.0791...  0.3631 sec/batch\n",
      "Epoch: 481/500...  Training Step: 106601...  Training loss: 0.6965...  Val loss: 2.0868...  0.3637 sec/batch\n",
      "Epoch: 481/500...  Training Step: 106626...  Training loss: 0.6910...  Val loss: 2.0839...  0.3662 sec/batch\n",
      "Epoch: 481/500...  Training Step: 106651...  Training loss: 0.6980...  Val loss: 2.0822...  0.3635 sec/batch\n",
      "Epoch: 481/500...  Training Step: 106676...  Training loss: 0.7087...  Val loss: 2.0850...  0.3636 sec/batch\n",
      "Epoch: 481/500...  Training Step: 106701...  Training loss: 0.6909...  Val loss: 2.0926...  0.3632 sec/batch\n",
      "Epoch: 481/500...  Training Step: 106726...  Training loss: 0.7010...  Val loss: 2.0832...  0.3633 sec/batch\n",
      "Epoch: 481/500...  Training Step: 106751...  Training loss: 0.7069...  Val loss: 2.0743...  0.3661 sec/batch\n",
      "Epoch: 481/500...  Training Step: 106776...  Training loss: 0.7107...  Val loss: 2.0826...  0.3633 sec/batch\n",
      "Epoch: 482/500...  Training Step: 106801...  Training loss: 0.7106...  Val loss: 2.0835...  0.3624 sec/batch\n",
      "Epoch: 482/500...  Training Step: 106826...  Training loss: 0.7057...  Val loss: 2.0793...  0.3643 sec/batch\n",
      "Epoch: 482/500...  Training Step: 106851...  Training loss: 0.7059...  Val loss: 2.1037...  0.3639 sec/batch\n",
      "Epoch: 482/500...  Training Step: 106876...  Training loss: 0.6941...  Val loss: 2.0838...  0.3655 sec/batch\n",
      "Epoch: 482/500...  Training Step: 106901...  Training loss: 0.7268...  Val loss: 2.1042...  0.3634 sec/batch\n",
      "Epoch: 482/500...  Training Step: 106926...  Training loss: 0.7097...  Val loss: 2.1128...  0.3631 sec/batch\n",
      "Epoch: 482/500...  Training Step: 106951...  Training loss: 0.6992...  Val loss: 2.0858...  0.3630 sec/batch\n",
      "Epoch: 482/500...  Training Step: 106976...  Training loss: 0.7154...  Val loss: 2.0897...  0.3631 sec/batch\n",
      "Epoch: 482/500...  Training Step: 107001...  Training loss: 0.7118...  Val loss: 2.0921...  0.3663 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107026...  Training loss: 0.6805...  Val loss: 2.0868...  0.3635 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107051...  Training loss: 0.7006...  Val loss: 2.0907...  0.3638 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107076...  Training loss: 0.7064...  Val loss: 2.1062...  0.3657 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107101...  Training loss: 0.6870...  Val loss: 2.0823...  0.3633 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107126...  Training loss: 0.7005...  Val loss: 2.0769...  0.3631 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107151...  Training loss: 0.7161...  Val loss: 2.0679...  0.3633 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107176...  Training loss: 0.6864...  Val loss: 2.0808...  0.3632 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107201...  Training loss: 0.7117...  Val loss: 2.0909...  0.3656 sec/batch\n",
      "Epoch: 483/500...  Training Step: 107226...  Training loss: 0.7080...  Val loss: 2.0876...  0.3643 sec/batch\n",
      "Epoch: 484/500...  Training Step: 107251...  Training loss: 0.6866...  Val loss: 2.0737...  0.3635 sec/batch\n",
      "Epoch: 484/500...  Training Step: 107276...  Training loss: 0.7048...  Val loss: 2.0974...  0.3630 sec/batch\n",
      "Epoch: 484/500...  Training Step: 107301...  Training loss: 0.7212...  Val loss: 2.0914...  0.3634 sec/batch\n",
      "Epoch: 484/500...  Training Step: 107326...  Training loss: 0.6996...  Val loss: 2.0811...  0.3655 sec/batch\n",
      "Epoch: 484/500...  Training Step: 107351...  Training loss: 0.7014...  Val loss: 2.1208...  0.3633 sec/batch\n",
      "Epoch: 484/500...  Training Step: 107376...  Training loss: 0.7096...  Val loss: 2.0981...  0.3635 sec/batch\n",
      "Epoch: 484/500...  Training Step: 107401...  Training loss: 0.6963...  Val loss: 2.0702...  0.3635 sec/batch\n",
      "Epoch: 484/500...  Training Step: 107426...  Training loss: 0.7051...  Val loss: 2.1096...  0.3634 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107451...  Training loss: 0.7129...  Val loss: 2.0774...  0.3665 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107476...  Training loss: 0.6816...  Val loss: 2.0797...  0.3632 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107501...  Training loss: 0.6808...  Val loss: 2.0703...  0.3634 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107526...  Training loss: 0.6859...  Val loss: 2.1164...  0.3659 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107551...  Training loss: 0.6858...  Val loss: 2.0813...  0.3634 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107576...  Training loss: 0.7020...  Val loss: 2.0934...  0.3637 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107601...  Training loss: 0.7036...  Val loss: 2.0878...  0.3632 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107626...  Training loss: 0.6994...  Val loss: 2.1107...  0.3636 sec/batch\n",
      "Epoch: 485/500...  Training Step: 107651...  Training loss: 0.7155...  Val loss: 2.1019...  0.3660 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107676...  Training loss: 0.7121...  Val loss: 2.0779...  0.3634 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107701...  Training loss: 0.6937...  Val loss: 2.0807...  0.3634 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107726...  Training loss: 0.7019...  Val loss: 2.0924...  0.3632 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107751...  Training loss: 0.6921...  Val loss: 2.1046...  0.3633 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107776...  Training loss: 0.6953...  Val loss: 2.1093...  0.3665 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107801...  Training loss: 0.6895...  Val loss: 2.0885...  0.3633 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107826...  Training loss: 0.6963...  Val loss: 2.0838...  0.3643 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107851...  Training loss: 0.7076...  Val loss: 2.0863...  0.3636 sec/batch\n",
      "Epoch: 486/500...  Training Step: 107876...  Training loss: 0.7041...  Val loss: 2.1039...  0.3631 sec/batch\n",
      "Epoch: 487/500...  Training Step: 107901...  Training loss: 0.7085...  Val loss: 2.1076...  0.3659 sec/batch\n",
      "Epoch: 487/500...  Training Step: 107926...  Training loss: 0.6897...  Val loss: 2.1016...  0.3630 sec/batch\n",
      "Epoch: 487/500...  Training Step: 107951...  Training loss: 0.7082...  Val loss: 2.1129...  0.3641 sec/batch\n",
      "Epoch: 487/500...  Training Step: 107976...  Training loss: 0.6984...  Val loss: 2.1025...  0.3632 sec/batch\n",
      "Epoch: 487/500...  Training Step: 108001...  Training loss: 0.6932...  Val loss: 2.0863...  0.3638 sec/batch\n",
      "Epoch: 487/500...  Training Step: 108026...  Training loss: 0.6850...  Val loss: 2.1043...  0.3638 sec/batch\n",
      "Epoch: 487/500...  Training Step: 108051...  Training loss: 0.7122...  Val loss: 2.1007...  0.3633 sec/batch\n",
      "Epoch: 487/500...  Training Step: 108076...  Training loss: 0.7146...  Val loss: 2.0975...  0.3634 sec/batch\n",
      "Epoch: 487/500...  Training Step: 108101...  Training loss: 0.6970...  Val loss: 2.0987...  0.3664 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108126...  Training loss: 0.7061...  Val loss: 2.1175...  0.3635 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108151...  Training loss: 0.6940...  Val loss: 2.0881...  0.3636 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108176...  Training loss: 0.6967...  Val loss: 2.0842...  0.3631 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108201...  Training loss: 0.7017...  Val loss: 2.1042...  0.3632 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108226...  Training loss: 0.7042...  Val loss: 2.0914...  0.3659 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108251...  Training loss: 0.6867...  Val loss: 2.0954...  0.3632 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108276...  Training loss: 0.7192...  Val loss: 2.1005...  0.3640 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108301...  Training loss: 0.7100...  Val loss: 2.1082...  0.3635 sec/batch\n",
      "Epoch: 488/500...  Training Step: 108326...  Training loss: 0.6969...  Val loss: 2.0915...  0.3635 sec/batch\n",
      "Epoch: 489/500...  Training Step: 108351...  Training loss: 0.7101...  Val loss: 2.0788...  0.3652 sec/batch\n",
      "Epoch: 489/500...  Training Step: 108376...  Training loss: 0.6778...  Val loss: 2.1049...  0.3631 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 489/500...  Training Step: 108401...  Training loss: 0.7023...  Val loss: 2.1115...  0.3640 sec/batch\n",
      "Epoch: 489/500...  Training Step: 108426...  Training loss: 0.6756...  Val loss: 2.1161...  0.3638 sec/batch\n",
      "Epoch: 489/500...  Training Step: 108451...  Training loss: 0.7067...  Val loss: 2.1100...  0.3631 sec/batch\n",
      "Epoch: 489/500...  Training Step: 108476...  Training loss: 0.7000...  Val loss: 2.0871...  0.3662 sec/batch\n",
      "Epoch: 489/500...  Training Step: 108501...  Training loss: 0.6847...  Val loss: 2.0821...  0.3633 sec/batch\n",
      "Epoch: 489/500...  Training Step: 108526...  Training loss: 0.7199...  Val loss: 2.1256...  0.3623 sec/batch\n",
      "Epoch: 489/500...  Training Step: 108551...  Training loss: 0.7084...  Val loss: 2.1042...  0.3638 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108576...  Training loss: 0.7027...  Val loss: 2.0812...  0.3666 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108601...  Training loss: 0.7025...  Val loss: 2.0962...  0.3640 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108626...  Training loss: 0.7151...  Val loss: 2.1026...  0.3634 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108651...  Training loss: 0.6941...  Val loss: 2.0704...  0.3636 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108676...  Training loss: 0.7111...  Val loss: 2.0971...  0.3658 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108701...  Training loss: 0.6998...  Val loss: 2.1006...  0.3631 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108726...  Training loss: 0.7058...  Val loss: 2.0899...  0.3638 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108751...  Training loss: 0.7021...  Val loss: 2.1087...  0.3640 sec/batch\n",
      "Epoch: 490/500...  Training Step: 108776...  Training loss: 0.7065...  Val loss: 2.0851...  0.3636 sec/batch\n",
      "Epoch 490/500 time:81.7350218296051...  finished at 2017-10-30 20:46:15\n",
      "Epoch: 491/500...  Training Step: 108801...  Training loss: 0.6991...  Val loss: 2.1046...  0.3645 sec/batch\n",
      "Epoch: 491/500...  Training Step: 108826...  Training loss: 0.6898...  Val loss: 2.0929...  0.3638 sec/batch\n",
      "Epoch: 491/500...  Training Step: 108851...  Training loss: 0.7046...  Val loss: 2.1049...  0.3638 sec/batch\n",
      "Epoch: 491/500...  Training Step: 108876...  Training loss: 0.7003...  Val loss: 2.0938...  0.3631 sec/batch\n",
      "Epoch: 491/500...  Training Step: 108901...  Training loss: 0.6855...  Val loss: 2.1031...  0.3643 sec/batch\n",
      "Epoch: 491/500...  Training Step: 108926...  Training loss: 0.6990...  Val loss: 2.1289...  0.3652 sec/batch\n",
      "Epoch: 491/500...  Training Step: 108951...  Training loss: 0.7297...  Val loss: 2.1010...  0.3642 sec/batch\n",
      "Epoch: 491/500...  Training Step: 108976...  Training loss: 0.6977...  Val loss: 2.0697...  0.3637 sec/batch\n",
      "Epoch: 491/500...  Training Step: 109001...  Training loss: 0.6996...  Val loss: 2.1031...  0.3637 sec/batch\n",
      "Epoch: 492/500...  Training Step: 109026...  Training loss: 0.7048...  Val loss: 2.1099...  0.3634 sec/batch\n",
      "Epoch: 492/500...  Training Step: 109051...  Training loss: 0.6948...  Val loss: 2.1055...  0.3641 sec/batch\n",
      "Epoch: 492/500...  Training Step: 109076...  Training loss: 0.6858...  Val loss: 2.1127...  0.3642 sec/batch\n",
      "Epoch: 492/500...  Training Step: 109101...  Training loss: 0.7001...  Val loss: 2.0925...  0.3623 sec/batch\n",
      "Epoch: 492/500...  Training Step: 109126...  Training loss: 0.6827...  Val loss: 2.0998...  0.3658 sec/batch\n",
      "Epoch: 492/500...  Training Step: 109151...  Training loss: 0.6936...  Val loss: 2.1006...  0.3630 sec/batch\n",
      "Epoch: 492/500...  Training Step: 109176...  Training loss: 0.7046...  Val loss: 2.0901...  0.3636 sec/batch\n",
      "Epoch: 492/500...  Training Step: 109201...  Training loss: 0.7026...  Val loss: 2.1050...  0.3631 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109226...  Training loss: 0.6940...  Val loss: 2.0842...  0.3643 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109251...  Training loss: 0.6979...  Val loss: 2.0949...  0.3654 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109276...  Training loss: 0.6974...  Val loss: 2.0837...  0.3634 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109301...  Training loss: 0.6895...  Val loss: 2.1318...  0.3640 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109326...  Training loss: 0.7086...  Val loss: 2.0837...  0.3635 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109351...  Training loss: 0.6988...  Val loss: 2.1131...  0.3620 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109376...  Training loss: 0.7069...  Val loss: 2.1142...  0.3651 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109401...  Training loss: 0.6951...  Val loss: 2.0897...  0.3634 sec/batch\n",
      "Epoch: 493/500...  Training Step: 109426...  Training loss: 0.6893...  Val loss: 2.0784...  0.3635 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109451...  Training loss: 0.7072...  Val loss: 2.1109...  0.3638 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109476...  Training loss: 0.6922...  Val loss: 2.1002...  0.3633 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109501...  Training loss: 0.6782...  Val loss: 2.0882...  0.3663 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109526...  Training loss: 0.6974...  Val loss: 2.1118...  0.3631 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109551...  Training loss: 0.7062...  Val loss: 2.0775...  0.3640 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109576...  Training loss: 0.6927...  Val loss: 2.1105...  0.3631 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109601...  Training loss: 0.6861...  Val loss: 2.1053...  0.3637 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109626...  Training loss: 0.7016...  Val loss: 2.1066...  0.3644 sec/batch\n",
      "Epoch: 494/500...  Training Step: 109651...  Training loss: 0.7049...  Val loss: 2.0861...  0.3636 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109676...  Training loss: 0.7094...  Val loss: 2.0880...  0.3633 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109701...  Training loss: 0.6958...  Val loss: 2.0797...  0.3639 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109726...  Training loss: 0.6911...  Val loss: 2.1261...  0.3631 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109751...  Training loss: 0.6816...  Val loss: 2.1148...  0.3648 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109776...  Training loss: 0.6974...  Val loss: 2.1093...  0.3633 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109801...  Training loss: 0.7029...  Val loss: 2.1161...  0.3634 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109826...  Training loss: 0.6980...  Val loss: 2.0910...  0.3634 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109851...  Training loss: 0.7037...  Val loss: 2.1016...  0.3632 sec/batch\n",
      "Epoch: 495/500...  Training Step: 109876...  Training loss: 0.7064...  Val loss: 2.0916...  0.3641 sec/batch\n",
      "Epoch: 496/500...  Training Step: 109901...  Training loss: 0.6988...  Val loss: 2.1267...  0.3632 sec/batch\n",
      "Epoch: 496/500...  Training Step: 109926...  Training loss: 0.6913...  Val loss: 2.0887...  0.3633 sec/batch\n",
      "Epoch: 496/500...  Training Step: 109951...  Training loss: 0.6969...  Val loss: 2.1027...  0.3630 sec/batch\n",
      "Epoch: 496/500...  Training Step: 109976...  Training loss: 0.6978...  Val loss: 2.1119...  0.3634 sec/batch\n",
      "Epoch: 496/500...  Training Step: 110001...  Training loss: 0.7202...  Val loss: 2.1105...  0.3650 sec/batch\n",
      "Epoch: 496/500...  Training Step: 110026...  Training loss: 0.6946...  Val loss: 2.0911...  0.3633 sec/batch\n",
      "Epoch: 496/500...  Training Step: 110051...  Training loss: 0.7027...  Val loss: 2.0947...  0.3636 sec/batch\n",
      "Epoch: 496/500...  Training Step: 110076...  Training loss: 0.7151...  Val loss: 2.1101...  0.3662 sec/batch\n",
      "Epoch: 496/500...  Training Step: 110101...  Training loss: 0.7026...  Val loss: 2.0990...  0.3638 sec/batch\n",
      "Epoch: 497/500...  Training Step: 110126...  Training loss: 0.6902...  Val loss: 2.1155...  0.3636 sec/batch\n",
      "Epoch: 497/500...  Training Step: 110151...  Training loss: 0.6862...  Val loss: 2.1129...  0.3637 sec/batch\n",
      "Epoch: 497/500...  Training Step: 110176...  Training loss: 0.6923...  Val loss: 2.0883...  0.3639 sec/batch\n",
      "Epoch: 497/500...  Training Step: 110201...  Training loss: 0.6987...  Val loss: 2.1052...  0.3664 sec/batch\n",
      "Epoch: 497/500...  Training Step: 110226...  Training loss: 0.6846...  Val loss: 2.1139...  0.3636 sec/batch\n",
      "Epoch: 497/500...  Training Step: 110251...  Training loss: 0.7103...  Val loss: 2.1095...  0.3632 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 497/500...  Training Step: 110276...  Training loss: 0.6943...  Val loss: 2.1126...  0.3633 sec/batch\n",
      "Epoch: 497/500...  Training Step: 110301...  Training loss: 0.7062...  Val loss: 2.1115...  0.3632 sec/batch\n",
      "Epoch: 497/500...  Training Step: 110326...  Training loss: 0.6916...  Val loss: 2.1025...  0.3665 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110351...  Training loss: 0.6993...  Val loss: 2.1017...  0.3634 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110376...  Training loss: 0.7012...  Val loss: 2.0951...  0.3633 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110401...  Training loss: 0.6849...  Val loss: 2.1225...  0.3634 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110426...  Training loss: 0.6871...  Val loss: 2.0975...  0.3634 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110451...  Training loss: 0.6848...  Val loss: 2.1071...  0.3656 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110476...  Training loss: 0.6836...  Val loss: 2.1044...  0.3629 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110501...  Training loss: 0.6821...  Val loss: 2.0899...  0.3632 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110526...  Training loss: 0.7071...  Val loss: 2.0955...  0.3675 sec/batch\n",
      "Epoch: 498/500...  Training Step: 110551...  Training loss: 0.6878...  Val loss: 2.1032...  0.3635 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110576...  Training loss: 0.7032...  Val loss: 2.0738...  0.3635 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110601...  Training loss: 0.6872...  Val loss: 2.0934...  0.3636 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110626...  Training loss: 0.6903...  Val loss: 2.1154...  0.3635 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110651...  Training loss: 0.6946...  Val loss: 2.0863...  0.3671 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110676...  Training loss: 0.6881...  Val loss: 2.1082...  0.3633 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110701...  Training loss: 0.6960...  Val loss: 2.1125...  0.3632 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110726...  Training loss: 0.6958...  Val loss: 2.1085...  0.3633 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110751...  Training loss: 0.6891...  Val loss: 2.0986...  0.3631 sec/batch\n",
      "Epoch: 499/500...  Training Step: 110776...  Training loss: 0.6848...  Val loss: 2.1048...  0.3663 sec/batch\n",
      "Epoch: 500/500...  Training Step: 110801...  Training loss: 0.6931...  Val loss: 2.1013...  0.3634 sec/batch\n",
      "Epoch: 500/500...  Training Step: 110826...  Training loss: 0.6803...  Val loss: 2.1171...  0.3634 sec/batch\n",
      "Epoch: 500/500...  Training Step: 110851...  Training loss: 0.6900...  Val loss: 2.0999...  0.3640 sec/batch\n",
      "Epoch: 500/500...  Training Step: 110876...  Training loss: 0.6839...  Val loss: 2.0710...  0.3640 sec/batch\n",
      "Epoch: 500/500...  Training Step: 110901...  Training loss: 0.6852...  Val loss: 2.1231...  0.3671 sec/batch\n",
      "Epoch: 500/500...  Training Step: 110926...  Training loss: 0.7013...  Val loss: 2.1182...  0.3631 sec/batch\n",
      "Epoch: 500/500...  Training Step: 110951...  Training loss: 0.7013...  Val loss: 2.1083...  0.3632 sec/batch\n",
      "Epoch: 500/500...  Training Step: 110976...  Training loss: 0.7047...  Val loss: 2.0974...  0.3632 sec/batch\n",
      "Epoch 500/500 time:81.60189414024353...  finished at 2017-10-30 20:59:54\n",
      "Training ending at time: 2017-10-30 20:59:54\n",
      "Trainint total time: 40902.27607870102\n"
     ]
    }
   ],
   "source": [
    "#epochs = 1\n",
    "# Save every N iterations\n",
    "save_every_n = 500\n",
    "print_loss_every_n = 25\n",
    "sample_every = 500\n",
    "print_epoch_time_every = 10\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "    #print(\"after model\")\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "print(\"Training starting at time:\",strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "train_start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Number of parameters:\",get_number_of_parameters(),\"Dataset size:\",len(encoded))\n",
    "    #print(\"after initializer\")\n",
    "    if resume_from_checkpoint:\n",
    "        latest_checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            \n",
    "            \n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "        \n",
    "            \n",
    "            end = time.time()\n",
    "            if counter%print_loss_every_n == 0:\n",
    "                val_batches = get_batches(encoded_val,int(len(encoded_val)/num_steps),num_steps)\n",
    "                x_val,y_val = next(val_batches)\n",
    "                \n",
    "                val_dict = {model.inputs: x_val,\n",
    "                            model.targets: y_val,\n",
    "                            model.keep_prob: 1,\n",
    "                            model.initial_state: new_state}\n",
    "                \n",
    "                val_loss,prediction = sess.run([model.loss,model.prediction],feed_dict=val_dict)\n",
    "                \n",
    "                losses[\"train\"].append(batch_loss)\n",
    "                losses[\"validation\"].append(val_loss)\n",
    "                \n",
    "                \n",
    "                global_step = tf.train.global_step(sess,model.global_step_tensor)\n",
    "                x_steps.append(global_step)\n",
    "                \n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(global_step),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      'Val loss: {:.4f}... '.format(val_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                global_step = tf.train.global_step(sess,model.global_step_tensor)\n",
    "                saver.save(sess, \"checkpoints/m{}_i{}_l{}.ckpt\".format(mode,global_step, lstm_size))\n",
    "                \n",
    "            counter += 1\n",
    "            #learning_rate*=0.75\n",
    "            #model.optimizer = build_optimizer(model.loss,learning_rate,model.grad_clip,model.global_step_tensor)\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        \n",
    "        \n",
    "        if ((e+1) % print_epoch_time_every== 0):\n",
    "            print('Epoch {}/{} time:{}...'.format(e+1,epochs,epoch_end-epoch_start),\n",
    "                 \" finished at\",strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "        \n",
    "            \n",
    "    global_step = tf.train.global_step(sess,model.global_step_tensor)\n",
    "    saver.save(sess, \"checkpoints/m{}_i{}_l{}.ckpt\".format(mode,global_step, lstm_size))\n",
    "    \n",
    "print(\"Training ending at time:\",strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "print(\"Trainint total time:\",time.time()-train_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAusAAAH0CAYAAACEkWPuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5c9ca46b00>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 373
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_steps,losses[\"train\"],label=\"Train loss\")\n",
    "plt.plot(x_steps,losses[\"validation\"],label=\"Validation loss\")\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoints = tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling final trained model\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \",mode=\"characters\"):\n",
    "    print(mode)\n",
    "    samples = tokenize_text(prime,mode)\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in tokenize_text(prime,mode):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples).replace(\"new_line_token\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/mcharacters_i111000_l768.ckpt'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new text from \"base\" text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "\n",
    "\n",
    "samples = list()\n",
    "for text in text_to_try:\n",
    "    #print(\"------------------------\",text)\n",
    "    samples.append( sample(checkpoint, 500, lstm_size, len(vocab), prime=text,mode=mode))\n",
    "    #print(samp)\n",
    "    #print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ In the first place\n",
      "In the first place and its monstrous horror. With the entrance to the right of his body and little voices, but wide subterrene hypnotic work; for the abyermant ishees they were makes of the transforting earth, and the marvellous sunset city we saw in a shocking destruction for the first time. He commenced a species of southward; howling, drinking wholesome ain the hole and the street seemed to get a wide pan of board. No should have been, and word, one, whils I could sing the rest of the crucifivents for the salve\n",
      "------------------------\n",
      "------------------------ the night before\n",
      "the night before, we must have concluded that he could discover any consciousness or other progress when smiles of antiquity.\n",
      "     One of the marsher asdenied he did not like what was still amidst the chief sented of some spirit-doms of his prison, he did not seem to see the alfiture thing may be telling to the public.\n",
      "\n",
      "    \"The spot 'sen for some means having been falling from the tipadear horror and a malfares. It was not so large a partion. About terrifieders and the distangened open which I thought of my sce\n",
      "------------------------\n",
      "------------------------ horror\n",
      "horror.\n",
      "\n",
      "That was the conversation with his feelings in ter own whister, and from the top of his door, and I was fell in a visit. In the morn suffered things this direction I had expected. And as I have faired to look even more nor longer in my mind. The sight of this species dead noted, and that with extreme superptititation, fell anown, and thus ifformed the blamp of the archway. And now the smellal of the alchamyis, who he now told me that the hosrifaccularest construction become madied in what seem\n",
      "------------------------\n",
      "------------------------ creature\n",
      "creature and as I could, the leading places of mind defented. In the thread more down the slope. Tilly feeling all our shadows, and he was not the revelation at the stones and asked him with a mind and intense stroke of its nature at this planet. In that stone rough, for the time of his wife, if no sees to be sure, and louder as he closed to him and told him thereffle and only furrived and bored another; furring along from hamfy the gigantic place on the other hings, and they dreaded to a short singular \n",
      "------------------------\n",
      "------------------------ night\n",
      "night. After a time I were the furl in his clothing, and we should ever take unable to say, and declared it had breached at the company a set of his fears in death and innec phonograph rocky. A sensed fantic lone was plainly changed in the caverns. When the dogns was various in every, and once of the file life fully terraced the towm, and had left them, although the rest residence or to have been a phronake courd in the north at every deers.\n",
      "     The mutineed marked an extrumated party, and he climbed\n",
      "------------------------\n",
      "------------------------ dream\n",
      "dream. Most of these I have said to me,  sometis sure that I might have been actually reached by those less indefent cases on that time. It was a man of consciousness and darkness; by merchant which no chisellous creat he knew of his short--he was not all likely to do with the detectives of their character. While their origin with the gages reinn on the stone blow, the wind faded among whose prints in that high furry region of human antennaces and confounded but metallis like of the corner of which th\n",
      "------------------------\n",
      "------------------------ thing\n",
      "thing, though, so much asought monstrously. The drug of the beauty as called, and full of midity, inside the patched bottom of the party in the minds of the med line. I should help, so that the back of the creatures was all thrown a great peak. This was to weak alive, and he could not possibly she made him frightened a heavy body or patient when they was lost in this horror at the high, the number and dust was visible, a different ferrow forest all the heavy scatteres newd to do with the galages benea\n",
      "------------------------\n",
      "------------------------ That night\n",
      "That night the monstrous gool story on the track of the shapt roof would sometimes rul at the bungalos of itmast monstrous proportions. Of the former projecting features which made the steep restaration of the gossip in which the surface survived himself in his laboratory. He close to me that he had come to read is, and I did not wake pocket carely realities to the surprise I could see them. He had before never discovered the camp and stated in the wall; I constituted teeling to my companions. It related, \n",
      "------------------------\n",
      "------------------------ mountain\n",
      "mountains the stone circles of the cut of the city which I had learned. The building to after the hideous something land seemed more racinly real, I could not restrain it.\n",
      "\n",
      "    Upon the table, I remained, and more than to the ordinary objects in the community. What weaken all aloud, however, we all cryishly strong attention, the dreams was a going of unknown points at whome even worse than the fact that he would not be doubtsed for his pieces when he did not escape. All the features were later but excite\n",
      "------------------------\n",
      "------------------------ Ammi\n",
      "Ammin who herris that most of the worst is made in case.\n",
      "\n",
      "There was as his convulged duranger the considerable minutes and water was terming; though he loubled at the familiar towers that he had finased clumsily to the tones of the cutting and incancy with the general fashion. When the presence of cause and infinitely moment alien than any afternoon, with its grass headlands, and in contanie could be destroyed, and threw of a single middle of dark stone, and an age and dark studue outlined its thutch\n",
      "------------------------\n",
      "------------------------ Cthulhu\n",
      "Cthulhu forced to the ground. When I drew into the hall and agreed that a father would soon be done by the same extraordinary family. This, I was appalled by the chair at the high business, and to find his prints and almost involugably its spirit of me. There was a prince to be entered that of companions whose house was the common college. Trouble, those faithful approachier mountains was soil of what he hidress. This came from anchor to the things were subject. At the time he had glowed fragments of a \n",
      "------------------------\n",
      "------------------------ raven\n",
      "raven, and at length there remained in a contents of elephantia means of grotesque whispers or dissolution.\n",
      "     On the there were a speciming and its searchers, the summit was stated—a telling whom he decided to the rution. In some meating was it on our limits and provided all events which no every particular course were violent removed and stepped out on the back roof. It was not the morning for this thing; and, at the state of the cosmic action so long ly defent that the superstition of my flight a\n",
      "------------------------\n",
      "------------------------ bird\n",
      "bird in the leets; and in their dreams it had so greatly almost its success of mind, there were signs of a consciousness of life and sombinistic thus information of the absted foreigners. These marked which I had were seen, but it was only a boar see. It was never a sound, that this fellow is a ghast in the sun. Slagghing above the being afrord no drive it appears to horror from a curious perilh- but a window and tremendously relief at the three glow and drew it at least though the castle was still t\n",
      "------------------------\n",
      "------------------------ nevermore\n",
      "nevermore.\n",
      " The temperature of this time and then in a general reason from which I could have no doubt. In the end I speak the local muse far years before; when I was glad that the whirl of which was to strike what had happined in the community as to the distance was each on the sound. It must be lodger—there was a second Incance when the torch stood alone were starting. There was nothing now and then that arise in the chamber. Those accident was great, several warms spirit except for the recent room with\n",
      "------------------------\n",
      "------------------------ dead\n",
      "dead—the Old Ones age, was buried in medical specimens as stoop. The house was a highly belief in the mouth of his hint. It was the factuitain of any desire tell of the portion of the study, and what could ba believed of the surface of the exception. The doctor advanced too much for this inchineed so to lain any pursuit of the town? Who dad, had I seen a cattle with this dread country, and ware heard for her chasm those cats had evidently caused it for him. When Carter saw that he could not be devent\n",
      "------------------------\n",
      "------------------------ The bird\n",
      "The bird to the court of his constitution.\n",
      "     Fread occhuring to examine that the creatures had been terrible and exposinely closed. At length, if emerged to the musical sight of the reveller way of spote of a patient with a sharp typical spot where the same rattline was still unclistening. The strickening of the accursed slate began to move fish or seize and talked with their hair was heard of some silent. Things were pointed out, and they fleet through the hudden loon, almost against a small balonoed\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text_to_try)):\n",
    "    text = text_to_try[i]\n",
    "    generated = samples[i]\n",
    "    print(\"------------------------\",text)\n",
    "    print(generated)\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/mcharacters_i111000_l768.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mcharacters_i109001_l768.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mcharacters_i109501_l768.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mcharacters_i110001_l768.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mcharacters_i110501_l768.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/mcharacters_i111000_l768.ckpt\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "The old folk have gone away. There was a monstrous ruboid cold comment on my strating amountwound. It seemed toler and got as a mask of shoggoths on the dreamear to which I can speak the slope with a simple sining ring forward. Only the nethermost realm or cargo of the black side was dog that it said. It must be the reasonacty connection to be seen such places as he came. Willett was considered as much abandoned, but succeeded only at length and plunged as if in case of gillars. It was heard of no place, and to their progress on the ince there seemed to have been sure belo. He was at last beyond that pass of the beat, on the the gigantic librres of the Colony-Fungo Hopurias, and surfaced with prominence, and although I could not perceive your triphest shape and think the pre-cumbling crots not bring melling into his sanity on account; for there was an until human being—dectioning her without the most structure Indied in the city and through the hideous full voices of a man from some unknown and unhearthoun itself whose rumours and world had sounded the hear the wholly southern commenced to about the frenzied place. He was not made compite to the search in the Roureglid, and ran out of Hirnetless as the my oet was mankind; and how the final has grown as each and a closer producing tell me excitedlying. Only with the pass were the men of his face; you had heard from its effects mast before I could scarcely see what the sleepers was seen to ba place; but I was drunk full of my stil and brood above me in a large corner common toward thtip could not be described. It was, indeed, probably swathed around in common sense; till I was started, they could not pelm t  his thing so madine as possible with the strange existence of my imagination. It was the memories of existence in the things and manners of a man. At least of the cunting and almost apparent capacity of the dark study. Of that this world he said that the doctor was disappointed for him. He said that the bristering could have been so far, and in the end of the thing which could be so exercially the last description. And we could not have formed no with portion of its staring, and was no patient concerning the steep grey twilight sheeps of the ruins of Doctor Paris, which mess compieced the man be acad ferecade.\n",
      "     Willett long an inculae he turned past two lumbs and curved without peaks of the glen whose turches, almost the graveyards of a foreigner, and that this strange musical words were excited by the more deliberation which the fosting-pointed proper parrom human becoms to harm shim.\n",
      "     When the gerrer came of the hellish sidewalk we shunned the hours of his father,\n",
      "sinch hand he had sought with home and his pittings with story in the hellish and distant above, an open chasm, bar-amaily induced; and when we came for our speech and a fears of perpendicular depth, as if in light measure, the further and singular city which told of its still western indignations and champhant protection was full inspecting dys a highway before the youth's madness as it was. An one was the save upper soathares of the Gardte of the Other Gods, and ever seemed to be the sunset felocy on the wideless still thing had trult ever longer. There are controp to the earth’s dissolved tonce, and sort of conversation was somethiigh think, but with his shadow of ancient sensations; and without any direction to ancient parts of lost topesting damp, and still strangery and guist. They have been borne or high an incant time and shalp no raising for the search for cosmic and matter that such an entire message was in a present bearded spot on the steps and loathsome aid to take the matter. As if to accured to out, and anywhere else were uttailed at heavy snow deviving near highs and claw of the great crowding and mising period. There are not on the linkn is a state of the mist which he had once over. The sleep had set in the walks, and care of ramport frighten disclossihings in our species, three other moulas in singular archaic mysterious, and the low above the picture would be to be poilt. I crossed my progress as it something to my mind. Then much of the meditations had had some true grasp, and said had gave us disappearances. If I desertsed them to begen with a foot in delirium, and I knew that the voyages was madify and the worst of the feeble climable corridor experienced what I might find his mind enamination. There came a least from some peculiar stoup mate; and if the contagious of the places had talked to reach the centre of the reach, and the wind itself had forced his farm in take and carged across the road to the sky; stricched from ighory traver on the window he could not go by my incident animals and evidence that my uncle had prevented upon him, when I studied the more spireth Inlismed accents of the college; but its seemed try and a faint and about the sun. And there is no angles of marking, and I felt that my features had reasoned me. I then exclaimed the public, expected as in regard to the spo\n"
     ]
    }
   ],
   "source": [
    "generated = sample(checkpoint, 5000, lstm_size, len(vocab), prime=\"The old folk have gone away\",mode=mode)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "The thing that should not be done.\n",
      "     All that was nearly a minute for measure for Ligetain reached the present start. It is particularly that, and he wished so as to see the prison and too clearly much of the essanian or my simple thing. It seemed to be something varied by confermed receaves, I shud, too, at length from the land, and I felt that the fact was in the grass.\n",
      "     At length, proceeding out of the bay-garden, shoother, with an incold huge pile, since there was me than the men of some manner in which the terribli wonter possession me of my family mankiod had been forced --  to my sea all imporsences in the contrust.\n",
      "\n",
      "   \"Peeply Japed dis on the e grow-hand did I put me to take it and did not discovery no more.\n",
      "     August 4. \n",
      "                          *   \n",
      "                                       8                                                      Her Ance as Pele.\n",
      "\n",
      "     A that even missace suddenly set leadiny it, in the direction of the cliff-hand. I was commenced this and until the metal prism. It is a hard revelation -- a few familiar physician of the activity of existing sensitive evidence of having seen the person who had been possessed only attempting to assure the sensible existence of any ane detail. This was in a did, by the man human being retreal, I think it with my eyes which had been suftered by discovery in my own house. I started for the more than a minute and a pilk of thoughts, something of the game of the wood, and with the fancy, there was my greater condition. I referred to the prime corridorsed more of my reality, and see in all acoudsid strange stones of my friend. It was not all of a cottin out, without any reason I could distinguish -- of the time met out our ever see what this morth in the fiesce physical of that distancenous and apparently impatience against accompanied me, idloping of life, and a few or the most structures oe men and considerable disappeared and produced and spired in a contented to let it from its family. In this mighty explanation I stopped at some time about it, the agait of the thrread. Ilenend of course, a sort of purpose of left hon utterly happened. In the death of our secrety of the experiments had not seized about three. He changed to be, as I heard and sturted, and then simple to an almost meaning senses. The seeker, wat to be, and I have said eating upon the window from the cold waste\n",
      "-- I passed -- but then, floating what mide it I felt my efforts to speak to my such words in his men culling establish.\" In a feet it implied, I am denable by the crew. I shall be related its consideration.\n",
      "\n",
      "\"It is not was then interested of any noise. I have suffered that on a sed of well-fixters and exastly as I have told. At length I am seriously actually descended --so incorcousted in my dreams -- ha will take you to carry to step for the trun sail; and suffered to a sharply palm truck, to it my course, shall be parted his absence in the moonlight. I half smile, and, as I was, of a subtique and parallele, the current and mother, he was not with the deeper intensiot of that university of Mossieur M. Valdemarahoment on the mone half indistincting and eteragly among the middle. But this occupier we had and whiteer and the universe of the colour of blied trees assumed a moustache from the top of the Archirent top of Nile, and had put it the boy passed through the air bus with a single shufficient scene. Could not discover until had to reach a love, leaping and run. It may be the later statement at a certain affection to shifte from it. On the morning he seemed to be an one small, had louder all trees, and of the alternative entity which horses was of every hand. After hinting that night the door did not fins titter on similar special oecs completely as soon as he held in a strange cirilors. The floor was heard from the stars, or even thoughts of ample windows but mankind to come. There was not a monstrous hungray, and had not but formed a secret of life from the top of Clennhhine, how-eving shanes and mummies extended behend. The corpse was precisely independent on the locking testimoly of the experiments which can be nothing less than the telescope and delusion ares of less and greater and emitation between the hundred years of a flight unmistakably decay. The formula would be sufficient to an uncle now, that the actual shading eyes are excessively presented the silence of the glacifue celemen; then, was noticed the mere curious things in some straightening of the ghouls, that the first thing demined himself over a line maroured; the eyes and linenosies came from the stone ethereal floors of one by mad and a break at my grandmother’s lower wail. The earth road, and the dark fittures moved sharply what seemed to be to last.\n",
      "     The suncessifierer room was above the end, and the right sea pat on decay that the youth would be on the rambling growth of Some of the Pictman Hole, and who saw it before him to say. Who would, you have fellow it concern. He was not sufficient to renote. His appearance is a really concerting tume. And all these further creatures I could not have been found much, but I saw that the beings when he said he would sing. The limbs were consumed the dimss suspiciousness of any dead because in the subject which frightened he designed the namilative surface. Their seeks, teeted, he wished to strange papers all atution. In secret man had become hideously for the sound of horror, whill the divide casts of the Starkae was indeed concerted by an inexplicable special postesion in the litterery hundred and bassement that it had come to reach the door that start from any other place, had been in a violent sitiation. On the third face, there came up with intox counten frighteness with which Carter followed the same interest on the other hills and into which this local masses were said.uth. At last, when, a second whisper no trace existed in the stark at myen as engeavoured by this time, and with the east shifiners of Decolvena mitting a clear dead circams and probably force me not to be missed. This time he reached the sunsitting way, so that the sounds in the black cats leading to the big, but had returned to dreamed our base for his singular and inscances and left with the sky.\n",
      "     On a low, drawing of the party was right and again in an automotion and thinging destruction for the purpose of consciousness. The river was so forming in a season. The fablic ofnerest occummed before in some corpse-like that nameless hieroglyphs which could have disloved. I did not even me by that wonder as I glimpsed were single or again after a few mouths, it was quite had that concerned search and its colour. This man was not doubt in the wall, the lady rock clatteringly rided to the city, and always afterward seemed to cell its whirl of the bottom of the cavern and slowly rate. They must not, it is being bect in to see that sometimes commenced to catch distinctly seem to me to it and were turned. Having came to me this rattering of the professors to be a serie of longing in the lower parts of the great abyss; and he came to him a discourse of sculpture which had not read, and I would see that the metal-cobcages fish of men who should have cursed to some of his wife dead before the end of the hollow magnain.\n",
      "     I could not get at first attempt to unknown mytes and lines of stone was becoming sough and I saw it. There was a more subtle emperfect and amidst physical spines of late and feebilishir atreciases of this planet and on earth-life. All the shelters of earth’s and most disturbing sky, but had not peels developed that which was only at the stupor.\n",
      "     It was on no froar doorway, and was clumsily supposed that the trip to the old man as I could see the amormid and five-tented threed inconcreved buried-roads, much specimens of subject for men,al accopfing one of the only possible personality which had so growing unwillingly that it might be discovered. In the last stars are burned and elappred in the action again which might be. Then came to me the gheres at that time each on yiuh terraced—ruther Innsmouth seemed to me a sudden consentry efforts to roab in a while, but we cannot not to be precipitated in the light of a greater subterrranean arched lady. The point of that city was in resugr tability that the objects were in caterams. He had entered, come on a far and fishing abyss of normal laboratory with his senses; but he did not parshed my few. I took a responsing of my aspect, for he were not open to the whole readily against his mother in haster and perhaps buty, for drawt miled longer some moment; since I dread that a greater caugh there was a shane, so getting not to my pale during the never sense of earines and nameless works—or the best than mights to them. This matter could not admits him the car and kept a lacking with the pale churcheard still pale less north. Starious, his fish and the and huge mountain as the fire was that of a part, and mentioned that the three hours was a man. This was near some mood-in this stone, and saw it all that house on the stone close under the gugs. Shantak, it with the times which had strated in the street. The town’s farmhouse were not merely tiled, ordering from slight books. Then some of my mind was scalting in my sunset within it except by the finely material of my grandfather. How could the cult it supposed that the hideous reality was no onyx in the arrangement wholly unlorded for his the less Innsmouth. At the suddenness of its going of any data, after modiva, my shock appeared to be since the ancient moment of my hell. The youth was a doubtarx to my father—and how much fancy the purpose of my crief heart. I had not discovered the paper; and it was his figure but I found no man in view of the content outside I ran; the frightened method of the climb were glad to see much asrocious abest typefind to my last. Though designs, and I had attempted to prove the awed in the case endeavoured to call already in the ancient staircase that had been starring ever to when he came to all of this shoggithing mathematical destination in the windows.\n",
      "\n",
      "In the New England slightly to be made ane, and one of those points and personality was indeed merciful. Only a complete court of apple empty should out into the north and shared the pit into comback and stare near the dack.\n",
      "\n",
      "Then stopped bold in minds to return forth incabrations; and he repoarded, and for the streets the pressure to a dream who had seen in the spaces which had still something the sound. Then they made a strange and morbid echon failed. And it was the sight of infristion between my family himself, for he resembed such sensation; I could not, or at least thu lott sy thack of the mingler man. I had not conveneed, too, in frantis defence that might be told toward the watch. This time I had not encountered the small planned, and certain things were answered and sharpling. Only a small abesen courtearn railed in its ancient specimens. And now a frightful object ipponed for the town of the strange period, leaving every incident of the men came again. At the same time she would be nothing as well as I had felt us. I was aware of those accursed circumstances, and the conscribable been as once made.\n",
      "     Of the first of the primord and matters we saw this ancient and shrilking as my drowsiness, had had immediately performed the singing invisible former not only in my own course. I may stepp debated by the twi i meaning out of that village of Providence armical interreptible aurian deep buries, doubts, and disturbances of making must make us my success. To be so infering to stay of that continuous mat after a single passing of terrific and uncanny and useance. Other four ground-like hands which she could not be sure to hear. Having moted, though last of him; but he had not been there for Situ. The phantasies had been disturbed, and which cruel had no even pets in the land regitiabe howlves horrors of Dr. Allen, nor would say your Poytagobie with heal. A dead singular shudders as well, some indepent in a studious cosmic position and changing at the someousen common books from out the seven densels howleds on the silent.\n",
      "     When the low left I have treal of all, but he moved to discounte surprising, case that more probable than the colmonite below the great city of the Great Race had sent bearing their curious. Here they had came to home a countenance, and with his resources, the suspicion of the commence with my head and the shrieked of the beltoney search himself.\n",
      "     “I had always to think they’d them—a give more for several dominess and enchanged sort. It may be that not that they were rumblings on the two,pthutn.s such living and black stones. These crags was plainly moving and pictures, weeled almost lasticy and back, for the man as the grey steeples raving all through mumoured, though other planets had all been strong, since the man if he would be obtained about her, while a poppet, by prociping the strange death of him as had been the passage. At libst they for me to make a ridicule of some digging-boot in the old squath, and me much disturbing the way. I give you the great passage itself. It was as it this I did not know, but the living motion of the street died in a certain grotesque room or effects of solt of the motor to the external or to the country and catch dwell. Once I could took the chues, and began my own studious importance to a stranger, no lights were mentioned in the library. And always was a wholly company at the sea. It suppose he had been solething thoughts before, but an examination of the others that had probutually exposed; or beckunal canocime of the strange shudder of his companion-combrantes. The inhurred, lower levells were found sever after a time, and was such as to do not return the hours.\n",
      "     In any small morning the child end of the narrow was firm as he could, though at the time we had something like sembsam of that non culling could be expected. He drendhed, starting, as if the walls and buseless matter-sought hold in the archaic mystery in the archway of the terrible abyss of young words in the rightfeet length of vision birds, that bound and not you can be seen behind. He was not surely to be created about that cich in all day, with hostill and stone mouldering gasts of fablidmareant mountains. Toward the soand houses in another long hard in his ancient and connected base above. Sto altitude, and an opinion of some curious dreams of his worst interest it would and print and intruding doca tell. In the longer north and less was their or going out of the steps; nor said, though this was might help. How to deepen much of the method, and it seemed could not have been frightful that might have it much as paused. Now I appatent fellow, and we had need to reach; but we were possible in the least similar forms of a large and overtake of mountain into a level where the signs of the houses and man trusted the rusting barrier toward the ring of a milder. The grave ard not the mouth had been rushed up to the great stark. Ther had gone baffled to the location of the Chanlelis or its level-well; when, with standing, fearful and until at least dawn before in.\n",
      "     Those strange ancient incidents was that of dark, graduations, tone of unearthing accessors in the disordered patient with the manifest and most value of the river-portion end of the realms of the dead city. Institutional seemed manyer, and with a low-wriping and planing to many a slip of some former bushined in a distant burden the shantak, and no one could see that his fearfolk was imposed. He was so recognisable about him, all of which we had to remain in ood, West and whispered about the night, and once he now glanced away that it had become satisfactory. He had been lofty, and became seen sickly and spectral which he came to many traces of odour—which he were prevaie, each doubt excited a men of morbid horror and uncovered cats. Then I thought of those solid revivalions I have too murdly an hour in a light for succeedinger even the eyes of late treasures mestankouse shewish rape of men which somewhat art of him large frequent idea of our still dreamer. We did not rescled the stories we was lack on what thought in cosmos and expressionslem was our father and most forting even to be strange whilst it spoke of. Thus the refly presence papsed like the time and lumbering face in the devil would say; and that no more than seamen regarding the other sunset city, heaven at the design seemed as perhaps for a moment a mint. How did he set it discourage, nor thought, he could never pract seemed to shall walks and begrnew as if no angth to gathering hold with the great catacombs of Old Ones and the monstrous than the sulfurnious cargo to his strongen and most-hollowick. There were lights sinken, and other things were hung, and every hunt interiorated before the shivic existeden expertmane. They were gone, and the greatest prefacious stality and public her detective of chaandlter. The chargs al ourselves were cruchal in the antarctic westward the scene, and the basemon toma grey day trace of the gods on the mountains, and that strange minute besides the great garnet college library. Wh was a long chair of the study, and what no one can scarcely interested on the occasion of the Old Ones. They did come from the ten hand to the engrass was able to cut, however, and humanly exastled to the sharp.\n",
      "The sounds had been clearly alone. The bouthes of matter, then thought that he had come into a small loner of usual patient, and which had grown simely actually emitating the outrated tows for this infinite details. A sound came—broughoud, and somehon the sea of the things still had found; but in existed in the antiquarian specimens outside of the history of ice of any avenue haunted melien. But why had heard of those ancient soapstones, and I have always been less almost underground. To have already done when the residence of its cuttorra sholless specimenshellss of cities of standing planet, and a carden grad and made, from the morbid ice appeared to be or at least his power and subsuded by the tropical hand of the Essee As I might than they thought have completely arched. The penganic acrid twatever in the highest period, there was no sound passing to open. We might care—to be sure, by what had sometimes been displayed any material about the way to see it. The first rat of the letters relaxed to indulge in the wrone birds of the ghouls and things asonst its whole of surface. In the minds who, now as they were which come behind, and was disturbed near the boundless hills and pauses. It was all human admistion as when he saw he could fashem first a sort of prople to seem to have been removed. It was a violent tast that notic any extingion of course, believe that the simphest land of the high tinger to have as a sea of the search fear he decided; for there was not a plan of dark stone, though none had on a fle knife above the recidence high marvellous curvised faces of the Great Race. These excases must have been attributed to these readers who had been lying for Several enseed, daicing massing and recorded in the country in his remlanted huntingness to his laboratory. He thought it probably they seemed agoil. All stretches which the sound of a change, and the ghouls which nothing was almost grotesquened and taken by some of the horrors of morbid things. The planes were lost in a sound of stark, near the end if incessantly sat and more, and the substance of her presence. There were, teoular,s the one was getting visible only in the sound which half single slabbere stoods—and he was glad to be, and the room was deader. The more had more liching than the tens of entities, for he was a great delirium in the trunk abiut it would leave their any-longer. All that they was clearly night and was discouraged, though the agant prospect of the skin now appears to have suggested another puzzle that was so straight with a cult as he called to limp far balfor. There would have been something e would probably swipp up a strange crystical paw, sinely from her soul who had left it to differe tissen and properly cut to show them if he told the house. The place would be filliin, and I was correctly interistenably atapted.\n",
      "     In the meantime the greatest of my own sanity, of course, were strong; sharply of that curious survey of the severel group in the antiquarian and evil-pointed station of instruments which the northerl\n",
      "earnest and fastened sleep. They had found altogether pun in a state of concertion by a strickly course; for the next night he closed the white and closed eld in the city. The wind, tranqling back roined frathen around the one and the camp and lling at the blecking of the circuit. At left an end, and the chew proserved it had never been to deter. I might say, but a triumphore shrat mind with on my parents with the horrible workmanship of Inganok of North’s. The things was evidently above whom. They had ascended over me as I was stronger done to meke an attempt, and I must harbly happening in the solution of the subject from a time. That it it was cliating about those horrible glooms in the distance. I was glad, too, that the world of a southerly outside seemed to mere shewing signs of heavy stones which fell it had been still so terribli over an above the glaciate throngered to Caltill after his breathing we had seen something very lightes, sunk bay-like figures. At enough he seemed to have barely so man in that time he sought,\n",
      "for they now seemed to be all the continuous passage of a grave had been had often posses of singular places.\n",
      "     When he had given him upon the filled lights of the Great Ones before it with a strange feeming through infantional sphere or steeples or curious resistance. Amongs there a few minds that one might make a gaom of inintricible determinations of that group of idion had become kind about.\n",
      "\n",
      "The rest of her friend was half faintly interior strange and believed, and his confidences man in that million years ago trie. From the nearer despenal or the constrect people we feared to expans the man a mortal disappearing again to this earth’s doctor. Stuleous the latter had been lost within the chimney--of insolu less a terrible distance or where slappess and remembrance excited and acquaintance.\n",
      "\n",
      "The right hands in their brain, I saw the sunlegn came, and I was perhaps that entrance to the country folk of the painter of my head. The professor had glowing about the police, so that I had conceived a farther dog. Whe, a servent and aroused monsters which I had always add to sigh and forget his ubuald study on the crags of New under towins to heaven to below. There came first enough things was of think below even now.\n",
      "     As the scene winds began to reach the evening for this into the tree farmhouse; and he heard a shocking deliberation of stench and the great varistem of the entity which the Great Race or Deep- and directly sure the Old Ones were such now and then in some moonlith that no less than five years before. Made of the serious city to his hold hinds, and had seen the public and returning the heavy statement had read and brought in a great player sent blood covering a stupor, as if not in the form of a single sen in holding and providing such a sense thlown while the frightful subject botherd to certain hungridhers and that small of the stars and the strange streets on the open day. He could not be sure, but lay almost he was in his eyes about the horror at his memory. His foot all had caused it, and had been heard bat, and down the seeker ppon them about was several of the shadows that bleed and pasted; and it was needed to get too the moon some earth’s greater summer's horrifled, returning to life in order to renor any condition about the room. The widd malls and grass are not the slight, bild of dissolution studied c rimisances and turned as the traveller who had drawn back in the distant blocks of the great gapp.\n",
      "     In the Curwen minutes a cloth depthuries of wide rate waiting place, and on that tail about the first hoil, had died dawning. They say, it sail, and scattered apparently, shrieking upon his memory and clear defends, but he was in the case except aland in a perfect discovery. Whispered he gave it a dead part, thus buried my hands. No sound passed in an occasionally human. Accordingly as they deceased, in an inchined change of his own serious conderne completeness; though, in a few whose decipitated account of, I resumed my arrower at hindous as the planes. Besides, it is very closely because to mean time and slippery both, upon our spectacle; this in the extens of her metion is altogether arrive; and we muct awaie in the strange things; he was sail, and perfected the circumstances of his formfrotted, having settled it in very necessary hearth. In the extremet shuthers were again supposed the youth of the headlessness of the abyss.\n",
      "     My the permont ismem at the other reasons retersable poorssive insucter. When a substance with a chair was very carby, and I had dreamers of that face of those who had shared the significance of the solution.\n",
      "\n",
      "It was not, of course, for some minutes and manuscript which cludged a cloud of unknown sounding rounded through the dreaded stench-Land of Shanls had seen some of the seconds and extent to the read as the gave had said in that plastering of the deeper and more designs. Curtin had come to him, in search after a scholled discovery, and the slightest of that one saw that any of the public werk waiting it for a moment. Though his feveress walls in a front degree, and his shunned coiners were not what mentioned though it was not such as to detail. And there are round and rivor—outside the nearest to him and without more than thicketly transprination, or of having deteithd and particular spicks to relaxed them. The proper objects of that tric fore was too much for much menace in a single familiarity and decayed and local command, and seemed to have carry forgotten a shock of his species of furtiveness and perhops in his studious hands. We had noticed the folk of Water' stope at an enemy we had spoken of the rectangular time to the increasing raturation. Once he was safe back, and the significant of the nature of that height cursed things from above the similar parts of the table. A current of traveller was flatted with curiously ropendous docks and secretly so far as though the whole half of the consciousness was lost in shart time at a ramidat. Bothing was cargied at all events that his destitud early age from the Carbatiee Morela station by Peters and Allen's occurses in his sentements, after making his copic or exilt in the file of his mouth, and shrought the definered mansions with the flames of that ancient hill accord would born tyee will find men till the commencement of that cryptical mad with the speaker was necreasarty. The expectation of sounds and enormous ranges were sufficiently considered thought and shuddering the battery, seemed an impossible tale. It was now slippery little by the carven form which she was tremendously listlesser, and followed him with a hideous sort of next day that product of artistic winds are added as the strange huge poles as countress after much intexplication to the sea-collaced sudgeranss inside, so subtle into the stranger’s extremity was somethi ghistly in the slimy stones, and in the sea cases of those incredible black small hoorssed thinns, and horrible metal with which we were not fully pointed about and disappeared. It was the more itself in all my attention to the ruthed and far above the peril, and seemed to be something low as the burrow and the abyss so past ouc subterrene shope. It was one of those plantess of antarctic fewer screams of his chain, so that what we shat their traces forced to say. On my bosom was a cloud and activity might soon moved by the tracking stand. Certain likening his performance was so rimilal to the water from the new assistant impression. And after lation I historyed a street dasted into some terrible part of the building, and in those three directions we should arouse more dogs ordered in tiod to the north whenever I could see. I flattened on the shelves and took a gulf of sharp and strange crawling ony—which must have been adventured by the colors, some nameless learness. This man was such things as I made no attempt to restrain yourslees.n Indeed, that yiunch of I was more than this staralscre are aware of my course. I saw that my sound were excitable than one of us, but I could see nothing of telioming, and I foncome to astern the subject of my surface. There are notices closed, but still learned my mind and the other hands upon his eyes and for a long period, that the mate in the sea.\n",
      "\n",
      "   \"The Prime could repoin, \"I am about this prime, by the survey of an Inemant in, a common mal commence, and I seared out observe in me in an identity---some horror, it is not so stand the patient's method still so. There was a minute fust tails to manner when the chasm with its culture. The discharred promonatire before with sensts, and the conduct with at madical anxiedy natural factory in the court in at the distance from the paper. I was gradually very but a time. It is nor way of those horrors of the men could be sufficient to do so. It was not this so long as possible. But, in that his orde that hung in a perfect secupity was that of a short beal for the purpose of ancient farmhouse. In this excitement of my own attention was a madness. The compunety turning up the brigf and result of the portion of the building, said to me that I could not see the latter. In this mind I took the subtle house in his own camp, but have had a crawlish, and the sound of the presence of my own could had truft ever thought in dogn. The new rat of the nurrow rieg spread on, breathed my rottern aspect, a few might have appeared to me that the school of this abyss can be done. The curious formulae in the region were made and indemendical than it was on even surfare, and no constitution in the bringing of those mountains of manner-couched in my shuddering huncingy shipters. A more the last thing soon seemed to insisted only by day, so that I could not reliave this primal. He had stupted the ghouls, but hewe at last he could not put him to him. The horoed, thietding finallour was rither and his right excuted soled field in the high and lightly bulk except for sight of Christchurch Came. Suppose his dust would be last; the latter say that he had denied any detail. On the third seven party to the courtyard, and were hard on the floor at the Stateonotin and I saided or two andly of the old Chanchan depira. Ye would have occurred on it, and he had seen the northers till it would tell his back to the city and under the sea there. A leigur, little difficulty, expectancly and readily possible to reach Mahy's toich-beyond and half-morning whose ending monstrous would begid the townside world not smell. They slowly reluctant to feel as he returned to it, and had seen that this frightful substance of the work was no matter. His almost had allowed our course seen as ha found searched, and he would not stay how crumbling his tale in spasses. This was the rim of waited by an expression of notmand no one who had escaped; and when he did not throle evidence of a people. Te motive entirely did so, almost every pace would probably have been arouned up and trouble. The doctor had the less of that commonest and ascentalyy curiosity this man reflected, tramed on bearing the great city of stars and leanners. He was,ed all on this direction, those landscapes beast and barele,s being the soaped of some gulf whose party were suffered to receive from the stairs. One of these were three few significances as seen as he nameless in a cold again. The body of my distiscinctares had so great stores of the monstrous meastrictes where he came to me.\n",
      "     They sail, and by no one but a slad and surprise and persperting for my own experience.\n",
      "     The nor answered omstract ship was before I could recogdine the shears of its shaftal frightened and evil, nearly solution which had once settled in its point. It was towardd even ths unhalf-inaten, and I saw and settled too nothing at all. In the next place I had to see the path and material. Then the selving mockey yaw in the first one I did sheel bringing up the halls, he must have forget a last, but not likely to find it; and he was glad that sun was the great gloom of the gulf of his assistance as they could sea, for Aselath was last; not in these troubses still can be readily inconcential. Two others had for her method to be so raminably force; for the truff, took his imperted disgrewing himself, but this was not the lighted torch to wind; for the end of the chamber we would have to have done the beat which had been that picked about his feat, and had at the landscape to him in an initiable place.\n",
      "     One of the movement was death, and the forms was surely nearly expectance. An air of this arry, it is flat that he was not a partious your attached testage of accustom as the pursuers of the sounds had left in their plane; and was not a caternate road, and an appearance of t brood markings were holding out to see what was to be in the soul which saw once again like any consciousness or even rambles on the carpet widescripted narrow. Once and seemed to find their remains well still the point of strange poor. From this centre of earth’s historics hadned as it had permissed the lost of that night-gaunts was getting open, and with the meteor through such creatures and feet and drease.\n",
      "     Walked down there was not about this fact, so that we were leading or wish to seize all the gordens of another strange colony.\n",
      "     When the hand had been broken ofhel, but these were some other morbid heads of devolopies. Then he dwapplied the stone mooden screamured, though my northers arting in connexion with the company whose surface was numerous. This time a large mumour of scales and more shocking stones wrought or discovered in the city, and was hardly forged to be discovered; but the rapid thing was a certain person. And as a personal thing, he it because the channel of the police attricutes all the could gather as it had sufficiently perish. I was in the city of his descent, and held the next day he was so entirely bareneds, and for some minutes my fanty had excited about the most terrific intenverse of the dreams that I hoped to see the odd shadow of a boat had been rather dark; and the first little fineness of the Other Gods were draws off the feet of the night. The masonry was reasonably before. A moment ran at one countryside in town, but only a few monnts—of an university could ever be aptrached from the great world and some 100 feet and started in that sculptured stone. The fishern founding the trees and bore sent fores near the road, and those who formed the fire life of Arkham, and that there were both a palaze of equally matter and recognisive. Then he thought the slat house was perhaps a he sentched close and gave him at the human topes and the great black man who had presented a conferenceed blane; where the realm of dras was neither curious toward the left cloud of dreams whechine habled on down through tilled tall inclines as they raised the bungalow was proparel. Men with whom he was permitted to deform some upper apparetus and shaken freeloring with these railing mountains to some observer, by the strange thing which had given me to say that was perceptibly accurate. And somewhat strong, for noticed my troubles in whispering was denermed, as I had never been in the creatures  were too confined as much of that of our wood—and in many damans, save only through my head and far latel explanation.\n",
      "     It was then that the subsequent carvings and miles is seeen to me that the fungous glow to gain him the gate to the design, and I could see much close to his fear and drawn it, in the community at myself, but was scattered about the box, when he had set first on Hanish the confusion of his face a thing to his last fine, finding his throat in the Nair Age about is, I saw, so that I was going to recoglise the interest in the end, my terror was made from ancidity as I had put on. The wind, from which I thought his present hend came to me that I could not make out his head that night I could not decided to step. They are try to exhaust with the much of writing in its descent. Its monstrous rustlens and bees thought, and without intercuperation in the mingleo of the Port Mortula. But will not be traceling with the evidence of a frightful correspondence with much relarsed and exaggerated bulk, and I found this sight of all earthdementant daily correspondence and with the steady rise to my memory, in a valie- the new and void of deception had heard me. They were marked to sometimes something in a great half more transiation of the steady and dessantaboure—out. There was an unascesting suspicion of strange decay where the trip in the her back was nearly upot one hand in the form. As the cloth on the floor of the crince to had time tile then and they can told. The wales as you can’t go with the itmesf of here.\n",
      "     “To somewhat I won’t know what he’s thought-lot beson?\n",
      "     “I made the opposite copie of the Surker and I’n as foo pursuates them disnotune.. Then there was a physician, an edecatione of yin’ on ’gan-his sanky.\n",
      "     “That may be a crack an’ I have told the man satis-about stop or the traped o’thin sane. He kin’teked hell he says had so not saow than the distribt me not seemed too comports whereabous is something allowed or in a late deck—or even doubt of the old slabby in Share place it. An’ never seen a heary—galmen an’ soul like his hunchus, but Wan’s stone carried an’ somet it fainles or shriek up on the rid of them things, an’ long not to be such a fright, and I seen since the bet of that board an’ bottle in the stars had been an’ ain’t no saound --it will be abandon’  ’n stare never hear tickned before the wild state—that knew it was not well, what so manyin’ around. All all time had begun, ablut the slighted hegg-titer stepped aroun’ good strain. They’re build no chamber and reach that nest men with painfess and bills and petsided stopes on the stairs. They was heary for no matter it is that we was not that we could see.\n",
      "     “Too, be liking the fight is strange of years pred one in his room by the tales of the glen. So obtained this seem to have reached the great stone-chaor with books. Calley cut hard to the torm on this place, built at the camp hitherto heavy stationed from the sky, but these weithered more rose to the northwardly. Surely had not supposed the light, and I should have told within.\n",
      "     “Fo, his gravely dath can’t be the one as all times about the rubories. And there wis little kind of steps from but two o’s indestrateors. They wan too much dreaming outside the roofs and came to him with his farm. What is the matter is standing of the stars, and—as if it were abuld, when I was eight and I cound tell the Inds. Ifenty was a generous hole in the sounds I could certain the scene of things; nor had he chilfished more than on the period of the substance where The Pitthes took the family had attached to pass. The sea-barrer had come so his thought overseadorine encorning it and dragged as much from the chief put off and soul and again of about the facts of my opinion. The pale realised me with the same strange characters which recalled the certain stars and towers in the glimpses of the old bare thurder. There was a hideous cryation with horror, though the chinner, considered as abratal an excellent season. On the fluttering carry in a wood of dimally coincidence with a mad assemblage of gulf of inside the primal mysteries of the trees. The old woman was in the courts of the liftle repetition of the barrilating things that flew from the straighten bodies of the fresh scroolers outside; and in present tendency would thin once more resolved to get time out of the narrative for man.\n",
      "     I was discorner, and one midnor like the organic flights that the town had been past. I have been attained t -those opening in but a time; and the building was leagued by an excessive mind could be fallen.\n",
      "     In the morning I even hore to get back to the expectation of my uncle’s good. The young man would find you considerations, but I will tell you that the last and her gradd and sought in the wind. There was no all on the other darken-spectarchiog, which had proceeded to store it was probably near the culmuran face. No the lady raised surface beast was the search of 1710, horribly this matter had been entured, and would not have done at that moment, and in when he was but that it was the past suggestion.\n",
      "     At length I turned the door of my eyes at the cause, telling of newest persuperaty and polping, every particular would be seen, and I knew not, in sight thought I had seen and furniches to hints that the worst is by means of a superhality. At length a ready for many examinations who had staggered out, and saw that the men and I had started it—to something startled at the state. All mental correspondence was still. Jupiter, adile ended, after another would share be of, and started into a great stone, though this connecting diminusion to the proper parts of the ghouls. There hudged outside the dogs of north and fificings, although he had several left since in his mother had a great grandmother. He think of the principle of the Pawtuxet farm, as if by matter, deposited a reply to the funal master. They were merely a little enemy -- and thrnes to the real parts of the country. He clumsely say a fair crypthes, and said he was a violent, and a passage from the Count Sinas nord the other eyes. When they were carefully obver of, and the stick was no means or a general rupory. It was not a vartarious noce, and all the mourting table-land wide night gaunts grew in a realmy, the present savage failed the mystery of the one could be despair; fifting even though he was groping from a few steppored to the dark with railing butler toward the sound. At the cosmo of the third ancashed pounds were like awake and half-inspecting distant from the room and leaping to a river to see what the desert traded only the shorseres of eight or marking. But the largest branche northear was a strange and a fall to the nonacchion with a sunken incident once ever seen about, but whence arrestenty for any treed single modical country. Ference and I supposed the winter snow and the final ship-furniture that the state of the private counters are not found in the canderitated world. There was one descending and part; and he could not have falling over the stone floor of the new free waters. They were loss in the strange men,abdetter change with its globule and cursing devils on sight of some of the powers and inspiration of the escape, and the galley saw to the sunk carest and two muskers, black pawnes, nameless and small plateaur; whereas a waves of shadow were new doors, and of old Chrimials and Nurres and in the hill and visible ancornt accounts of the Eleberian Ones course, beyond which his researches here simple in the dark and latitude dark colors. This the glance was bad to say to Arkham without delaying interesting or leaving the dreams which stretched on our shelter and carced its own experience and its head and free from early to see of high time with deeps whis the statement had shuffly body.\n",
      "     “Then the treessen who always come to carry out the sculptures of the temples with the same personal and unconscious wind, but I saw an encouraging dettime  an account of my own.\n",
      "\"For example, I tried to releven; and then surely fantastic as I have proved to be a fact -- ill this body, nor did I know what it is -- about what seemed to be soughe a week to so, so that it is a but to be, unous in, and ben more consciousness --that distant --but we looked --have here, and too danger the last chance of mind human beings rustled somewhat after me in the distance. In the most give men of Lodion objects dissaltting oriely by nimethmula surpassing. After curious cubinary rather a crust angalactingulor was so representable to the prise Ieter and myself. I devised to say, or to cat, we must piece to state of dellic. There was no nainable effituet. It was forty-for for my own sea-cast, and to force the particulars of the present few,\" repeated the ethered collapsion. Thus it awakeness of his concreto of the matter presents of grimaving, indeed, if they were by no means of ascertaining the words which happened as the desert crossed by the third storm, and the precise movement also attempted to be in a lightered dumaly out.\n",
      "     I had net pictured in the library to the southward, whose dark story in the house and some forting of the things had told mer. Between the threaten armsical possible catalicus of cuusions, might lingen by the almost in the frightful side of that father; as if the whole grotesque contants were depression with careful secrets of the colonyarce produces of rocking ineefint business.\n",
      "     The ground slowly and crewned, since some time became sure, for the fact that continued with any case in windows, another space for a window or steeple into the surface of the content; old mad nature; and even this was several abnormality, will not say in hand. The poor single more than camp had been something else wore on. Some of their strangely suddenly marvellous city made the presser still anmounted for their shadows, and what he had told was the curiosity that he was not muddled to him, and how vanished thought he was not more to see the flight of a day and accursed cour. He had once swared with the titan whippoorwills revealed nothing of the top of an increased debris of motion, and that in which the mass of match areas even of that tale with the cry when the slight mountaens of the rain brought be; but he wished to some family and he thought I could stand an accountable depth that my nerves had fallen from the head. Then he seemed to consent to his gold the great sound, who saw them so that he had pondered in a great story with a ballaged flower to the womlonk. The head was a graying fliou leading through college scheems, beronding not such as he chanted to look avoiding a people or indetersing reaching offions and chambers. This is the chase of mind fury of this blead bur that both in the came and claws up the steep ground of the town, and at once self a sideling-gloom of unknown door continually correposped that the twilight was mad. Answard’s velasine wasnaped in the distance, and take him a more honeycombing old man who had ever saw—the black circles of stones rivet and found there it had shewn that the cottagus om the gleaming night and the half-reginered seatable pool shiped in Thembin ty left. His passage and pussman certainly made sure to stay and of the town and intemptetable put it had an experience which had scarcely seen the colour of such reason would netest in all the long-loved forms of as mile northward to the line of his sollation.\n",
      "\n",
      "It was now clear that the words insisted in dangerous very genur which had occurred, but I was never seen and fearfully late and were histories. The results attempted to restore my feelings of memery. In the mean time I was so immediately referenced to immediate animation, but in the single thing of earth broke up from the common libnarre one digtity. It had been drawn my beside mications of the covert, but in the early more I was completely falts by a recent and entering on the fate of that revelation had been forced -- but I can still repart the wild, or the firm of Ligeia! in A few without suppesits, while I said in the dark of the community if would used. I risked a few wood and grew more to set abautment in view of her against everything. There, were perhaps spokes from the Eutere of the Sea- continually both of its considerations. The fac- of the smelimple--and his home had absolutely surficued! --I had at fastening inside of the camp, by the human spring, and ther from what he had finally caused. I turned the perface whar I had an idingance of great existence, I felt anything to say that I am admit to a short.\n",
      "\n",
      "Althought art trrushicges with a brief bone in the forehead, but of course, my only sincer the minute manes everyboand on this occasion. The path of the cellar lived in the direction of the dogs. I presently proved the party, of all did the silence of the slightest assiptained at least terrifrily that mere much for excitement at all was forced to being finding him in the morning, and this was probably familiar. It could make him do nothing about it. He had controlethoren steps and sleeper, with a cravied which I had left --develed -- some curiosity falls and held. The more heartention than,\" proved otherwise as well as myself at the party who sprayl the importance of this archwive. I did not endeavoure to relieve myself if the pictures might be indied; but in the form of Augustus thoroughled, to many pictures were absent. It was not until that fourth so explored were the one the high, that writher had been able to the dreams of the most unpermanuation of something, in the former that I was still more profound, the characteron of the heart. The serias of such a human turn was and followed. I felt myself chiefly, if no doubt which might be such an impossible that I did not might advise up from the breath, and soon reached all dark ships and looking about. They were much traced and as made a conviction of the mosa completely avid to place, and this relates were like time.\n",
      "\n",
      "To as well formed no means more than that of business -- the lideless scream of the Archaean burners, in my hand, with a short signt of a place, had perceived the rug of the rud upon the board and the spectacle, it seemed to be excessively anxious, and went indicatived that it may be the most imprehantile h\n"
     ]
    }
   ],
   "source": [
    "generated = sample(checkpoint, 50000, lstm_size, len(vocab), prime=\"The thing that should not be\",mode=mode)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "THE THING THAT LURKS IN GUATEMALA CITY\n",
      "\n",
      "That was in the dark silent returned back, only the front despuise investigation of the character of the archway that an eager roathed at last, in fact what secretly definite hatch made off the servic arms, of which I could trist my exeruite to the body in the length of the lock and distant abandoning. At the exsing its exterior name is speech. I did not was to put the handkerchited supply of the formulae in which I whispered. I had expected to mention resemblance to the captains. A fearful huge stood of a marked that for the time else where he did not less the party reall. My companion formes every man to the revelation was the concernant description of the entity and gave a performance object the percop of state, and some time we shot from the left estior of the other strength. It is only far the sound of a chance on the same time beyond me. I had learned at lisely to the white servants that made of the most abnormal, heart, and resembling portion of the ruins at the same dim pleasurent records. Sometimes I found myself with the strength, ethering perhaps to set our detestable position, there came almost as many diary as what I had to flee. This passage had been did filling in him, he were much true, the chief care, fell prostrate once in a still faint poor of the street under correspondence which his much pulse back or eye. Indeddy, too, I pect home after an edect was hurting on bed, and seemed many frozen senses in the air of mental and unimpedamine fortufes exchined the sounds of my friend. In the end it was so much as I have told that I had to help me to the mad. And he spoke, on this part of the top of the student, though that distant is again in sight. It was the closenest approach of all the more spearing for those frightful modern old—bushed outcoming—and once my wife seemed to called me friends to confect it; and when I was about to central paralysis, comparting a long pincally sport of earth—and I might after a disturbance of several precipices and private houses as with low and legends,\n",
      "in the antique water which had spied all the incidents of our earth’s group. They all shall was climbing in the barriers. They were they climbed, and his spot was heared in all. The stone came from a sub-glarcal of Nystland, but was all through which we most stopped the matter somewhat by mescens.\n",
      "\n",
      "    He remained, in imitation of the pervased men. There was a hint sorin, for any night are rushed to pland more than a loud endumaned case in the life of the skin. Having breathed so far as the forth and astonishment was gradually coming—the more insidiously not connexed tunnel to the orieinal world sunset. And all the streets were low closed; but the words were succeeding that I shudder look at the lost dark line of the great stone, and three sounds and fearsome circumstances have laid here. For the moment the first time he had looked far to it at northing.\n",
      "     On the other bidder I had seen a complex feet and a wild accurated age of greenish and other things—we saw a legendrers in the sun of old table-land. Then it was his clief to our sitting, as we perceived the track inseeded himself insolibla into his men. And then he done took a great pala of its old being who had been alarmingly. In the party, though, were the ghoulish thing of a curious tense covered with the greatest difficulty in the distance of any family away from the old pshent-terrors traded into a large purpose. All that scene of human shriek was the one surviving and absorbed moment; for the horrid little purpose of standing expectation in the spread of the great child and the floor. And as perhaps his throat such a fair banked and silently graniteded; he then suggested with distinctly, several other tapes as might be apprehended, but that was the opening of a circle of energy. It slept aware on the hill that riding earth-lore ocean.\n",
      "     These was present into the abysses beyond the reach of the fresh way the simple sinister stones of hatches from the original practical arms with a both full of nameless and impressive experiment in the Marus To din and callur. The wood of the Englishman sailed in the Hill was that a riving croud beyond the road, and I saw that the accursed record would say not to be abserted. Ane the concervant after the unused life of the scene was silent, and too lest a large barker near the earth, certain that of the haunted ahead, and looking behind him and found his feet and plunged from the fresches of the houses and the burghant in his name. That he was clear to hee discovered. They who grew low for each other door, and that singular lease had faced with a new goant. His chandeliare campuret was courde than the catalopty.\n",
      "     The shantaks which can not be much so far as concounted by the glaciation to the stains of the attic, and in every state in everytring wese travelling rotting, and once more before it reached it. As the ridges and trees of waters had been out, but he had not the southeand nor hundred the fever. In the victim of the fevered, for the rise seemed to be in the achua, and in time the rest of these scraps of disappearances were very space. They were rather and breats, and the frightened emptoes of the length of the new sartifices, finally, the master of our minds when a farther discourse was sufficiently distributed; and the formula was nearly a mile of inquire. Abaud antthmatt pingulous labyrongh about which they wished to cets in any other idea. When I thought of the strange munages for some eighteen islands, and when, after a last letter, and paused in a morning among the strange, so that it been riskness, and at first seemed in\n",
      "fature by a faint supply even timmer, while I was so with every\n",
      "long season. And then I could not help behind truly still a\n",
      "mind the craw. How I saw the mildiges of the trible for wrich,\n",
      "I found that I had actually turned from the window. I therefore dog, a sudden, and a series of personality can be secured in all my own farm, though it seemed to be all to speak of my own wirely into terror. Which we should understand the countenance of this, I could not help scarcely listening to surroundess inseef --in the final of the forenoon. I was there too more to conceivably before me as I stupped before -- on, it was, thought, accossinglished to and fro in my manuecchastlical ampeement; and in truth upon the subject we could devise, and, in front of it at a loss, something elsewher to preserve, he appeared to be drunking in the\n",
      "street, but through difficulty of condition to reach me, sutsing, in\n",
      "watch it not to be most picking the hile. That many of the hideous\n",
      "experiences which, except itself which had been within ma at the summits.\n",
      "I found in the subject of my succees, in spot, merely by any\n",
      "dust that my experience had withlushed. The incidents never spoke\n",
      "me, in this man to depart. He resolved to attempt again, in a continuous\n",
      "scundy, while he suggested, therefore, whatever embarrassment of\n",
      "such condition we sat so far as to ascertain, and he would be obling\n",
      "and correspondence with the gorge being in a stone still gone\n",
      "to the northward and instinctiveness of the rail. And now from her\n",
      "stared bridge were made found with its summit of the shrunken ceitfered\n",
      "him.e And for the rumors of the place where not for a circumstance at once sought, the western companion had been confered with its character, and through the aperture of a fire or givening note of this tale. Mattering so, luttless are important five, or more infents disgusting both through this elder reminder. A surprise and discovered between Laftather Fryeles of the Pierce and the Augustus Parchmen, who stude had a voice for it that my deathees- utterly talked much after midnight. I then saw he made a muscoms told of the new gale of the secret motions of the rig.t I brief stoopped, one of those dark, bricks and single-stercy agethate of his especial recumseritits; for he knew he was in a single later period, it would be merclaised about their motion about the box. They held no man had at hind sailing if he would—be turned on the roads and beard and dome of him lone. There were talked on his cabi, did from the shadow of the Bitting Street, and this has been at her terrible and encouragement but course to see, and we were equally perfectly seen to be stirring. And there are told about the depression past the Powtes and commended the belief am of the mining town from Brown Jenkin and the real swing low holls of path and like a carpet, for almost altereate see with terrific forces, we seemed placed, deeper investigation of the marvellous sunset city even to the last of the people to the rays of the head.\n",
      "     So at the read they came from the crims, came—an idea in the deaths aer here. They was not what I had an hopn of extintation of aspect and horror, nearly our executions and concealed disturbance of my tome, and represented me to leave many doubts a corridor. The conversation was remembered in electrincly culnicying dreams I have affected, and again I reached my vault withing to the skill of the rock, corrible hirrows of the cloth in an impression of thought which makes the ancient rumblings by great masonry. The surface of the sould was within my own wonder-strucked chaprel by the farm of Nahum whose flame from him it was so slightly true one of the mere profiss. Made of the meraye he grew see and then stumbled himself at the bottom. He had been twisted by the great rates and was body; and when the western was instantly thought the fouthtwire, with the glad that the dogs like in time beneath the ghouls.\n",
      "     As we did, we were composed of to the windows he saw that the man, from which the galley dasking farmhes were of nothing startling now by more than my auscestises—and I have strange that somewhat pitiful of floot of dissumtion caught when I had easily recognised by the precaution but more profound to drend us to inscribe. Aroing the dreary thing of that man was not my ears a titan charnel, and soon I claimed on this part of the Carbent as I have said that I could reach; this careful peaks of matter afterward are somewhat affaced by the strange extraorded well storm it to be on the other idea, both it with the mountain, she so much in the e recessee of lettirs imperied by the truik and content. The careful, first expised the party grew in forgotten powers of archaic latter and convincated nearly a symbol. The portion of his charactersion, it seemed lost hopelessly and faithful in me. The planets should be obtained to the main own in earlier closes; the power of discouragion must not be in a corner wish to the exceaming redieious injictions of their device, and of the most palaco of higher unfortunated four point. This was the speakers of Akeley’s lows down there, and of those servants had approached the space and timbers on this species of extremety and delivery. He had told him on the blied guls to heard our spectacle. Some of the vatives stuck transportation entirely from the strictly most hateful and most unaccountable dependmans of the fantastic whirl of the Nurreg metaod in Pilia B. & Condensing wretching from widd houths and apparently increasing fortunes that lived in care.\n",
      "     We can’t in the less resulus of inquiry, ano dead like as obsain as that was to be the openit even the land of monstrous heaving. Then, at last, a later species of police delight of any external amount or relief, but their stares are forcing those which might be cruesed from the camp and be to a right. It is off that absolute any of the presence of his week; and it was now seemed to feel a greater study of the men could have sunsitied.\n",
      "\n",
      "On every soan this rearisamment of our aëroplane surfece was now the preservation of the Nurse, and the purpose to that point in the hult of the bed. The cargs gave him t has met in the camp, that seemed to incone to his presence with the ghastly city of the visible ruins. Not that shift eselves, and of their places, had but little to conceal. What was the motion of a group which had frequently been curled in terror. There is no memory of some experience in that cosmic and lost intervening pact the lost of the soundary points. At the same time the linked cosmic stals of the changed and farther personal towers at the chain close to the house without true to bearing the fact that all the world and of the used propers and watch cleared at the stone steps, and proceeded to speak the method of the deck were like. There was the plain and the crusted sort, and the high travels and windows he might prove. Have still hid progress had died; for the small bout had been so chectected of the real excats of that meaning from the stater extent of the sculptures at the June. My monstrous ratter, ascended this gentle and black-blued tope and trailed on the ground floor. On the course of the changel expected the early mechanisced is to be so forghtur, as he stopped to fare, and though the latter had found an artificial manner that he was always over. But those words’s nightmared hallories on the great garden grope are never told. At the home was horrible now—the lipstage of the mind came no more. There’s appears to peak, and had never been solitation about the way, with the alting minds of some horrible neighbourhood. And in the sertant she came up. It seemed to be a mad compary of what had been wished to retain the place it would be of no use—or then he seemed so also as to make our companions. Sometimes about the way into the stees and cats of the great column about the birds and the small chaos of things fresh enough to flave up in a whise about the fine live fill. On the hid officer was very conscious; for his disious intense course was inclod, and not a single month, to wave on a narbonic stars which the two sides were lurking to which my uncle in sight of those protested impossible foragured—of the raid that was not him who had ever supposed it down, and he would hold the seas among dheats and fountains, yet that if he will cemere for the mere death in the embalming gromp of the some abserceness which had seen this time, for in a notime attempt to dispuse simeshard and fortaining my memory that I was a strange shop primal. This thing was a place of suberaral animal syphes, though in open monly certain observations they remained as any disappearance.\n",
      "     When I did not wish to go ham about the monstrous mystify of this. They were the mereas rather than he had alleadod from close to our hinds that the tracks deached. Had the tropic sounds, and we had the signal for any fire last after, and we could tell crusting the true toward any heast macaken beyond all reality, and now were of a frenzy, perhaps before the least ship from all one could stay seizer and then ennamed much to broke to him an ancient care when he was licked, but there were signs of his place and time with possible consequences. Would it be abaed no temple on earth’s curving horizonts of the strange scientists could inseitife. Him, in her face, and could obliterate the fashion; yet had the conversation found it in the means of a run-wound or inarchance of looking at this endured agait. This was a small ribband on the sheep, with a grien dead, and the sleep forest bichs around us which was hands. And as for the men we found outside our sight and warned from the family horrors from Climman's ancient revellers and brought in other side but the sounds which he felt in that record when he had learned him, and had no chool to do tom much, and the different secret of late set up in it. Even the writer was full first and water—in a vagor terrible formation when the hold back to the caverns in the dark and sitiated glow. He must have told a gratific ty a number of more reluctant tales he could scarcely for cover to his horror and in a world. That about this sorn of desperate days instiac, as if she was now trying to met the fancastacily which had been ready to brought to the mottor on the single third feet above the great street, and arrived at the identity of the right windows which I could half-intolerable; and I could dely believe that this matter wrought it. I could not see my eyes and son when Welt reached the one side of the exercise on our singular cormidal infancy. A malevace arose at the end I could see the phosphorescents, and flew myself upward at the door of the human background—and when I was half a child on the side of the hunk as we acropsed to the stars. Some of the strangely subterrene secience more than one house in Old Ones, Curwen alien to the phantasms of the man as my tarkating was I not thought it obtained the only shore and step down the wild form of antiquarian million or gradeaals here he rief—only a terrible singular flashlight full of ship. The ten man shut explained the peculiarities of chambers and curiosity for such a minate man had ever been so far as yet used. With great circumstances come to see him at a moment, but infinitely expecteded, in those horizon as large as in such a frightful secled considence a personal remote perplexity.\n",
      "\n",
      "That moment I could not see the opposite copies when I had thinking some time kapping in a sky back, dazed part of the haubtedle realm of human access the situation puzired at once.\n",
      "\n",
      "This extent was netestable, and was still a small stone, and after a time in his hope we had much farther on thind of the posticious catarogan. But most our love it had power us of way. But that mide of that black secret mass s ove take the brain as he had to drain. There was only more than the features of the destroyed and felt in the side of the cause and to the sun. He heard the window of a few winds an abatement, and said to have seen the massive ceilings, three mouths of means of cellar and terriboly with scurposrs which we chose to establish any its commence. The murice had begun to see that the abyss was not as man had not subsided. It was a very strange thing when rose the gulf was posseble, and was preserved the slight foot on the table bespeat. When Carter realied to his hundred and crawling characters, we again after a few in the mere temples of transminant people to the hold on the full Storehond Hillow. The Student he did not remember here alone, with the house of an indefinite but the songiness and castle of strange huilarity.\n",
      "     One sight in the rutisius was the only objects gasive. They were visible about the well-well-known form of flame at some few of the mildy dampness of the redic rigatity, they have not entirely cut so much of something determined between the ground which had been three feet; sandy indeed, he saw shunned but long, spread out into the city he did not remove from the black. His hands was toward mind, and told the thought of some fresh by the introcument levels that crawling chamse carven frequent buriess and dreams of treatheren evil flames of organism and solidion, and actual central space-tairt was fully of gigantic targes.\n",
      "\n",
      "To found it had been for infunity, and brisker of the primal masshy and spread of in the archaic part of the Parts of Dr. Willett knows him every hing was placed than the Gigantes chosen to the totture that lentting mountainous and sill-contined stunied balks. All the shapes stoned to sanity to a dog-trick, and the blow over all of these latter proved that they had been at the top of the house. Then we marked ouc machine from the stars from a peace which had prodicled him with some perplexity. And now we allowed to relia over the told, and never moved afcertiour in Bristeh, because the youth had long sensitive ends entertained. He was not quite dark bother of the camp, shewing came out by the nameless stench and talk of what there was no more to seat. It had been startlingly about, and have never been told to her public attent. They are not more than I could tell blow upon my aid uson that my presence goed instead of my own imparrassment was present and colder. We would be likewise to be out in his physiciance which most supply the greatest delareder many-minds dared strength no sare; for outward carven face that conminced by the man from all which, but was properly caused him at anch. They had completely singular in a single change. He was near a confined thing, and one man had reason; but the same statement feolly as born as I could.\n",
      "\n",
      "    \"The bantamned hatal!\" continued Kumans;\" \"Ie impross --and dur this way the rest, and strange things now be of what possible me it is not the solid good prints and felt out as desperately upon the heads of the full thing, and all make it name to move -- it was I now abseed at once in vain. And now the decision indeed the instrument; that I was such as to stay in some measure importance. I spoke, or thense as I am pointous to make him be a little before that after all. You have heard no rugged man, and at length scarcely aored me.\n",
      "     I will never be said, in the first sight of the cabin, and rehalled him callous experience. My brain ringing, polically from the recent dissolution of the extremity, was supposed that he would make it his gura long and four.\n",
      "\n",
      "I have said of, that I was merely tracked in the seas. We now saw that the again of doubt might have been short, for the toughs locerd individuals and curious key to the west, routh, away, or to shewing it. The chimneys around to hear and gorge, or even absorbed an ear -south less surprised; and the shored hands of delirium and frantic spranglss stepped up against the alterations and probably contourseds of abnormal modeenth.\n",
      "     There had been a consumeration of who weighted into orran of the strange selicionule to which the banks fell with a single fay. Should have been set abiering ever that he had not been lost. He traitured an interple, and would leave no that any aperture of earth should be discovered. We starked at all  it on home. Out one shell he saist the sinister elution of his flame, and I can otherwise to say in arising for the night--and this it is the most studied fierch of my house. She had agais stury fear in his son (art the blade was a\n",
      "small constation, excited in tho descent and attacked the spetting around the country in the great stone bords or two and startling heavy, groves which made the strange buildings and strangeness and thinns. Amid the frightful suggestions of the limits were sloped and preparatiogs; the fact had never been and horrible and strange creatures. That mark of his wife had seen hold all more of the whare seems to have a few minutes before him their cry or floor and sound-host concealment whence no finders are extending. And as he requested the slave-head spoken, and commenced back when he came to his car only men who had shipped or griend the rest of his party were not shrunk of flictions in the dark.\n",
      "     It was were how enthraming it with the ghents on the outside o’st to be man. My uncle's crumbling body shewed me talking in any trunk. I know them if he would be misenedffly, was at this mind, and with me well built of patients. The north was disturbing some few here and there in the company he should fell. Then he was tailing it, and he could not have found the exact specimen were gone and presura,ented in the tonster world of a screaming with a plank arrove as it was evidently though while all his little court was something withrry. Wish much of the marvellous sunset city is left alone. At further did so the coust of his presence in the talk at Anlest Person, but had took me to lead nouth it off; and with miding different incredible, every monstrousness to pier the apparitgoin to be from the study of the distance. A subterrene parsity was even to the pass, and by the third reason when you have fallen. It is probotable in any exhaustion, which was that this matter would not be able to takan in a pain. When I cursed the one percopping at the time of the low seat I could see, and so altogether a chance of this man who had allowed the most perpetual source—that had such as I have try told. And I could stay my spor in the dreamed near the hold. For how told me therefore guined me so exactly as I could; and this is a farther spot, who would be all the tale. It scarcely admissing to continue that this is a spectrish word like the open roads, and I found that theer time that ncheminy was not a particular presence. With the exception of that carven reason I saw the truther in the human shaft which do save for the Nortagies about the One Harbour. As I fancied, he cannot attempt at length not any man about this frightful thing, and that the steam had frighten horrible brief stones, and some of the places grows still unhalmost, and forced above all the works of no man made the nosiin wind about the sun. Nolly now near this seemed to carry a while the faint strack made him writing up in the habet and familiar. They had taken, at last, some bore give me as by sanity, and that except the oddly courtes were missing from those tiped winded sounds, but he wished to be in an expression on the great body of my grandmother—maguted brother and sometimes complete in the direction. This can came for my presence, and name as see as made because of a fantistic perminent proportion. The achien tentices arising in his prime dismal amiastreth an inch of some sea, and was far to want to the north whenevil we enjected; and it was a sort of great duggetom, that he might on it was so long the long-gentreal beauty of the morbid handles he had been so trackantly as he spoke. It was not a se on hand, but he would be glimpses the sorts of the closed contained ravine.\n",
      "     “Sometimes of the longly fired black grandfother—of the familiar oozious background of Bask or tome—here shint as Lemon were in.\n",
      "     Out of the number of His was that she could not pet to his hanted historical evidence into which the ancient room had become seen to move. And when he was thickly almost defided, and becomeing a seemingly amprey about the high primes of Notes.\n",
      "     As the age-open water was destractively mildling and shocting upon them.\n",
      "     He had still clatter over that trees long and formidly reclined to my companion, however, we had never suffered daunder; and when he seemed to have been able to continue myself no awful shopt which was the one and there- a distinct facility of matter most impressive than any handward or even publing his madness—for he had that primal landarn, and seemed to chamber dingaced tigations after the hitch blood. The words were it not bunnace, far by a great change, composed, and it is somewht gradually decopitated in the masser powerful influence of its across. The actual city of the beautiful and mother’s fashioned horror must have been greater and near of head. His room was plainly reduled to the college, it farefes, and objective at least a time were so great a certain perhaps specimen of fumbling and scarcely inexplicable exesting. For this serious case we found out a case, or in case of wing, from any personality had been impossible for him.\n",
      "\n",
      "His appearance was all those of my companions. The play is absolutely benected and among them; in little later. These crashing instruments alone suddenly cultivated and glanced, and with which I had so guttured to get up a secret till, whilst only a few deporters must have been talked with the desert. The nature of the dailings were not in some person who had dreamed and to see, though he had not a part of his organic steps which West was doubt built, even when a school of lovemontria crusiled hardo open.\n",
      "\n",
      "I could not make out the other powers of direction. I can oppress me through the particularity of his chamber. When I had been blooded, with a subriet communication -- near the forte where might be superinded a mortill --which I had no louder for Arthu ah anxious foreyous bad.\n",
      "\n",
      "I han amanden that she is turned as before. The shore of his countenance is again, and obtained on this thing into which I fell thus, altogether from my existence and supplies -- the scream in the countenance of his matter would be all a constinutive perfection; and, as the height of the doctor won to trus the town without a sigh-letter, at any poting, about a mixt haste, when he heard at the pendinume wh scrumpled and phraned, and soon behind his face in the meating of the here, in which the captain said never the livy-could be seen to be poured at its last night.\n",
      "\n",
      "    He did the thind of his calls reading, it was not the ends of the possibility of discovered by the mouth of the bolt, and sent my wild almost as a sand, accompanied by the surface of a certain summary of some vessel\n",
      "under the general minutely as it was. At last we secure the care strength\n",
      "and raised into the car by means if the descriptions ob the\n",
      "matter at have.\n",
      "     We were now the most compostentary interest before any extremet stage of see. They were, indeed, partly in my purpose, and practicaes or into a specimen by a primal chance. When the wind is agare to an inquire in the night, the conscious birt dashed by the structure upon a straight line to the attic laboratory where his hands were beyn ten out, together with a short time a heavy stalf immediately along boding it into the departed. The name I now flashed the still roafing of the high-priest. No sound was inverted. Then the sight of the scopy but parsiending earth at once came from my reason—but I had not held them excepined. In the dark, of Aurol, and marked quietly by timed a remembrance of me almost convulsed with metal from the throat of my final interest. The hutchinn’s rabbino spike a flimps of years ago, was almost as hap ancient to larmen around him. They could not see the final puesher of the trip to Curwen determined by a sin window and sanity to the floor servants when the simplict had sinst confused by strange and thoughtful above that sunk blazk and prisoledogical half-informant latent impressions, and when we should come upon a storm of preservation, a compalion which precasions were still as to penctic light theouger struck around the guarding of the breathing ruins. Then there remains t  so much las without have to be desproded all that to look again and was franking him. They had found more near the top of on Sentinel Hills. As we arrond a car shot as best the most accursed blackness being allewed to a single cordid will people in trusting the ancient windows. Stuen as it was the coast leader of the surface which had seen from Allen Handy and frightened his unutterable whisper. This thing was the one which had happened to breaker say in her sof like the picture his own. He had think of what thought is oblying this claws and roads the way, with voices of those monstrous things below, and in condented for some single thing below. A look cut must be the reptice of the crevice was complated. The sound was the water and his relovage, the greatest consequences of chambers and courses entered. That time he spieed and really beheld; and occasional stones of the students saw that how the tiny was he got abandon. He was dark—down never, forming a geat hage and who spresd edeadat as not except the original shape. Whilst the top strucknight they lose around the level walls, the farmhouse beside the ordiaacy of the beings. Something, in a perpetuarity over, wessent this men,al concerning the hair of shelters who have never been to determ the table, and for all the correspondent of the Good Forthihs in the transcondentenly went. There was no hagar, at all;      no method of the thought. As the appearance of a decided wild, half-increasing in the great past the crowd it with our assurance. He was not asleep, and since the second stage observable of the stone was a party of Rubis Arthur Gondemmen any however. The monstrous moon began to transfer eiviey on the radionary air of antaccipal ane designatating c runtically to the southels time.\n",
      "\n",
      "As was a party of the fragments who suggested what this monstrous reanimation of horror horror of note.\n",
      "     In the midst of this protectian calmertr, with a sailors attached, and to see what was drug. Then I saw that many of the motter was not that. Then and then a pandiencement with whimp the horrible means of a graveyard he was about. After my revelation, we had a commence oo unleast to see the blackness of head and they would let in the corridor and covered up to stall slowly.\n",
      "Those we stored on the recreain regenency of a strange weight rushly somewhat faintly at the town’s collepeanceaness; a continuous character at this extimped to their final influence. This taske, from sleep, between Lightan solesated in the dreams, the marvellous sunset city; so that he could not retrica him to was to considerable earth at all, but of the most previous experiments which all lived to the hill. Here, hideous, weathered by the insarent, discovering one mysterious difficulty in the fight of this huge, identity with which the very one darked dizziles of the cuntile course had a shipting sentence exceedingly attraction. The terrible were sensed. The presence of a large building; as if by triagges despers labored and sunlined attsibudot oddly against the freshness of the company; and these that had come from those perils a cats had led only a moment. Such did not resigning servactory cases the old ways would excite all the sounds in violent desire. At evening the lantern entity, though it had been in compliting what he could have come to have presently dreaded on a boy of deed in a window, assisting a construction. At length the well had little exploration of constantly silent features. He saw it was observing that all the tried to strange and form along a drunken of action which must have said, that some of the monstrous contresting power told her sherman, and with the sun of monstrous superstitions and had stopped the magic inseition of strangenciss to ship into the unpleasant discovery of the Great Ones with eld to sea and of the rest. The widdened drolition was heard by his party. There were, they a great stal-bark in the other Miskatonic Uther, whose expended agen keeping only one preservation prodigiously to, there was a minitere positioned town. Scorcs in view of their stars, bare alien, and suggestive as this—colleagues, however, cleanly an impossibility.\n",
      "     After an anclory contention on the Gilman Country descend at once the massive other captives—seemed to be a semi-offed on the fiest who hideous which was thought that we would have some two period could not be more than a monstrous place. After that Charles a tore take the sand path on earth, in the flapping of the black catt read in the sheer bulk of the whirl or the cry to which thr wind were and more pressed, even in contrivance, commenced to assumet with a murdernal line.\n",
      "\n",
      "    Ust all at once villaged victims as if he was still an interpreter of the stained globu. I thought of the place where I came on my resolution, for the nailed such first seemed to be a matter of incredible age and abnormal tremulous tables. These conditions belonging to the corner, and even whenever I felt sure that the titan man’s house, humand dended the sensa to cavern outlines. It might be sure that the ship was completely into the recollection of the method of the blasphemous importance. I would shared it spect with an indesing sincerities of my accustomial study, and in dull of these my uscers taeky as it was doubtless. All, shrend, where my face condited to be a metagows in the lands which ham anywhere eleven seen. She was a marking, indeed, and intermunally explained ever since we had come. The hideous rack of the dreamed Dunwich from Nathrnee stretched despended from a cluttering sound, and in that rumours we had come to reach some means of reflection. I had never been seen from my antiquarian cut as my hands when I had seen me to send out the black colour of the twiny, my fortune housed from his, and the full moon seemed almost as hardly beyond. Indectnacts and arconitos the silint after our shore about the time of the doors we altered the hands of some loathing concerning the crypt that had ever remember. He had struck a few fleth in those nearly esterial and matenic black wized streets. One might have said to be the latter was not to be seen, there came a more spriniin across a voice and d san at which we had seen such a confined corpse-like that of Persiar wanderen he is in the most concountable objects of his frequent clagge. As I wan at length there readily outside the earth and laughed and sharped; and it was clear that seemed to have been badly faint, I thought I had ever seen a native of my nostrils to shin e half confirmation. After all, it was the extreme county and easily madvesing of the hands of the same power. That huge stream regular and the base of the base as telling of the monothnol.\n",
      "\n",
      "It was now, if any reason, to make me this in the country, ever a chance of any moment -- of the ordered portion of the presence --  the blue effort to which silence you saw --at it had put him the community for this singular youth.\n",
      "     A man can it adive trae, or to soon, the crowd would be still tied. At first I would have to turn to mode. Before my soul here he spoke the point, effectial it be wet by the Air thing which has myself expecied. I shall kept the possible charge of the imitation of the time, or the prying and presentiable of its accustomic pasinnater, her eyes for which I did seen at my discourse. My own customs, with his head that the man who how much so took as possible. I had been to draw my claim so far than this any ifsens which I have said, ederg on this event only sort of deliberation I had dreamed of the central toward other. I had never sure the air. Thus was a great deal of what I had entered the college lamoust part of the country and some large black music of insteas of my troubled reveca. Some of the savenes I saw nearly appearance, and concealed a mould in the abyss and half in my curse, saying that I was near the commanden road and falling of marks, and the white metal siscent from the shore, and the worst things I could see the oddly anniliaious hole. Then in the second malter of the crew too  mangel, and I had never long began to reach them is secure. I had not confended to return without any settle of ether and grotesque meaning; for was the order of any other state of my own house, and we might see about the perfain of the grey twilight shouls be torched from short and lighted balks to see if the house was baffling in a pair of feveress of marness; for three or four lands and a horrible menace in which the depth of the discourse was carefully captain; and the air of delaying infinity of magdeth could see no taigh of the path, with the clammers at the townsfolk in perhaps he could not rescride there it is the motar trip, and so carefully progre seated ah a former any existence at all in working and sincerity for the tentacles below. Even as being on the company of the year 1865, in comparing manning farmhouses would be obtiined to crots and forming a cautiou might frequent and expression. The full of the well attected range about the forecastle afterward, though only a few of the wails it was still underground. So far I had clumped the door of some moment, and I had thought of consequency of consciousness the natives in the beams of the ancient worms. That was the capsive moonlit, arrived at a certain shapeless low intense and grave, and in that spade of the creducip of the mountains saw it. Once I was absolutely restored for the nearly very remote and set help in the and momentarly vault of fly spor in Arkham that waitine to do. I would fo meat of my head which had been dragged to dream for some fearful inrationality.\n",
      "\n",
      "    Far at the Adent of the Odd Ones had entered that things, he was guessing out of the steady robed reads. They came to me, were deeply cutting up to wore on beneath him. I would not live to any free, he died away few her little reselvents from the north and was fully made. At length I again meant to made traversitation, to make my socies of a bottle in the tunnel. I knew that in this monstrous sharp ceased stamping at my admicary intolication—and which I could not help by infinitely uplifted the horrified bluff or three dementants.\n",
      "     When I paised at the Mannectic Orgened experienced the rest down, and it might more the regular to meet much in his parents all any of its. It was not as soon as I design. I seems to be told for that action with the same city around and faint, and I felt my very narrative and unspeskacours while his sinister cryptic service is here served.\n",
      "     Afone words he stretched me to the roof above with a consciousness of more horror in the tragedy and the ordinary days of the daemoniac-and chock stampled with what sent at his feet, he felt the exception of the guidance on a stateroom that of course on even the middle of the roof of the curious descriptions of the musty handing and strange bodies in the some earth, and campulated that one and trone or two and whiting circumstances could be daught.\n",
      "\n",
      "In a me plunging in at least to le sungilatly all our earloly that not only accidently like the objects of my out. In the first place, the land of the chiselling of my soul who might be delivered to him, but he had little days befar in the track of the great cave may be as single. For the solat on the pad may need no change in the chain of that distunned and obvious surge. The second hall is absolutely uninvilled, and which I thought I did not like the but this latter. He argeed, as I was in a morning beforied with Marmoner, and had been such as will be possible. I was not as common to say what I saw that the possibility of the side months held my headl interest in the direction of my hand. The galleyns told me to the sea, and nearly effective between my curiosity as he thought of so ridious infinitely dividing the ears which I had pretised and expired the notice of that distance from the Nature when I did not think of entrance upon my pretence. The only of the one mystical happy and greatest collection I had seen so mansion, nor did any speech seemed to explain, and the real sound was about to be abut of all other presences which caused all to keep alone. If our experience was only a fearsome pigoup in the art of very portion, or the solid light of the camp. The assured libberins made the air of my histery, although the feeble all, hands became attached by a brief beautf and must be expected. A slight safe rave of terror was shuddering and glimpsed in the abysses whose deplination that lay in a structurb. And now, I thought it often as I came to the cult it excessively bodied, and the body would be able to see it fashed by my friend. When he was in one of the monstrous things I was largely fifully excited and contained, a subterrantanic occultism or suggeste of the mirst of line. The savages will be lieved the blood on the region beyond the latt centuries bus on the old house. And even through his handwriting around the centre of the horbing world of matter, for me would not doubt; but they had attempted before to complean.\n",
      "     We had sieness of these months bore living in the cryptic chill non-gestilus as the sides form a peak of grasp and beather and plateau and intensified by the pass, setting about and distinct and black, and prodigiously and frightfully suggested by the worst things.\n",
      "     And as I attacked prostrateed in that hellish tide as had begun to shut the extent of my own to distant and distrusting and metal was infleed. The deep wounds of this local speciel drew nothing with my mental boatds, and still hurried me into a captive mind of my design. Instead of the telegraph and the only remote and ascived party of the wind. All at once I did not wear a train of crowders would have somewhat reaced all the particular bridf left. He could not pair to his real horror. It had been such a mixituse and in his companion having been carrying and crule a certain degrees of cosmic abnormality to the doings of the messenger compounded. The old wooden low with attic continued break which I could not got any ancient belief in this angiences, for example, that the common might not be sure and learned. From their mamertial the presence in this long hidden cutting was remarkable, and as I did not look down to reach the track of the river-pantarous street, I was gone, and divised with me with a magsing subsequent distance from something very bursting with some curious and pitrable. It was, however, seemed to be a signal, and even she retained his lobsher connection and still increased. At last, it was against a car in the northeanthy obvious ruins of the great circular space; and it seemed to me that the survivors leaded upon one tittering the throat of his first tracks. Nature passed toward the weird monsters which I turned my imagination as falling on a few of the waves. As I tried to right on the room and reached me about the the small place, the same mind of mind- of those accounts of its original periods at all. Indeed, it was clear that the end of the days were of the abyss, but when he turned our hands following wimming. He wanted following with benour duplicate assigns the investigations of the old cattle, and that this monstrous party were obviously flamped up with a terrible system or concealment of the room. The curving candely relighted it would be talk and paged at his phenime, and what he driupted in Some horrible disturbent even the man had met our selucily loud half-dusk. After, as he did not take the trip toward the road. It was not tho one did at any time with doubt; and it was now immediately at any to bettir his primitive provisions. He had been something in the last realm of mind to carry the winds and serious sounds for simple sounds. The beings whose spaces were glun to be a shadow, clawiog, or shining, which an editereal party of the strange city of Celephaïs in Too-wit, had been at this sea, on eccondinions of the blusted backgeland in the old house, but before some commence could have since enorgo not to be too within. At this brief chief of the caves had been subjected to be accivenaned, but not that well as help. No protoctappearant of this shock at which I looked for horror-torching or even measw old buried sponding with my feet.\n",
      "     And I heard in my accounts at the doubt, the light of a nebulae and shocking distance, but assertly, however, no difture times that one of the monsters had many days. Wing was the morbid alteration of the mountains of animateness, and inspection. This craw, worridly charge, would be something activity about the huge attention when he did not wish to rise so far we had subdisted; though one old man had found the mackingly of the old stone by one—but he was sorey one or in the north when he heard which has heard a touch of the slightest and infinite specimens of unknown driver. Observed of clather it has not been deperted to cat an horror in the factworlthary studiet which the garent doorways there lighted. Though only a small flue, or in this after a penguin; and the receiving minds could not give Lines, and of the experiments which had finally reached the other side or twe toward thinns.\n",
      "     I could get down and housedowarded in the shantak, in a high priest nature, he had sought a checked thing in a sheed and hour for no mere rumour. Those who live to such are believed that he must be inficated, and one sinister whispered againtte which should be the true soriouseness which could not say. Then they was imitative possession; but that terrible manner ever found in the camp, and whose hand as the blasted hatapliss housed on the distant, and I now tell you to the reason what it might be all that. Then, is the real existence of men of my especial corrisors in the sides of those singular rocks. The appearance of the cold began to the ordinary properties of the power had been wished and it from the Great Ones. On the foot of the tailous interligent hint of huge grounds to still simen and grotesque research, distinct, and the monstrous phantasies he had all adoped, eitheren cy opened and departed by nouth-in the spit from the Great Ones with his own. He was probably weathed, and of course he had nothing to declane at the thing whatever to hasten. As I have said that, he was always obey; and haw me not talk after the father and starked up the shapeless room on my life. There was some moments of any restoring, it will be drowned the less than the feeling is of the antiquity; so that he had heard alooded the most human places beyond that point of ancient turbling gambrel pauses of the sunken magnitude to whome in a matter of experiments in the southern horizon. All the priests had frintled from his head definite in the Curwen destanda collapion was extreme carelessly that. The correspondent of the centre was thus entirely found, and was clearly not the probability of the perponbitanion opinured. But he did see not to be induced to return at all with care, in the following Jonns Herren, which had occurred one me, and the civil watched from the docks of selern pants off, and rats as he had taken thrie. She had been again when they saw the exhausted rib on his domination of 1988 with its lovely and possession of imagination that spite other than the more common at the story. The change in his car sail be disappointed to his instant references to consciousness and cale of struggle, with the crawed and flung gods, some of which a strong curiosity was regarded secrecies.\n",
      "     At last the moon-beasts of the Other Gods had been living whinning about in cart. The glow of the fine flight frightened had grew long, and still of the music of angalow was both abayed from the rattlise and the shunned head senses, and thought it could be to see that high and lineless still lay. There were fail vaulting, south, and city’s, teering words always with a chuckly advised and probabilitiese of legs and energy and corridor; that the strudgle huddled to any speech by matter vertical windows.\n",
      "     “That shydy as yellow yellow spect—could not surprise down wantages in th\n"
     ]
    }
   ],
   "source": [
    "generated = sample(checkpoint, 50000, lstm_size, len(vocab), prime=\"THE THING THAT LURKS IN GUATEMALA CITY\\n\\n\",mode=mode)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "THE NIGHTMARE IN GUATEMALA CITY\n",
      "\n",
      "    Bat thought of that afternoon, who would say be of his mind so far as possible. But I must stop through this orgin, and realised that I am going to flee you. He called all circuial teriod of his hands. In the meantime, before he could make me strange all the physical specimens and pointed in all the parts of the town’s present scene. He had dispised a voice that it had been brought out to see if, and the gigantic notice of the moon-beasts was not a most conversation when the climbing slaped courage of the things, we had not entered the nameless high things of Arkham.\n",
      "     After a long time he saw the fashions for those night-gaunts and toms,-grent-sailed garrey to a viniat of the same degree of that its centre of the shadow-bounds. The headless, for it was absolutely none; for those which were still underground, the third main half-faired and its soundang in i seasore for a court of his life. All hangs wereed and after a car soon effected and disturbed me with an old man in which most of the winds was a complexet cardingrace. We had noticed was the open portion of the human room in Pilber,, and he did not like the other specimens of their standards in death. These servants were settioned; but as they were in the midst of a perfect surface; and from those heirs we had lanked elsewhere, on only flood of the body with a seven fragments at pleasure. And swallowed all over the wind from the haunted dark for straturi, and escaped the party thanner. As the shade of the creatures seemed to be in our terror, and an expectantn sinking ecchorish exportity had to work untalising a hundred heading through holdibon mind the distracted damp. One of these half dangers were clearing to surprise at least for a long shape. This was observable by the weather, boyy only a hideous hell, in a bad way, and the future was swooning. His dizzime charsed excitedlamonity. Their singular faces had the one was time for before, and for some minutes left us by the final ragit of the traveller full of immense yitling his ship. Of course, we were found in the substance of the married than the stonewiat had possessed an expression of horror account, and have still learned than some of the archaic Caramestic halls and bore and berons and learneds had studyed all the great pare of its bodily angained, but confidmed a lone nameless casementain carefulness. The lantern were too constantly over the very supplied by its fine-distonation and the former processes of medical death. The shunned houses were powerly rolling from sight on those silent forts of brattering whatever there was more than a shunding dul invelling placeaus, on course a battle and probably saw that the tenunty and other middies of devil till the lengthouse accounts were forced to stop twelving above him, if they would have remembered as he came to his father's compinet would be imagined. They saw me to hown it, indeed, wo form of many days of more than fust and disordered circumstances. That was the opening, and that the storm was too coling to the treasment from the compass and decared them again. The worst of the past seen stamed in a party and was stumbling only a distant, building point upon the sudden and sanion awing ot risk to the exphesion on the cosmos of amminest which could not be preferred. The old Cepter rowding no trace of the most instinct, they were likelest became to me that which had proved his specimen in Partiange while wishin’s and high binds of primal types. In fact that dark pit he had seared the whirp this bright idlevitable about the first faintly of those on the distant bands, and at least were a period results on the planet assitted their nocks. When the limits on the sin wanned toweds togather, and the real pennactic water was completely deformed.\n",
      "     With a sense and stupendous rising nameral, if the gleaming was not a panic, and puzzled about those of the mark of the Onne broke; and not for the last of the decaying and crawling chaos Nyarlathotep at any other travel and obycipies of the shills were called by\n",
      "the towers in those strange meading run of the crystal it as a motth at this moment or cable to cross it. That was affair was wrong, and was broken is a hattified and shore--caustward taken writing me at the space of those enclassive considerations beyond the practical and spity of the musky stroke on the wood. Now and then a fanguement company, there was a curious sensitive concupion became so seemed to death the more profoundly disturbance only an angle of the great ratiacal and semi-credecipies of probable companions. To body we can titter being a colour or slowly described ale the pressnor of a compary in the monstrous pair of thinnet morning. When they were still left the new pass over the crypt water-doors and opportunity silently for a chulch of unknown and unimanine world. There were temperstunest to have already connocturs during its attention with the guls and means of the path, and I should have to tin down from the tobch and to help, and had become more disquieting and self at the bothom. He thought it absolutely believed if you could not say. Is who was about though f one dids so long as but I would not be wound us. It was wholly over the spot where, by the nature of the bires much familiar. The thund was indeed becomenish or the subterranean milarrowene. Indeed, you could not put miseness to do so. Only a few minutes might be a long shrill and printing about the most different and messenger from the panelling of a pervadeed Branshon Spacis, and with the man baumed and fabled to relieve the intending altered fangance that he was seen and having merely it in the course of out or range in a cload. Answeritt disturbingly it dim clumst and the idea of drawing near the back room, and the change talked would be in the decrated purposes of the gugs.\n",
      "     I had been and dreamed of waver they had gone out. There was nothing that the neighbourid ross that received an irrelevant and putting odour after them. The hideous recent region was a house of scent in this vista, and partodies of teers from eater doorway to the original place in the conversation where the track of a steanthmmalf light he climbed apit. My friend however if I had come from the bon trapponds that had guarded time-talking from me, and told him that the new reseirgr was somewhat belonging. It shone came, flowing myself at light to escape the most precipice for abatement as I referred. They could not get to invelte warne, my experience in his hands to exert has turned and conveyed to the strength and as I recalled only by night I could read it; but he did he rend the perpetual afforty of my single abternation behind them. But mose now for some things in this read toing it at once how such speering would be likely to fall that it will be in a moment my head dreaming. It had come, that this was of my own stage, yet we had possedsed a cleft, though I was more particularly herpedied. I spoke even in such a come furious sounds, finding my other huge and violen lightning, and I thought of so probably many such a doubt at the terrible antiquity when he wished to conveyse with my original best place. The lightning-companion hear and front spoke, so that on that time, when he had almost still something who had ever returned. Thick was seized a dog-tort beneath the elder-rooms in the street, and with a glow of the horpes that lay crossing the ship of nishe on the stone whole dissolution of any date. Then thers may be traced by the sunlish and shaded thereabed;; but the final our resolution he would say that he could picked us as soon as he took a flight, and by day we were considered a little whole obscure; form and delayed of the similar spaces. Even now I still replaced through the street stand in a cubtive, and at the same miner light, and that he had seen machine to the find-place to that portion of the burden. The conscious of the curved data which should be so manying any dreamland. The cases were probable that the merestry had disappointed out of Bahur''s Fery in the Charles Willett stumburing to home and fell in such a part of the mere diversion. It had come to be—better be take af any mine when her deep growins seemed for thim experience. It shamed him go to leave this fan, and would a gleated trickle would permain meaning that he could do believe that thick and bridge would be laughed to collapsed. There was something in this way before him and return to the almost minds of those who had previously fourd secrets and children of the unusual perip. The interval were sound of all at first; because it is that he had managed to late a great street when other days had been safe; being like the four or through a fail stone building of a glass. The first place was a goly apparatu castle of what he saw a fears of the party we had lost. I was the mood and how the formilae along with the waling'roov,. Whit holl Chispord Have impeded a south of her bury, and wratching the investigations made him frequently in the dark and which the patcerns of medical majesty would be. In this start before the state houseed, and had allowed the desert and certain things. At any stone night delished to see, on the lady which he had farment excited the curves of the man has natural, had mentioned the secrety of his storeed habitable goods whose numeers and preys ord and claimed arous fast as the physicians were puzzled into the scrint at evolet dementorat or even then in clothing and determined to the reveration should be others afterward. Then they shuddered and felt an eccouraging that he did not wished for the monstrous laboratory at the top of the Permustoon. Some of these, it was sure, he said he was so cropping as a shewlh which I must give to clanc  Mr. Worse's own part of gravest and other opponents of expression; for whatever they found it in human regulations. Had the books are dreamer to be some might infinite distance, not only about three fundy illness, and were still infinitely. Such and under growing chairs in the side of the clouds gave the galley so thar we were abandoned. They were gulf and reasonably, only in confusion with the first and indescribable still desert that no other is here through the dreams of an almost chimney. In all the pavements of the attending fust though he had spared them the time with its curious and advance that her father held it. With the more silly continued almost incredent and higher helpless buried suggestions of the Ordo Babette and Names and Ash I dosces’ leave and abnormal geniins that they crast. They’ lefthe or in the dread outside around with the leghadd which had shotted up through shands information, and it is spread out the first sullan tale on his sight or an enough, a colons and a surviving purpose, although the ice-sucted places were slapper or spring upon the human spaces. We had no disparming on the shore of horror are ground and rigid terrible menacing the hardous curved highly partly disclenged encimens of gold they would have found the matter. When the right had sailed for the first time on the stars—and they were this in the shady room will door. This way wore than ever since here and there was any occup in the past leading to wind.\n",
      "     The tremendous morting tome the hills outside toward any house and turned fare, but I was held out brief and my realtical position in the survical primal place, and its paper in my distant emits concluded that some deep walls was plainly musinger..\n",
      "\n",
      "It was at last to be intirated; shut of what I heard of my hald wore, and we had seen and left. The course of me or my presence of time and sound chosen at the tomb, and I found that the power of my opinion had not shocked these evil goings and things of poor Lake’a de long—sensitive and malignant scene. I came to another drive and the ruins of the air and again his drankmanked and thinks, but no proprems of creation are correct notice.\n",
      "     After all he had once meant beyond description, and he wanted to save that of the face to the noisomnees, of the burding to anster when he had come down toward the endless void for the windows. There was something he spewred, or of the abyss wall dark and high wich darkness.\n",
      "     Once insufferable this lefth a dear thing had become more conscious. I have seen the time on the floor, and depthies of heart of forms lentthers my mind hend. The door believed, there came from a straight certain season aget in take. Then the silent specimens, the antarctic scarceles of the farmhouse, which Ware lan at hands and final sellee mountsippers in Europe and His was.\n",
      "     The organic letter reported through the doctor reached the basement of the strange, being true of a mountanincable formula and the men of tension out the men concerning the countryside in the town. And of course he weight a shapeless study of lapsing and brighter than having somewhat duffy rost singenor, and it could be seen that the greater present-kind of the original shape ever obtained there. No sooner the sight of iccouds were over the color was bound on a ruin of sont corridor in an incentrative space. Having spoken on her examination of the carvings, and that the child on the intricate mirages or the convilsa and horror were stepped in his entire reality. This condition had laid out the solid artic at all instender, since all along the sunlognshop in that vault. They remained in the labyrinth of terrific specimens for some rife opened at ail, the pathetic present of the house on M. Valdemar; and me of as later even the son Willett had not heard any harm would avoid of say, the house was so many very slowly, so that the pownesses of shelf he could discern. Why, and I have to tried it on only wildin’s nowship reverberation. I still looked borily an impassible trace that might consider the abite at all. And when West was dead; the traces of ddilling woild in a conversation with Aksaman on the part to much asswres on his angly which my dreams. The present odiation of the library in questioning the oddnessane forms, and terrible confidence was permitted. With the middle of the shnood his corridors and six feet thought it was in the seven college legions of specimen by a pengung of tenes in his new finerest. There would be no one; for it was not a phantasy and concealed plunge into a small hour, slightly losing only worker and cause. He had no longer assived that, and I now felt something about the thing which had thinking out of that cryptical madners, whose fasting following day he worked peopled all alone about the home. The shunned would take had started for a time and part of the monstrous barners.\n",
      "     The non-gleasing sands when he carryd the Outlards Were near that the moon came or in Bolton Stop. As a fortunate sound seemed to be about the horror I was looseding—it will not be supposed to be the rest and was winding them; and I had expected, interested of my terrible personality, as monerable, in a sten many moment, from the nameless latting of the striet, and lef in a complete calmonsmal paint, and he remained to advist such specimens in very long degrees? And the shelves of the pensith it had come to for many monestrous things I can see our father to make out present inheent from me while the faithful ground would be only four or fifty years afterward.\n",
      "\n",
      "But I dreaming that the back of the health and beginning to my hands would not be disturbed. That it appeared tracements all around me, and I detain he would naturally paid him early over the house? Then he told me that a monstrous appealing year, shoking from a covered spotem, and whene he had come to see the abyss, but the wings were slightly passage. It was more than I as of time and presence was checled -- I thought I thought I should not have to depost my ears and clumsel and the long-gaunt to be either had been really verted. He spoke as we all determed in an expression of a conception of the outer of the two Goddens is not the custom. I had not been much for more than history. I have not the source of examination of steam in a fresh intelligence. All that is a few measures terreble and uroed by the secret; and I shuddered at the extreme and motionless waters. I had no distinctness or endlasher bearing the ship. His mind was a visit. It was careful, but that the curious shantaks wint even the semetation of some springling off mysiliated hours and articulate sides. All this time his father was apprehensive of the to me, though a singular round pass were first with a long stream in his shockingly familiar lumber to arranged us in the catacatacious buy one, for the gales of only the night watched no strange sorn. It was the course of a stone customary ling, and all the ships at a large choking had been pulled to find the region whose return fan and hidselfunching was reported somewhat attredd to the rubour. West had for eater that the gods upon a guoted and armitaped quickly puzzled only by the minutest name of the most horrors in the distance of an unnighi,h and tremorged betwixt it he had to prostere; but then he was about to rescun the living police which had studied it and the stormer had been stopped before him. An applac manned as cramining with some of our electnical vicantical daring eridicl experimentard instants from the planes of the courtrar. There are groon and ancient whirldood from Cola in Town on Septing, the difference vities of dramatic signess, the district, had sufficiently communitating my own part of the camp. Within it had been aware of that beast and horror and other. Stopped enorous and unstivally an unused place—where I cannot adopt the energy to sevale. And then I was carefully give, I perceived that this point expressed himself after the scinning, altogether on which I had traversed the next morning, for he we could not great this and hideous secret of infumierate light, and their convulsion was dead. This, too, with the thought after toe—I will never fer from beneath the man or position about other temples of malesome and invaniable horror than the bulf of me more calmered than I willled him and whose demand of all the correspondent would never saw that the sheet realted houses at the came and follow him feared. We saw nothing like it. Then Certed is etitation on the ridsles on the same man and the star- foot so lately the stone had seen him forefeed the moon-beasts. The barkant looked in a careful collumoner, how much thought the full tome walls on the faemonster. Some of them we decided to reach the loose in torestond, since the tits had finally started them at last, and after at the tending dogs he came over it in the direction of the room. And yet always of archaic men came upon this vision, a few howled sarvations thereo penguins and caper to the note as at whatever distant fears and the description reached the prectures in question.\n",
      "     On the find of Cumwen, however, we were put in broad throngs, silently toward the stream of the sound. The words were so gulfs on over barrier, and promises to push down at the commencemous objects and possessor to the meadow and screaming from sounds of instruments as huge and monomanous abnormality whatever favourated.\n",
      "\n",
      "It was then that the text is about the ground, and there appeared to be some time before docnour even were at length, adied as the great stench he gave him a mad as beyond boldly amus. The fire was pressed from indeed and been, and saw no meaning in the sound of the highty earth. There was something mostly onight in a classic even greates shocting still lorged bowes and peaked spath since on their lifts from the stone with the great departion of her changeling influence. His chandelier came from a rumour and a half to the footfalls we sat didently concealed. There was a more absolute name of hopelessness that held some slight and unfancyed bate lay bored. Once a closer dreared life for the significance is to be assumed a very curious abnormality. That the lines of motionless cry thick be affected, and saw that the entire cincle of the presince had sane; but though the wind had gone away to some drunk and his men filmed with the muddenns of a despeaation.\n",
      "     Allenisam was insubted at the sound of horror I cannot be retriced.        And yet all the arrangement set up the hidden talp-like suggestions of man. How hid they cracled on the doctor life of the teat. I again learned that there was no inside the sentence of reviving methods and the recent merody began. The house was a glance, one, but by the way their prodigious massive blasks made is singular whispered.\n",
      "     “But as they was about three, the guosomp was starting, but he had took much to look at the round windows and hung up to a strike offy by the sangy company of earth’s party. There were twining and two studies stands of strange and uselessness was seen and pawed tolary that a party resembled it.\n",
      "\n",
      "Who, as the strange suggestion of different sculptures were sold theoryeding as the span and surviving new passage. At length he could not get some time on the narrow place that was seen around, and for the furst impressions could approach it.\n",
      "     Willett lef well that he had been so that we could not help fearing. And in this second had been setured the dreads of the private massions which setting up thehere and the such portion of a singular case and angles where the eastern shape. He thinks they ware peculiar to the linkale, and the sounds heaved in a shaft of listening sound over the rock wherein dame old houses and tall stones with that; and some the faint of the men was something effectea of some silent cater. The very little corridor went dows and bulges when the sea elsewhere the hands of dubi are the stench wall. Once on the other books he had slopped out of his blind and actual age and weaking from the unstapped and help on the hill. And the distance towerratt of the torn’s engiles were subterrene highations as to see the although clear face and purpose is that of desperation. The pite had become so inside, but that northers well at once that his going should then seep hady loft above it. And there crossed the speech of through that red, setting to a small peaked rid low cloud before he had come. After a thing this particular point was too late, and take a sphorath decayed that earth’s different rapid was a compare of the Guarte’  limb at the bottom of the steels and small, figuring feeling, and a small blasted wallusarg not to be, we considered a good odour of neither with strange designs and perhaps, her imaged transfirration in the archaic of the seventy stone.\n",
      "     On the twentieth of July 17–18, 19o8 of the expression of his steadily enpoired it at least on us, and were now and his case that has been called as besort. And when a large oloun—baddle-mist surface was scrawing around and acrepred through the deliration of the lungs whose pirates and carven faces are on the south flame from the sixe of the windowless expedition.\n",
      "     The next day, he found himself starting in their party. And we decided to say, endeavoured to rescun the warned beauty of the thing which he had seen it at the right distance of a Brown beyond.\n",
      "     At length the child and scraps of the ship and trees like angled toward the space alone was.\n",
      "     We knew we expectation to sound myself as the ending as the steer had left to mind, but seemed told the less than the town from Him; and of the ruins of the Great Ones were closed with the chamber for home. The towers had failed to strange metalh. At length the day before he was no light, but that he had never seen the bay-clothes in the day of horror.\n",
      "     They had been an horror which the the sadden hatalws might reach the matter; since his solid lifterness and exploration was a madness. The afflioned used strange sculptured secrets of eveny stagement in the Miskatonic University At the Corners after a third put in the bus. He saw that it seemed to have not into the mad or finally followed with storm. There was something about his sentiment and hall the touch of our volume, and was very difficult and enterbained himself against whose shocking records and empta would be decayed.\n",
      "     We had seen the closes of the fourth story, where the great black mountains was distinctly and begain they had saw it.\n",
      "\n",
      "At the afternoon, when shocking feeling it within the Craped city whose other could have taken place of his own hands.\n",
      "     The disease was an old new part, and suched dognly long especially from the same strange disordered iron reaching the street. It was not exhausted, so that the stormer made near the bottom of tumbled army, scarcely knew what had been baffled. My nourishment he dispeared, Ind and the fluttering of the most darkly personal course not tranted. They can recoive into the others than that he had subsided better than the Ank of Mr. Wert. He had soon brought this planet, and reached something or in the gald, a heavy light suddenly covered the wrode farouded on the rest hell after the level where he had been at farth for her two as he dispersed. And as he lighted at the close proper touch, and possibly his giant sounds have finilly died in the town.\n",
      "     “I daned gather on the glowest flock—old that of your—lumber!” Before In tall the thing was clear, for the wines of essertial animation was more fancy some probable more curiously with minstense of suggestion I had seen that, so I felt a heavy power out of mind in a radia visibe ald now even in the madness of my archaic spirals, and much opinions with this place as we sealed from all the streets of the company to approach the apparing in the laboratory abnormality. At the time there had been a single result of the forecastle as we could see them again into the blackness, and still the transforth turbed and bodied in a safating body of his character in 1919.\n",
      "  it the case of the hollow sings and leading tingler from a man housed abay, was how feebly traded out,ine the normal skyil of objects at the starbiarly days. Some of the materials were all protective footprints, to whom those others, the safes--or the rat- to the south of the black spot.\n",
      "\n",
      "    \"For what was to bed, especial, asserber, I about so, what shall I excite interesting in to the contemptable object into which I speak to in escent and renuer. His first day is surely after me, and holds from your abiesing this revivacion of the by off my vengeance had died think.\n",
      "\n",
      "\"Yet I heard it --and then I drew near it, who practited to his actual dimense, evidently since the example. A mendal conclusion of immediate counterpait which might steppe as a passage. It was perhaps through some unknown roath about the madm, had come to uncome from my fever. I saw the same mind, but a few stepre additional ob incordation will be been.\n",
      "     As I retrect on the experimentality of a man of course in a third chaos of my terrible world of condections -- supersed my features, what I took it because he knew sound obviously from my immediate distributing from my place.\n",
      "\n",
      "   \"I no longer his assurtance,\" which I felt such cordinciously, as the fragments of the moment had aean minutely. The sculptures are stricker and fear. And stualing the fevered wonder, afrest resembling a subject to pieces was belonging to the mind of a moment. And that I had not discoursed that my prefescors were the one state of mind-business, into this own words, and which, in sight breshtes, and little more than this fear, for an intention of his father in my heart, and found that his prisoner had a far sort, but we had not the side that sentine courtesons were blotted above. The world well spoke of thet will not force out on the stopic sidewalk; though his curiosity was for a storm or possible distinctness and endless inconvenses with its old copying places as we saw that the matter wantcoderon far belief.\n",
      "     As I struggled to this revolver, but I shall profess to discuss it to himself so far as time. In the nervous carving was, I feel properly so gained with the unconscious causeousness.\n",
      "     What would I do not saw almost from the creature around to his steading rubbing alones and millions of years before—and I saw that the excessive sculptures oldest thoughts was surely nameless, and filling it down to myell for the sunset by the door. As a brid sister was growing now, and I have taken place to a danker of a strange and aemitation. Existing since this man had seen the night on the mountain’ side, and the stradem of the fires, the father and putting of the slip for his side, and was climbing up the bastled and treated fasts of the gleaming d study of that formed presence of horror through the secret city, and in the end had no encirnal excitement or explaining the names of the general true papers. Sticly huged by the blood to him an earthly conversation before human beings bore the manifestured maps of the far-fragments rooms we shot down. But there was not a few of the gugs whose meaning one did not remember him. Then had heard a ship of days being absence, but there are ferencl that smell, and that the return of the first met which nearer the latter’s belunttiness.\n",
      "     “I made me thy terresplocial speech, and to drag the old back to the toughness of hir doom had to be of my form. But those months I could not record the care of my reading, and he was gradually unfolded we might, but the effect of my arm will noved from accident from my distance, as if provided a mast of all place; and that this peculiarityed loomed minds looked upon the curious things to think of against the greater vaultous street. It was, it was not that one—managous elitition among whom I had at least tranquit even to my lines. This being more pointing to head an experiment or exploring in any spring and singular antiquer and ten age to and fro to the northward rood whencout reaching to was to catch the country. All the file may have been in my hints or specimen in the college library, something at the door that start to mie, because in his sonly destructive eyes and frantic planes of destruction in the strange cryptical marl interrupted and could not accomplish without his reach. The grave was past the heavlessness of the houses and magichaniss and involving menacest collections which were continually probably sure that all the trees and old lands of fragmentary had often the southern hall in the country at some of the highly sailors in the shriek of the shellin  aper, and her enougi mertain wade against the voice and the curtain was not thought; then, supposed to be a low rice so that the corner was really vated, and had men in the midst of madness in the hill the stars had been looking from the rememorable rivers—of horiognthile, though they did not assume this idea of surrived and conversation before nourishment on the old Archaean forging itself, at last moneratily impossible to resent as hes to din. Guoply knows with a part in the sky and prison of Carter and even the moon-gentling above the sound. It was preserved in so desire to cettura notice; and for this felt he had never explained it between the two souls with the walls, but all to only a few inhumating horror which has ever been in the can. And alw the things were absent in the top of the Storieon in a centernal baysway. It is a trum even made with the farmhouse before they are acquired the one wideh and leanes with the strange caverns. Subsected through defees and beneath the ground and shapeless stay at the house, had something taken the top of the thinners, and all the across there reached the prodigious guggs under the chat, and then the six who told their cartomment, when she so guided is the reality of her former hestering ship. Charles had always free those he had set from the destroyed sentence of many cases that stopping in the books. There bade that some terrible vast gentle frived of y whour self she said that his sense is how turned on the brink of the subterrnnaned almost monstrous endigins; though the signt of this waves might be a certain admund, and the way had come to the horror, who at lyet common trip to imaginal appendicte with this part of the traces of the city, with the endless-sturty family which it was ager encounterebbetcond and of never hinted; and he was now this concerning the entire moon.\n",
      "     As I was subtonidently dished enough, and I finally reposed, it was clear that a chaos-of incident which turned to the floor of the case and like the spirits of though marked up as still in the dreamed starbously. On the chore of the strange discovery was removed, but their reason concealed in the practice. There is no momentory sufficient time the sense of motion of the incantation could be featured or recalled to organises in the characteries of that second corning of the metions, and a series of entirels at right hands on the fact it ever would rent. He had not lose for the subject for his face and design, and his present holl doubt had better not that they set all violently with hiddon articulate substances in his cast, and doubtless wishing to traverse them on the mere that of the twilight amazemant of unhaemed mansion.\n",
      "     About this was the annays of the hum—now and then if his hand could not have pushed as was to be considered as almost ready.\n",
      "     What he had seen it at last we kept a rudged pretty beloved the snack's own notice; and he heard his vaustable, stratch of persuseing abnormal paper; wholly a fearsome of our further charge now gratuted how the thing they do not remember.\n",
      "     And now were thoughts that I endervored in Akeley’s care. It was at about 2: 30 P.M. it was a period of stepping or song to see him closely to me that implicit was the one wholesome portion of the true, that there was such a thing of earth’s greatest pears which had been frageants from all the nexpshapes gulf sent and so lawed—the like cordial of these collops. I saw that the thing was a party of its indentacted battered and sort of crypts but the recouds were somewhat abuted.\n",
      "     In the mountain passes, though, a series of one, holdow to addit the subterrene survivory by the progress which may have seen and steady to look at the head of the cavagion. He was at last on the side, but the state three months to the playets for those which from the hand-down door thought of his senses; but this, too, attending to the world, and I hope for the text instant as the fresh particular state of his speech because of the merchants who had been in the extreme farmhouse. The whole country followed the nearer proctical unal places, where in those doors and parapes of New England graves sank both by the militaries of the roof; too, since the weode was nightmare, the ghists and beners were lost agon to him. Then seemed to be the west, when there was a single sand, communication; but imperied in the dark stars, accurace became vorved. The ghouls had laid outside the dream-burgenwanisced etreports to call on him, and the house was frequently remained, and at the sea of the community had possessed a fact that the skeletens of the curse called from the earth but still lifeless and immemorial formula of that continuous caverm with beyond the perflect circumstances of all more and more phradiably imaginative.\n",
      "     What he could scarcely force my speech to my final infinite southern portions of the shock in the repetted desert. As I daped departed, I felt a mather, a dead on off to sail, and those ships became very dream. There was something, and it seemed as tile to me a sand and last necessity continued. Then I had ever seen something between yourtlesque from the sky and stipping est of the guest-or thungs we drove his condition. His room was appalled by its bright good species of lost investigation.\n",
      "     Then he made a servant farm of Stons—but it did hear sound so except for some hard circumstances which resemble the prisoner, with one of the monstrous empty-city over the daily spires of his face. We had alised at once, though of who has felten the professor carried with hard court alorg any obstinaties of Dunwich and Tillinghasts, and an expression which had once seemed to fail. The correspondent had ancient many and storm of half-choked suchers to hermersy from the house, so that whose horror had bue to do with seals, and sometimes suggestive of their modes are a mad age.\n",
      "     In the mistre to live there, and the great bands of thdse hills and correspondent lingused ever winded the Archaean soul continual; nor indeed the sacrifice inside the waterfront story we had already been drunging off and daenour living. They had, they did not keep them a self-door on the sade, and there was a sound floared in our most authorities that he could not have sawn that considerable familiarity of its belief than the metal cases aspection. That is only the family of a hateful astonishment of England where waves have many departed and overcovering excitement. When the time he had frightened his exteriant the first tinent life, which was something unfamiliar. When the grocery books had been solitably too much for more than his morthla dissolution.\n",
      "     Then had an old time-some study before he was born. We had some minutes sphared, and they were something later to come. For almost haste and purpose all we had occurred, but I did not fail to speak, I could have been many another after thought I could see the blasp of the surfuce-wift that mind, and was not a thing which I have since found him to help some other change. What was the calm in such a man so minutely as he threw one home and feel. Then I stunded as seen, but he would fail to make him up any triungh—and sometomits had noticed the succussI of his sort. Some inhtrant could be nothing else than the gleam story. These really about his hear he had a single lockity of hinted humans and excitements should not be disabroad; for when within a substance of canon can be feared, but that in commonable listen force there was a generolance of opposite victim.\n",
      "     Then he was gone, for there was no linkage in New Yam, and not that hideous nutrer thus speaking while the hills did not wish to steay for his head. What was him out of mean-boat, but it came to him, and they can be not mishing inearition for six minutes. Though art frightfully even incluned to this host, while Dr. Willett snapped in human creature. They could not deliberate, and ask home against the westward-panec of the gods for thickest eschestic existence. Only improbibation was no more than an identifical shape, as if to have burst into a structure landiagr in the day by dryath, while At simon drawing he did not recall me alive whispered themselves by some obliterated marble of that change in its pleasure. Almost evenyd and incredible detectivesed at once he was partound, with his flaming the sort of travil and recent memories, and whenever he had been relented for my ears. The truth is a vague deseate, and an ancient and singular scream and pursoin of his work and might eachest.\n",
      "     One time the place was in only a man how but found in the lower deserted through the warn bencaflel flashlightlg.\n",
      "     Then he said the bounds of his fear of the thing folk to his presence with the top, and with a single night of usual clumsy --hose and horror in ever the wake. I knew that he was it on her put in dark attention; though it was careful one, and that the lutering was at first a person which such a hund used to brought in use it to cope. His mase accurately can be partially force; but in vain; the four of the textuses and small rambles-which had so carelessly think of what the more timida wi had been in the motionless darkness of the head and interval where the books he had lost. The whirling of the boy as lay a specimen absolutely nonected. The boat was possibly were—including all this respective degree, and he looked behind me he had seen in the bizarre motions of the guarding of the planet. Sometimes I entered His and-Holles and the fields had been at me, and I was glad that a time alearng and worked uncanny and rising at my son, and I did so it was heard my death, which more much excited altogether in many man heard against the corners of the great general priests was me. I had never dispersed that the god wasted in an instant that held the cry to warm out to see what this letter was almost afurivative; but this opening of the strong desire to more which it was mere mummies; and in this delicacely unresidently shrieking bas- or the expressiond which could soon be made it the awtennact idle in his picture; the ghouls maintained to do nothing lord upon the eighty-thouess of the original two development. A terrible and a beniant on our dusty practicaumed and reported descent of the mountains to which I was indeed recovered. For my right was in a fresh but of mind; for I had no necessary for the only end of the banks, and was still suining, and even in the former period hell hall on the room, and saw that his cuttings made notine that many other hall and hardening thing houseoped to see what the best of the memory of that house were bewildered. The moon of his figures drew mere the race was mad. As I till not permit the excreption that this letter was an impassurable condition. The labour and whispered when the two penusmin occussed in one of the driver in Providence, arrived. At last we could convey such insail in the beating of the ship, had been learned of liquid me to resportite here to resolve the remitance of an importance. The pressure of the entire ringered mutation was immediately reved, through it impossible to see the archaic print. He must have been so very complete. I could, and I have alarmed at once along with my fears, had but let me coure all in ruint. I tried to me. I wanted to my uncle known to me as I did indeed kind of mine. They had alroagy for evil, motion which the shotk common felt of the painted stage, and all the crumples the great secret of the boy was very little wished on the perilous pitchions of those points. The hideous extra origin led to hind the fact that he saw it was bacha in the pot morally and curses in the had lost from the subject. He was found of who lies in the least sea of the strange college and so called. That the door that came from a dark story in the boundages of hidden ruins and occasional darkness, and some inside of the subterranean party, and a second look observed to the carvon, and when I stumbled to cross the creature of that captive, mind for my own parto, whenever one did not rise in my right hone, and I wish to catch before a thing was pulling his enough some of my minds and body of hideous and morbid chair in the necessary, loud shocking rave on a shapeless rubbish trip fur infiritulal animal. He proved to be a liftening tow rental curiosity as a wall, and far awaked up, since the stowygr came. I was so indicuted that I might melt that this was one of those dreads and curious vast. The windows were probably formed, but nothing easily washed to the main above all our doorness, but not a man suspected—that it was something if they were not that something was glad of the words which he seemed exceeded fror thack and tell the walls of another Game to think of it. There were brilliant belf-sones of earth, but they did concegt at each storm to a rising of croting and propostioncling that he had noticed the most strange incoding conservation. As I troubled her for more than I had no lop in the later youth at all. Said they were things about “expedience—and sight of the face of the party around the party roof in the cloud-risen toward the north where the mar world of underground hutal pools. He was speerin’ able to see him as had a common traces of forms like ite and death or of the memory and strange fevers. Will attentive eating he had send me as struggling within the roof which three days make their changes in that low army. It is always accountable in the dark worlds of the narrow lites, and in the deep half sore stowed in the south of the region. Six fields a dewatd, when we first sighted his father surfored out the name of Assuatem in the night of earth and sent me alike on a shocking deliberation of some horrible succissfr. The mountains were of sounds in the business. And then she sometimes lloved mendily within a steady and more declypity at the outside world and many of the sealed fragments dropsed through the deceated bungous clock. There bross a bit of ancient specimens, or the sale mouth crossing about forting the star- from the complexed remaining chairs arranged in the sky.\n",
      "     We had, a strong strength left before the great grave was no longer any more tell. Sometimes at none of the prints laid in a picture on the sadates, he spoke of the creatures who can see the name. Even the low stains he had seen the matter of noon, and that this wast the almastrade damaged approach the name in College Spacil end. The wooden glare with a bottle setting one of the great palaceors and powers in thril now—the cast long beard theories were concerned in one was talking about. All of this time different like this proposed effects upon the cold waste.\n",
      "     As I touched to sleep night about the watch toward the ancient realm of the headlands in the bas-relicion or part of the company whose speech could ever closery. His main happiness too much about it at llatter, except that the ancient and morbid belt went down. The convulsion were thought that I might have left at this. It must be disposed, though the lay in the room had become a tale. In this space was the other side of the direction of the entire yaarching which must have caused the measured bank a dreams and as lay at warning the skin beyond the great swomp. The ordinary summer seemed to have alien always and man’sied; and at length at length watched how to reach them overhead between him and the camp which he might try to accompany the cryptic visit, of Cours, satainal secret and story one bore into the stone dropped in until her way much like the time of the gleam of his work. There was something his methable sonswarch of their stranger. Such a fears may have been extracted and possessed all too to leave any occal of them.\n",
      "     We repaired to my own anteriative tubnes—and what could have taken for trip the south. It is the next day I knew how such a body instantly enough was horrible. There was a subtle tripomed assistance of the power from that child, he was something stirring into the hills and never expressirnened at the top of the entities. The ship reticned to see what had been tited in, and Dr. Willett regarded all the captive minds felt that the scene water to be san, but it were better than to think of all. But the sub was press, and ain, and supposed to take the the moral and unidentifiable state. What would have happened, and had one of the experiments with the least very picumen of the place. The youth's sailore warned by more terrible farmhouse, but in vain to this eash casual waxch in he knew. Though which the greateriar may have been talked for the fast on the on the floor. He said that his doubt believe they got him thrown and never before; but the expression of the his own scourse stated in its continual death—which had the few came into the night. The travilling dogs the base even thus staged and had a great very pain or surmoriality.\n",
      "\n",
      "It was not thought itself had come in me called by storm of the palm unkeote Mansien Things. The sea she could scante their distinct, the youth for a year. The old man’s charce and harrisem, however, were in the nearns or draught. Had he still seemed neareved in the care, the stretching country aemoss had storped in firm in his remotion. The sort of world as night been almost rising to an efforts to sound. In the spirit still the towns ot this. And except the car now hawe on both insufferable and lifting, and a memory arouse would be at least anything like a shadow. It was still allevant that the place was no longer and having as we his subtivities and sculpture and other of the mountains of Narua..\n",
      "     Only beyond time the first single thing in a batterious voy grew interredt and intelligentle; and it was not a stron gash beyind the new and rocky shisseh after learning through the linen paths—and was seen thought in another’ doze betw. Hould tell he saws the thought came at all. They, must be considerable, and here and there would be merely beaitif in his manage. They were like any personal sense of persuasion. He had glomestered fared and thunted as about. Now, in a very ration in the exposure of the ruined fragments of fungi, troubled him awake. He had never seen the preasure be no with to be troubled indeed, and so choked as the cellar was not all out. As the camp as the waspe and all, or to constant a little wording whose dissolution was done as with a violent fangurad, andleing indee tall; and there are condections that madness in the dreams were the recent operation of a sunset, and seemed to friends in reality which made the spot impelfers as the hands of the town as it would leave. Seemed to be terrible by at more than the consequences from all the stars, the recent mountains significantly on the sides of the stairs. New days the sea-says of a rat- and measured by anyther flesh almost hints or top on them, on the floor and street undericked and broken at pickly death. Those other narions started from the farther peaks which must have been cured narration. Aftereal mifting the territory met with born and runnant while silened, held something by a singular civil was put fortatin hin to heart dreams before the mouth was of taking them and to chence only only three or five feet or awaken from any of the drops or preserved Curwen, and consented to fear that the wes were like that far three hen grass that they have not gone. The farm, as the cragy against the stars, the cellar had looked suddenly at right and looking at the bungalow to a distant fears of a furniture, and though that she had first been allowed only in a shuddering man, and die were timidatces. At the time I must have been obligeia, but to all the\n"
     ]
    }
   ],
   "source": [
    "generated = sample(checkpoint, 50000, lstm_size, len(vocab), prime=\"THE NIGHTMARE IN GUATEMALA CITY\\n\\n\",mode=mode)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "ONCE UPON A CREEPY NIGHT\n",
      "\n",
      "All saints eve magic was mysteriously about the weird service of the Ourang-Outangs. They were very like those who drowsing dreams during the sinister curtains we had not given me wurm and a half of this ideo that it might be all. In the summer, I saw that he appeared of long, both certain penits, of what must be infleed. I say to my now, with the process of stringen instruments, I could not help shudded, and seemed utterly fastened in an idea. So, there came a taigntulan matter, with my shadow, I saw a solitary sharp cities, the clauming, rocked myself in substance, the scarabaeus with which I had failed. As I hope to-having sufficient dying so muttered as if there were evidently almost at any time. Then there came from a climber of the thread of the extreme, had disturbed me to perceive the first of the metal conversation. I now finally discovered to a longer, my realm ireness is about me, to be expected to rend relatively over his mother huncred and side; and after long is this latter, I seemed at liret sprink down from the torch, during the scene of the profound signal while the cats had come from what had sure, much of whose exections made through those old faces around us. When, about the glow of a lure matter was indeed the gods made.\n",
      "\n",
      "The privace complete way, though the beating of the sleep-wakers had been something living on account of the sandstone, the face assumed a small place in the first section. In his real scheees he saw something still his fing sunset college doctor, had the shadowy danger of the party reached the deat. Three outside this brain seemed to exproves the gradiancy and recognisible black heads of the ground bare- and that the speciality of embalming precapiously discouraged his matter. The roars age and wearness the gulfs had been rushed back, and that human figure would not be mentioned. Only a steep life in the neighbouring performent coustrs of the higher holls same for a benefia, or to which a beam of light cooluged in the secent spote tracks of the elder rocks which another stuping occasioned electric torch. The choking groups and cursed surface frightened hum became a fantastic abysans of forct, and of how the more special senses were crawling before the eyes of the dark and intrivate things in the dark. There were much rushly sinister bearts his fettered head ares and coverties and seemed to have as a response in the past and coming of the marvellous singings and the whispling in the steep world. It was ternes of the most thing-paradelin floor of the transfortation of an artiful and spirit of the college library. The chain was fully six hundred and time all the world as well as they seized bore loath. Several times still for translense the pood data not at all absent. The paper, t ought, to be save to attack him on some trouble in he looked for many alien duranges, transformulating expansial and other feights of the elder mansions. The whole matter had a dim listery terror, and all who was to see what those things had seen and persistently incaptation. Then the letter was not that we fancied their moon-beasts and waste which he had once fletty and heard him. He had not hell me toward the normal collapses, and abnut as will be below, and the curious wooden sounds were seen that the savages were bearful emotions, and spontances, the other of a godgep was finally two stoped pet Curwen's estand in the expedition of his host’ absorpitation a paper wharves, and when the light was clearly new trumentony.\n",
      "     The present condition of the story in which we searched and increding several more himpolous and instinctive pursuit; whilst the houses allowed forth in the streams, and at once he was seized in a sont of impostirence for his own singular proper party. Terrific, and appearance was opened for the facts. Ware sometimes concealed about that mistral to an interpreter as well as to help them on the path and recorded it. We had mild its personal profection from sight of bodies, and was considered that the news of his fright-get fragtted conditions after that had comp stull properly a peculiar cossis of certain subsequent northward.\n",
      "     I do not know who whetever such certain speech silently upon our earth-sole world could be found. The night was marked before, in whose ground had so very glimpsed a hollow boarded from my soul with a lot of point and into the carpet. It was such a monstrous revelano year advanced, and told the being on this period, it would be of necessity. We had obvered a space where the primal remote at the hill noises was match. At night, he was very nearly rived; and back the lock of the confused steps, and that my driver may not be acquised not to be seen, but it came to him out of this stone, and that on the foot of this state, a precipitual profound age the supposed through which the distant island of the Gilman house about 1800 of those who stupidly explained. There was no more than the creatures were now observations. They saw that they were even and over the cellar floor.\n",
      "     As I was possible that none of the titan cases, of the ghastly insumfirable absence of the glowing shunder. When the distain wadning on the stairs seemed to broke them far outside; and at length the same profound and carving in his rigid significance had not been so ridicule.\n",
      "\n",
      "It was not far as more than only a chance, though this was not thought to be determined to recull.\n",
      "     Having filled the crins he saw the numse said the hogs had always seen him to his datter. He sprang forthing and frequent hieroglyphs which made the more solution as the treatures and gates were of great gold in different poilto so to stoop anyate. His face was at least a large bundarow, his lifs more of my darkness were seen. It must be the side—a perceiving ten and sang before,, and when I told him observain in that side of the hillent town with the clufty of the faint glow; so that it mas should be not that it is to a general near it at all? I say in a sharo lot of steps. And you know that I have always at disturbed the range, but somehow to move a definitely lifted familiar three hings of ten or flying himself nearly of that power the cult as proved that she was heard after all the great careful perceptions of the mundices above the cattle. And at least he had to do with them, and he tried her water in the Mourt. He had not hinted to grie, its since the party ate stone to any other place in the cellar; and thene with the most intense enst the whole of those boinche roumed and something about the river behoodid his settled neighbourhood. At length it crasted all the take ctring out and staw after a kind it dedicated and assumed a specific harrour, transferred to her stardingmorth as perhaps an actual celemanisched around. At times must be the charned emparmit of the bungalow-peaks, was in no cryptical flag. In the end he had tcrown hime and saw the bluttered feeble flyors and cassaly people whose other monstross of distant cleat in the old beaks had come for. Nothing new those occasional capture and the unknown real forms of any anturbtind which shoulded long ago, and that the consequent specimens always werced in an open space whose cave-nothimat not carrying and exciting any emotion came to a sight again. But what was to live they and the army ouseash-presencedening once more that he had not been arrested by the antiquity and disappearance. This he had hair with his counting as the trees on the narrew and shadowy fighting together, and in the disgristing of the chemical heads setting the riders one to the same starfish. It was the thing of a more sanitating white number than turned over the beings who came to the Apertuarit, and then his starm irentity.\n",
      "\n",
      "THE HAND ONNESH\n",
      "THE NOUT\n",
      "\n",
      "\n",
      "Whe told alone with a child showt the pataless, or person, but I have stopped to an orge without the sea of a day, or there was but the steps which have no bed in all my search. I could see my head daemberwald with her man when I did not part to be there at any time. They comletted the college life, the corridor inother live on the room.\n",
      "     As I have say at last thought I could never have come to a slight and moring not for the less freiding and dessert.\n",
      "     And now-  the members of a subterrene half might be sure to be denced the slave-fired stone or the most dreaded state. We could not gettin himself with Delby Alearness of the sort on which I was encaught for my consciousness of his silent penguins. I felt a minute betone my felt, or in that traveller, recognishd me at once. But in the expression was indeed one about its species. The piece of mind was in a great distress murdered into present, I contrained to a deticher party from the evening.\n",
      "\n",
      "The first of the level is that is the sole personal coincidence of the man minutely began to aspect that the warst wails at night; so that only a curious grandeor of my family had been a simple secret of scholar originally capacious force and habitation—or analysis as would be carried on its distant planes in the sand on the other and the grave and the bulky tilthed of his horth which that huddled abone infertions in the dark, and survived heavels at the end. To what eastern was the least, and we were really catarly mistaken when the strange secret had come up on. This was no point of cattle, and for an instant at his country at the bottom too legend; retreating hollow, over all and disappeared or some evening and evidently distant personality on desperation—and persistently shaped in my memory, frightened a hagn sun, that I should think of it. There was nothing to be seen taking and struck one both my good and a half, and so feelly and little lands. I could not go into it, and seemed to be a lorg voice when he saw the most building of the Public Marler. That is not last less accidentally left him, for it was a party a sort of what he died. A concerning and far squated and shantak, whether the less grown hurried had guessed at all; so that not more human bodies had the feared ageed to the ordinary steen in cosmic college. He had, in a pervense part of its number in the local disgusting palmies on the ceneral guard and sailors to the ray bown in the forehanden and disappeared learning in Carter to the famile. After that wa closed his progress to the semi-or Oshopical parpherence, though they were never before. After that was no more than he still lustered crosses themselves in the dark, and many other physical specimens to the space and the alternation of broad squares, and only i had completely believed an abnormality at the top of the house. Then he felt some material in its neighbours of great results and rattle aroused musty and seasons which the frightful sculptures spoke of the strange ancient tales of that famile. It was the family, after a minute, and was probably tear, and at first he door be able to speak. And all carestelling a port of old walls with a car be afterward revealed, and the steamy hoirs was now accompanied by the survival. All the shore of the party were plainly indicating, or all more spiritiem; and had substantly carried despite of the profuse of despair, that the man was right as near a copyigh and archet gold and hid either torch for some of the mountains that spoke with him, and it was not asserted from those airs. After that Joars and Elwood reading her friend. That he had something to do in with the same discovery which Washington once he had told him what his petsund had given the night-gounds now. After a cload and route profounded lower with a mere condition that it was not only not to think of his filles now. And there she had been half some side him and were hoped. Willett had waiting at it to a measure world belonging it before, and she had been something large experiences a second and mustle during the fact that any other or my dispraiged matter now sought to do with in distant rooms. For a moment he worked of homestica open one by one’, with the huge stock of the staircase, where the the most accidental device in the case of the my own presented and gaining impedions of the sun and most experimentally observation, and I could not report from the ordinary apprehensions I had ever come upon a more degree of fact, that the innited ma merean deeper subterrene science of an entire material in all distant rarrowless could not fail. His tape was, as well that, as he closed on a distant-room and of note of unknown blackness. But a motter brink buttled with its roothinal gainger itself only to odour what is considered any dissatience; but there was only buritht properties or expression.\n",
      "\n",
      "For my outside the time was now abment it with my singular advent. There is a party of the monstrous soul, and seen information, I had previously horrible, on the serune of my own sea still leaping from the bowed to my fare. He was found a delich of mythol general beautihhmen,, without the first, with a fresh, and toncuffer at a few strongothin summirarious aspocuation. I have abreptsally elapsed, or where I began to feel a determined twicomphing, or were proceeding and arrived, and weighted into the throats of the primal mystery. As fire, I saw, as the descearad point of delucting main absorbed is a very singular beauty as I could not put him to some terrible infinitcently oppressive.\n",
      "     The decidenation which there was a little objour empty which has surely been throathing behind the dreared mountain walls; for the rustion of the rattling bearing and draperies I exceed from a delicate entities of dead city, since the man in three minutes of the chest came to mind and behind the lamp. All shill and their story was a light suggestion of four-light to that nute-mountache. She was the capping shotled of the shelf and he had directed; and when he did not popper sleep that they stood within a sign, there were two mixed speech sculptured accostlipsed. The whole section of this corridor was corresponding to the more than a maleval of the immense subterrene shapes. When they careful go flow out of the hills that north was only not that in the fear. The plain was done about the moon, and the star who had obtained a recognised fragment of the more hideous incabtions; but even on long is the clangs of entreated friends—having meant to be the mad Assumaties of the Pastage.\n",
      "\n",
      "Athless the mighty growt cult of small schoon both supply out so safely as the French characters were not sure, out that all was the light generally liked the trial; and as he could not shake out the best words with her things and had sent him to hold it; but it was the great glances of experimental proportions, and there was nothing as well as made for action. The white party, wholly occasionally found both of a marvels which ho enriseded, and once had ever corred this tales of that perfumed face of preparation. On the least sea, we aged in attending down toward the nearest point that they was almost disturbing. At length the spite of old Januaroun the fursime of the horrible persidies in 1915. It was a material south, and worked on his fathers which had discovered and held in at the city’s feet of his attack and began to speak. There was something vainly obscure, for that last have been making wise to come up to that way the last thread immediately opened hamf. The spheres had come to a sinister earth in the apparent air and half a chance of although shaped like a spack and now that afterward it will not have began to hitter on the side. Old Marker, two of the Hill speech.\n",
      "     As the childy paths seemed to be the same paw; and as he stumbled into the right sound of the revel, on the fifty moontate, in the plateau of her left the carve, and then he shut him into his left house. He would say now that he had gone arrested. But he had seen this portion of the tricket development; once in those state or more than his unconscious experiments—for only those who had prepared me—and after the flutterer of the conference, they had been brought up firther under Matheria .n the forest it I become state; and I could say by the stantary which we stood. T ose flygs wrong about time and clumber the whole function of the command, much mad, and can at back. The old man’s descriptions of the fifty million hings is taken in the dark of our legn, and the constitutions well endurened. This was found in that party of tenred lattured among the freshors we had poseds, sacrificed and preservates, hence the old sections who shunned a past ama moved, and whose designs alever shonk arrangements, nor this formid expediment held in adjectional chaos.\n",
      "     Was it not we were observed that the rest sail replied by the hills being in madife and discoveries. Then three lays of that large stour denied light of than abyss was still above us, though no soaped frightful even feed removing a series of deaths as fath as the ultimate catered stones of the piratus were surprisided when our companions had the cut off a shadow. I have flashed this time. He saw that his spelims arm so grotesque. These applied diarys, the others had sand disappearances, and hunch and effects classes to the outspread level of terror. They seemed to chackly the herm the hard girble shones of his fetry and party to see the Other Goods when he changed to him. The horror having gently fuctive that the bluff of the electric loom was released, and I was partly dispraying that a curious such perfocmed slattery mass, and of attendants had taken that other good sense of man and ages—but is if nothing almost some subtle into expirience would not be impossible to recall. It seemed as this portion of the tittering of about flue, the first thing in vain for more than advantage of my personal trouble would be intumer up. It had been close to my body; but had no high since that wailing me unable to convey me. He had after rean maining too much, and I can scarcely knew that a trian began till in the door of the laboratory. I surely died of delicious mine as the man muttered, by the next capts and the man as we was sucheded by any distance impercopbeative impossibility of their forming apparatus. At least of them, although a capture time to a positical experience; and of me a single most growed, no more mine had gone to some other point in the town; but the next seven of his dark surface was still dispettion; and I seemed to have a complete and unmany cases, in my fairh ensued.\n",
      "     I was close again of the formula had not been fidentled letting only the mnotic and farther evenith. They had haved on the source of the thoughts they reteined to him any chance of examine would be all that the fourte much of that fastery must have been found before any other marking- change in the noisome college letters; and so fell capers in the decayed sets of worlily insidistnisation and the presence on the one. We came at last, and as late we had suddenly received that his constitution to do summary or gaicisa this poor, might be, no doubt, with voices in the morning afore some sight of the suncess would be absent.\n",
      "\n",
      "    At length Certain cause conserved for the matter at all, beginning to see him from a second and human features from old Cockorban Situaricies, thour arrival windowmens and overcrows were spiritual. The only approaching number of powers had been the wing; but I had not the less scarabaeus later on the world’s lime to his head which I had shoked something last. That he might night on the head and to have before heard the tower up the ship. He had been over some chimney-citting on his persenvous. It was about that survivor, and we were not like the most depraspit to compare not an easterly catch. At last was which has not been taken by the time. All this crushed his discuss was this. He changed him horsely, but he saw him seem to real agen. All was able to resemble the car become ansist and ashured them. When a wishous tongreusned upon medifica timster—and this car out that the greatest excitement of the same principal assestion is included anything among the family enough most of the hunchaley salation. There was a subtly time enough picing in his sands in terror, wing of which the cuntilence empty shoulderes heavily. There was a sudden visat of the fact, that he can hear and describe a great dishalt fright where he could not have forget.\n",
      "     When the dark country deeph its son mank absorbtively noticed in the State of the missing limitless earnest the great spectacle of Nichland's place in the unmitthed potences of that spirit impossible to say. At last, we saw that a certain scene became visible by the skin and decorative cubbances. The almost-human scream of the terrible-ratting there was something areas of the most horrible discroppings to which the camp was still anmist. Th set highle passess over giving me had accomplished as well those mysterious finity melody had left the cliffs to the end of the cover. It was his work about this time and really took a temporary vacution in the country. The gla or gathward slapping down toward the stop at the black granite stone beard, and boarded me dazly to repeat the pall descent.\n",
      "     There was no have changed like evennt-trais hadfled on that dockous and same merchant and came in which he was obtaining on the suggestion of imagination. These calls reached him where he could not have forgoted his dark sight and slawting upon him. He had petitid all this the longeving beauty they spult above the spare elder brooks that had happened on the way of board around the Godss of yather, but not to believe it anywhing we saw that this whole day was done, and there were sight of the faintings and times of his claws. In the night of his fet barrier dediced tithes on the hill creck to A rail Carrolls at beerioms the highest of the Old Ones hardly four fresh phinosophrotomication. These things, whiters, sit it wink winh as he should occur to the arched, during more than two of seconds—though nearly under storm they were, as Carter seemed to have described home and feeble and fearing the world hands, and shipply dawn in sight of a light interpent about the curved droppiness for exposses brood.\n",
      "     All three had strongly about the farm, and everyone felt that it was not from sixty or anomhed on the wild peaks and the cold writel's disorder below it in again.\n",
      "     This was when the tumbled mass talk of the Great Ones with its half-contacter.\n",
      "\n",
      "The minutest frigment of what I had a tracidy story, and which was considered no managed to use the marige visoon which has been brought oft. For the door of the tast farther held it with its passes to do so far as it had followed, it was only a fragrence of feering in this matter to me, where, in the second portion of the afternoon within the circumstance, by the ship, threughing ig over to the mind's take of hhow hersither of my heart, he fllowed young plocetum to the rad, and at what sea everything surely hand once work it would avert but frightful idea in the side he had pencting into the patily at loss of the carved fami.a There had been hearing the gloam of sole low lone. The whard was newledged, and when remained to be said of horror. When I listened this spock were the merely ancient terrace of a squall, another and more important brick assesting; but it was that of all that was a monstrous ratteric and probably course which we sounded in a carical lower problem of right and look despite the line of their most sight of his plies.\n",
      "     At times it lay, so that I had not like this about that thing will get the melory of your age—the next day. The figure was selenally guttural; but as I had great as the origin of those person were here, and there was no wind. As I have tried to remavk attempish on this particular place might make it as something about the..\n",
      "And for that peculiar point whente this strange first was heard on by another plane, and when still influenced, above all, over andoged events in the mountains about which its central door were discovered, who had once gnew he had heaved her strange column or guessing out of the strange stars and taken them all about the, with ansiet from the gugs to the old with a boad which he had passed to an old peaked stone stone by an ammising. Whether, it selmos, beauted and burning to life in a converies by a chance. To the wall, with which he had given a monstrous objection. The rest of the blessed man, had great history of all kind of sanity; things to his own sea as he went in. And when we went to refer one or the manuscript to the teleparks and was, though a single shoot for some time before; and he wondered what thought of the survivor, his own strange enough rose for ghester than the ghoulishteek recalling their armsic little workmansion. But now his only peculiar cleasing was the most appalling in the cryptic pashage which had recovered from one of the marvillead opens. To bear that the enchaster would be to lie; but even the most called under extreme hazily, or could not exerting his knews furnished by the time, and that these things had been learning through the general area of the outer world of steep, and the first time with the skeleton of his face. He was now much, as he aspected, in the earther arrangeous banker dark to a perform cruckling reach of thousands of feet. The peogle now spoke up at an armories of the guests who came to my senses or complex thought for mind and matter after me. The formula was so steadily in time, and the more completely strange to a desperate and articity at the beings made it considered all things about it were not thought for across the road. The building was not a true mud at once hold only with dreams of having been seen. We seemed to found the execution of all this much like my own remote and somewhat putting down from those prismatic countryside but the objective disappearances of mastering, being subsequently cut off and should have been. At a simple thought it spoke with human coloaran square, and sometimes checked in across the wide pitting which half hieroully tried to failly between his crest and gradation. I now glad that some days rideleds from the narrative of the countenlevat. He would be to be so remote from my fear of the time, thought, I think of my confidence must be alternated.\n",
      "\n",
      "When, now had I put his feelly at the design, that he would have glimpsed me to the longer part of the more certain signs of his opponations. And from what I did I struggled for the side how the instinution of equipal and emits were purposet than the grave told must of immense young Ward should have been ensount all all turned distance from any real singini on eccheming of the plane. The lower devil-wares indeed that neighbouring methods traded to the long village of Professor Angell. He may have travelded the ghoulish devil-seeved of this interest and sensations and opening world.\n",
      "     “That has been ability to see, couldn’t all one ofly utterly devising.\n",
      "     “I was there I have been at a loud ran of some superplital think. It was the sound, or if the gly out of the nameless words—were all alive—out of the man who should about the fate o’ more nameless half-reconner.\n",
      "     “Well, Mick Street, Aman satan,as the Outsides never detendared an obliver, when to distant sensa to his former professors'at Any thising whisper had nutuatled before more.\n",
      "\n",
      "For the dreaded southern had taken it to him from his situation he described and curry their employ, more dirtty which could prevented its final substance and tending ever winged river to regular parts of those—the grey ways were tones in any single ran-ear, and that thickled would cease of loud frightful traces of all dark, subjects westered and bere approaching its whole expedient on the moonlight worns of a rudder in any person with the region early and carried in the face of any standing signs which the windows and other merchants would surge him. His feeble still lived in the Providence and touched the region in the State, terrable. This to make earth at every what could be in our own intermittent and forced their passage with a preciuious visible moustateor of sinister material. He had attempted to stop me as a gradual and among its delocation; specimen-blowing heads of murderers is indeed rather northerst while acquaintal stands of earth—was shocking moonlight over the stars on the boatd, and there was something about the mixing inside. There was too looking this intelligence rather than anxious common later at ail. I would have to thy confidence of what I had deneed. Ifter a single mind of my own was to find, screaming and ready to my mind. There were atels of the wild rides of Arrenay’ before, the top of the told hinted that number of graves had been seen in the Archaractic noisome as Young Worde wan't strained. Gilman hust, dow his visible beouth an antiquere watch because he had not seen some history from the abyss, and the worse thought about the stone. His face was the silent starthing of the great stone of the Goede was more farty-like than thicky and worthy one. In an instant alien the short between leagues of garmous soon is the cavern as a proppect on our marking- which he was noed. They had too considerable as I went by too grew holdours, hence, flown farth, and the first lateral wanter he would not be heave from her. The next day commenced to call the camp and started on in the red mystery of his house, but he crossed the little top of the house, we came to linked at the blood of the sea. The formulae these seemed to hard the arm with even thought so terrible than starting that they were not all another point of the recensere travel. Not through those plateau’ shell, but heard glimpse of the mountains were now my missage. An approaching queer coloniage for with means of real cande,, and did I was thus attempting to understand that I was now somewhat, and all through elder towers were complex about the world.\n",
      "     That was the leaves of the fire and the firht, and had a final talm at the end. I shall not see the point of the stups, and lighted with the head and the course of my companion. I think theo for the fourth dogs for the antiquarian so familiar. The sight of which I had been subtlered by heaving the external rattlory handing arranged upon his ends. At first I should he often the clammout public sentheran age indeed through the ladyer and merely speech of mankind. In the floor of the entire types of titan places, where the reason which ever believed men. There was an old position in common to transportation and seemed to be torn off, and seemed to be absorbed by a going of ittering sound. This, I reflected, must be some time because of the pipe of my dreams, in any possibility expected to practice and fancied by this way into that cryptical studies of any-conversation. If I stirr sand to me than the evening again through the aid of some thing at such corners. The face was over the countryside for some forming laboratory in which all my ears was starting away. And what he wanted in a fattered, unknown town waited on, but had not the lustrousen not one mouth past all the house. On sharp to the lose aperture was various of the embarrasmed thing; and Crumbes with its lateral appendation in the sades of the savegripe of the schoolmous house, and wathing the window as it spedcel. That it was on a great sparke starred bank to give to any land the other course. To the lunchering of the Stude Sound, and the singular stare, and patted down the picture watched his eyes and showed to the former new and primal ison that marked their soul. It may be that we could not help fearing, but they were in a sharp tent with a few steps, had flashed the number of the gugs.\n",
      "     As the man might seem to him on the higher lime, though a sight of what he had talked in the windows, and he had left the bod easterly on the window, and his preyence had sufficiently geartely. The sound come above the level floor—thlith on the other one great right artire sound, for it stoupped at the police farmhouse and struggled to spon up and drop enough to shew any work with him the foarded-crowd with a high policy and amplified with the magnitude which made me as I did. It was visible on the same arrhad,, and I had no disticction to that time, came to reflect upon the surface of the entire revelation to his way. There were something of any obstruction, I could not be seen that my proceeded almost surgaly a vast deep is allotances. At the end of a bottom of all was stone in a serious and incrmasion of the cavernor and unhappled with alley had been stepting and listening even that I made no scanco..\n",
      "     The natives and materiol was now and then a distinct haunted nurse. If an attempt ato attempt at such minds are public expraining to look upon a malign notion of workmankhiading this things leaped for the marvellous chaos of diameter, and had seen the beard and two hermits. West had been a singularle abandoned proportions of a crumbling wall, grest two of these left of the room. There is now, but of any commerod by the meanancs where the case on the land child’s habal servants. The clay was as a past affricted and partly tried, though the name of Common in 1747. This, he came in day before the boy of Lake had come, and in the evening I have tho outcouts call to me a shriek of being common buncher and fell in a high pare to get through the arth. All soon crushed to as the horses he had seen so ready for the other spectrosity. It was not, on which are to call along the street should gather the reading and triangular street; and at some time I thought of the stabaide and absolute monstrous wooden particular water, or of all the subsequent dissecting-paths and chasms and myths from the solid senses which could scrate of him be in hour trace of language. It was very common in all different specimens to hith on our species have the tendency of her father and harres. Of these leathes of alfote was pressed for brood now, too, and presented the authority of other roads. Looking upwards in the brig, and seized and duffled before there came and haupening. The tops into the cot might have been to the spring and the shade of hollowish-gestial fragmts and other buildings that had ever seen a carven features of the great catterids against the wand. The whole indesition of existence noticed it when he had seen that was never been the one subtlabieatity. I had frequently showe to the steps toward the lower house, till from the minate tormulee of Dr. Lobia. Roweed herd from a bungalow with a kind of relatively exposuncation in the consciousness. There seemed to be no meritable of museum-present and instruction, and even such a sound coming down from the cold tabert air. It was noticed what we saw on the habber and at once; and even when we resided to the strange stench things to hinter on.\n",
      "     One marriage were a priest horror beyond any mistake, and only a freery and manifelt upon that look directly.\n",
      "\n",
      "I had must go desaly in the abyss sent more at the time. With the meaning I had a texpication of something hustriddly, indeed, she stull into an opportunity of lifting in the company or matter or a second of the country. The grach previously being insisted us because of the intersitibulous coming of the creatures we had left for the new or some strange colonial fashion. When the principle of that cloud-like parapted brock rouned the firm of the men on a dark starrating. Steel and tleesnow Armitage considered his prehence and beginning, but had the letter from case, and the crates must have been thrown on from me tasked. Its struggle had been a line to mention him.\n",
      "     I had spoke about the strain of my prone so farily change. We were back from heaving in the strikt a guiloung sheed to those and utterance to the spectral intension of a thing better. There was no speech that mustake shoted out vegetable upon ose of the matter and give them the lear of a bare accent. The one through which he saw through the himself well ngised or the town. And I know noth of it. They was a scream of at liberty—and these are always possible to the cottage in the men.\n",
      "     Then, is still a long,r the garden gold with great lantern; and as Wesp wrisgled in the same stone for the fungi, for there was a great deal with a marvellous small, but those exceedingly ancient machinery was evident that some week of secaral cases and odour. To the eriding full of his first external case was none the onyerest natitotoo, which had previously disolded from his public leaving to the cooth had not saw that the man impossible told me. Ifter answer at the experiment of substance at once, and then had so dark as the stakewitg himself concerning the hill notices of the Other Gods, that surface, bore in a colour, and a carven pared below her fatil concernation among the dogs that proper heavily. Last opened the largest of the Great Race’s continuous profoundes of the life and what climbed the spring slopes and chastly after which those hardlified clammeres had been listening for the utem of the glow to get or start into a damp. They had taking the chances of his struggles with the great slate-eater, all waiting to see the path to most fro in the cousty dissucted advices.\n",
      "     All that the marvellous stattered man as they culling ter steps, and was supposed to see that came with during semmen a huge dog. His voices exposed him to hid of our splinters in the black cattle. And as ha flattered senses. In his short, however, he perceived the whole gareen the primal bas-reliefs, which seemed to involve a repressing persona in the land of Curwen and Hisham. The sto and he said that he was a vague demonstration of latett agends. Those shapply were growing material, and I saw nothing might felt than they might be descended. In one point to haste it is the modern instruments to him about and with the skin of Leng. And it is when one can see the outside bowl of sensitive centuries and bearings, the greater barrel and time-containing manking had set dest to link more than two passess. And as I donet that the sound of the cosmic specimens even one and a pallod cutting world and one stores of wharves drew near the end which had come. The horting days are not even six seem caused by a notice of its ancient records in which the slanting further mumals were the fishered and inner walls of a city, and Carter was now we could see that of the burst, and should read in the silent door and were guidane. At six of them went tor use for infold to take that primal antiquity whispered out of that cloth; and had tome walk to lee on a fir thin matter and loathion. On this one black earth stationed, there are commetted circulscriocsts which had taken the outside by design, and I see nothing at all. I perceived that one wind was no matter, but he saw this slene, fishy-smulled merchants, and spaned almost upon the clear oddure that the youth was as a still appear; and I could see a letter from anything on the ground below the hight unclosed and unprecedented wood of northwest. Burning of thes struck the floors and designs and examinary many and useef and general point, though the construction mulders had heard beyond dougtft through which sat had burst and frightened him from imperfect subterred me outside. About men he had known that this most grotesque and ellivion burntour minds and one can have no poig.\n",
      "     At last he decared that a school tradition was so far in the banks of the steam had been flaint so violently among the Great Ones. Then there came a huge fixe; so that a thir time the tables on the whole about he plunge the ruined gut an authent which crossed him.\n",
      "\n",
      "When they reared in the dark, the camp of the centuried which I had prayed for tho samptings of the curventy to be, that of the atticl bounds, as I saw a matter after mortal inexperiment by a million yellow significance to the straw and brittle current of the farmhouse. Through this daning san of aperture, many sures now the scratchings and let the water papers and leaves of the stone with its cold. We had a careful gentle of emutial cryptical features, transmatted aweay, indided bot of a matter down to the straiget and every state of threatening. But they would not clear tercestrouply a horrible more infimitive dark mass; but the first time it was found that my uncle had left stated in the morning all the primal monstrosity was scarcely expesived. The name of the clouds this time the gods was very singular. Englash as a whole abys below as had returned. In 1927 a thought would be it at the centre of its thirty-five. Almost after a line of a pair; the abyes from the incline to the contral and leading to be of a glow of a former thought that others should never have paured and warmed ever since it.\n",
      "     Then the sal and human cletry of the steep tentable country shewed that the current received the sound of a very strength of something whose colonial aroused men, and there was some peculiarly sharpening alien to the new partious world of the General Sspeer. He had thought of, Count into a stream he realised and in concerning all the streets that the spring of the manos fell or such courses within the room. There came a cross crose than an aurorous course seem to be in it. The tough gibled wate men shall never sweer—danged of being taken in some terrible and uncomfortable profound. From the peak were the trags as if with the stars, allays fragedness on the farther snow on the room. The shop of the rest soon are at a passage, but nothing of some indispused merchants and a place of steps with a land of cursed reply, the same explores had rounged him and went down at some excuted pit.\n",
      "\n",
      "II.\n",
      "\n",
      "The feeboring of the latter still livid all the excepre of the cuntinguish and the events of that scanty selves almost claws and blackdeline. There were now and then a sudden example if the once collegs, teering ba strange and of all the globise vanus as well as on every hours; but the latest difficulty was observed us in our present station. And who have really seen a proportion of the carge of the hill—and now a mullity they had been long again, when the other wandering snarles and odd, and a large musoninc od accendrated twilight he had set out for the stars of his way by the calm, And that he spoke it without mistake, but there were some thorough relatively lasting of eagerness. What trems lapsed at his pleasure arrangement as my readors.\n",
      "     At length I believe I was coming to Akeley; and he went out to my book a fright, and at men took the panecling to her attack myself again.\n",
      "     After a minute exposure if I took a path to closely and could scream our report cut, my single looked confession of those magic encountered me whilst in days and horions and devisions of my fellow it. These respects, the fashing of the shrelding oil--and I had no discovery when I learned from other things with the lamp, and noticeation was screaming on this one of a building which he had had more tellinglythed to his fach --and as I descended a new mountain that she had dreamed of many lateral persons and making in sushance from this moon. The supposition of the history of my deperted more spilit is conscived. In my corderscap it towered any further ngigler that I did not mist the top of myself when I felt I, but that which I had sound on the southern hall is a namile which had shivered as I had heard if their sinister unknown conerest host. It must be fratten from my side and ask the tright to dr. Bum then the night mained eighty to harmon supply. Of the moothsalo was the bast of the three pendains who he humbred cure and changed him to describe as if he did not peop so hear in the same corn rain. After all, it was thought of that tangible way that hand frightful suggestions of abnormal matters has resorted. The colour of the vegetable watenfiled ancient gravesting spied in the Gilman, and his whole great stream of device would have truly selioved to do. It must be likely to fell him to the most teoregries, not being abrordly bewildered—by the seennofed in a state above his ancient and corner, and at length the most stored tater still bursted they crossed. The piled desert on the table now saw the signs of earing temporary lower lines of ancient galley reputed to say and of his slibe at Carter and Carter spoke after he finally feared.\n",
      "\n",
      "The world of the old would the sea chant through that offund, for he could not pass the way. This was merely this distance, and saw the black slope was interrepted and finally completed. We had nothing left aland all walks and presentisms in the distance; but it was at this scattemer another to shake of his face. This is a britter like those inchusts; and since the matter held out of the roof base to the wolled on the evil myrch on thear around the simple panelling abdet in a cold streat a base which must have could probably explain.\n",
      "     The fury tall child of near Mountain barely described, but when he had left no slepp was governed by the strange bodies of the guose and through the extreme horror.\n",
      "     As he thought he had searched or great liberal age they saw the ground and real singular form was the solid rict of some very curious absumpted south. He had flinded and bury up from the hame which should came down and accursed the tough square, when I let him unsoletely he wus blittee. He had not been long after the first of his apparent ancient familiarity half and in a mannaged in the million years ago; she illisten newes and mine in a little polyctias and assertly displayed into a streat, in various points on the stundy; understanting out of the barrier, the present actually near around him well, silently sufficiently carable to transittones. In another doubt exceeding slowly, and we were looking at the top of the master, or of those periods as the fact is which he said about whose extent or his new resorting times, adoption to the potretor of a distance. A terrible whisper was not that had one bey while at first one exposed the next day were out the ship at which he lay on a party of traces. Of course was the southerly most strong stibler and slabbery over a pair of deseath- not to be truly a feet of probable end above the pain. At a sing of notine sounds in a straight indicated central active syllables by train a car that the yeals reached all the walls of their anceent worrred. There would be odd descend and heard the rud meanw half been a mithtelen person though he had not his hands, and that the dre seem of railes of calting gaties and crazy and ireful connection which seemed to hartly conceal up to make his solitary experience. The whole trie of its self-suspens earied to back to the tree in all willers and mouth later, and feared to back the ecross and discovered outside me. Now more recresint that the mess partucurally up inta the sea, and a curious land attention could leave between dogs. The conference of the whole shore spectacled    I had been a little forthtrating into a slight scream of mind by the range of the face of the party. It was only fain to a houre, and found a purpose, brought a broad into the land as we prepared to buried them. But the light did nothing but later so culling out from all the streets or the summer, and had been there, but the futterids must have been.\n",
      "     On the first of April, 20d of the thing which was his party of Cyclopean with old lastic arms, and with a somewhat roofering at the one west wild and hidsenfus; and this he writed their purpose, were sufficient indeed to heer. We found that these monstrous party were surprisingly coloru,. Then merely was turned to the red curious stone of the dream-buried, which speak of an ential ancient profound silence, that he was a more careful secret consciousness over the complete living in a side of our day and introduce an abysmal more degree of intricacy.\n",
      "     Then he saw the monstrous place, though I could not get the thing and rented another. And yet I were dead in the burning vanifeerctif from that time and w dareting some of the mountains which I can see. Only few apparently half-feceds which had disliked some of the motorst and mouldy valleys of the point. At length, we flut and really stumn to his house, and talk about three lengers from the room, and soon speak of his cellar and debris of the occuptive and hidden. His galie  could be ascended for that space, and thrown dangerefs of him, seemed at once the revolution of out.i He had seen in Their stlengnhsly stones haden and with him a highly effective possible clouds and bages, and he could not help several strangers at once again—the facial portrait of the nearest sockett brought about fifty yials. After summing a doubter and intercritable construction, and screamed during my vasi usual committic consciousness with regenties for curious. Of information was opened for the basart, whereby to thote of the photographs was a fact in the purpose of the great words, which was seen or another was born, and was recalled by descend for his purpose. Such was this least to death cumitaus, and esched freal lintleswitts through the small police. The thoughts on the apparent chill of the redicie’re certain signs of all his own single powers of baseless or ever again, and what cloud they the streams of his mother he dreaded when they reached our door before.\n",
      "     Of the fine thing was sure that the doctor had seen, and the fungi was to say. This was it in tremendous fright. That they must be impossible to felt a menace from the speculations as you see.\"\n",
      "\n",
      "Marches, did not, have had only to start from its recollections that completely seemed to satching at last into action or every ince a strain, he could perceive it. The whole carvor, getting oned attacks the party as it cremined to attack their speaking. Only a few odds and aeknor how miles of million years ago; so that it was the graspy irlevend on the rest of the dream, but the lonely damages and chances of the evil persistent confurion he was left. His foundarded considerably we had a committed sounder, since it was clear that the year should conceivably buried him.\n",
      "     There was no trace of the start\n"
     ]
    }
   ],
   "source": [
    "generated = sample(checkpoint, 50000, lstm_size, len(vocab), prime=\"ONCE UPON A CREEPY NIGHT\\n\\nAll saints eve magic was mysterious\",mode=mode)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "I have never been so afraid in my life. The trembling of the legs is uncontrollable. I do not think I survive tonight...\n",
      "\n",
      "    I know that the eysine geometic man of my moon about the woods have never deeped true beneath that long man who looked again by such a personal impertince infleeding. I have said that the twi in the figures of those whom she thought of dead would have been filled with companionship, and we reached the first shape of that frightful altitude for such cases and of great continents, having noted in that temple or thick to college despair; and the singer used to with more or less experience, the staring eyes who had not suffered darkness. Very sering that the six hadded deep rate was sufficiently force on for domes, and the first tenticles of that time the secret possible made friends of mummues it is possessed of breath. But in this time there bare ampendons of the Great Arches thack aroune us, all after the old woman and the hundred feet brooded about how she had so naturally hadmed the awful traped into the stone from an arched garden. Those hours is so much impossible to seek teo, and how that heart was the last nameless handncy from the right shocked eight ending of our orable footprints in the lowest primal waters. It seemed, to ba sunset fish things but days, out of course to say no home. I knew it who this in there were things but life for, in the tenentom of their death? That many supply of my brief -- as sate as he comminged more fancy, the first than I saw any sunch.\n",
      "This driven discovery. In this manner I springing about failting seef as thick, and, last suddensing, and as I gave him at a momentally physical and belief in the air. A certain period of the thing, who had cleared all this time, as if they were about four feet with which it was on any matter of dissame. We were now dead in the corridor, there was a hundred and terrible and arrangements. The same, now too, the point of dismatter a window shinted and apparently intricate, for the mystery might be excepted to a sort of subler luttirs, and marvellously covered with fright and embarrassed. The heal formula she had survounded only even was restored to sometimes about twenty-feet in death.\n",
      "     The sensation of Certlist Revich had dreamed on how the first participation had let gleaten me from my child door of the cause accurate—that myself was speaking and roaring two shorts and pit bloods, and I should think of it, and what it might have some time looked over the boxes, and lighted in that stone which I had more. This man had teled to see the face of the book and in the abyss beneath which he did not like to think all of his remnants of discourse. It is the message and shortly afterward I began to gut forta off from one of the more shadowy overhead.\n",
      "\n",
      "I speed in day.\"\n",
      "\n",
      "    The next day, the matter of unisenses were shewn the meaning of the tall, we could see that the death's-house is and other house; and it seemed to be a little to our fook. There was a giving period of life in the apprivaiced man whose dark gulfs of the horror of the Prison Harros had gone as the turn as a very trade with pause, and this was seen a careful replated triam. He supposed the once more and more reasonably much successffollower and pirating hast faintly.\n",
      "     All the streets are doctor, and the stench of eyes, had been at once to let him to come. He had strong following the busy, and he proved so much a real of almost unearthabated thing. This country, and the others as well alone, and the confused countryside took the same improvishing officer with the chief amounts of form. Between the tree-ternible circles of slow clear passages which caused me with entire murcular strange impressions in the side of the gods.\n",
      "     Then mishing mes in watin, for I had still truly seen once a luring and a very showly mercial face, but not lan at the original perid wh shool finished about for fiency until the hideous whirl had booked of his wife; but there was no steady as told. But whether any poring of his planets had lived through the city, and had seen at interpals of strange knowledge of all the forms and the stone with a colopsea east that nameless horror which I have seen.\n",
      "     In the morning, I considered a wide permanent evening. This I had a story from sound of any selicin looseness or momentary influence, he dirliged a criple our insistent or animal as any of the motions of excellent thoughts withhrand. And as he did share, the came on the shelt of his feveresseots which distinct the polar fright. With the dreams of the ruined black bracks, nowning water, down at the stands that a fire was fortative coming for strange soldies. The star-walks he saw what the drile master, expraiding the great distant pawh, and all that the savages which screamed about. As he talked men directly up and down well remembered by a brief and portentoric latest days between yigning any things in the league for some eight esteeabarrs. He they had answered on their simple goings, and are forced to stay about that formal which the hideous neck of the great single stars and fabulous tense above the altered place, and with a great paralysis and labored some of the most garrel disturbance or creatures. Then I saw that his whole spoke Iriled malign antique and uncomfortable fortuee. The brooding course they had been at the counterast for the first time the limits of my own ancient roted torbors of man in the ancient Morell  faminis that we had not been safely three mysteries above it. Armitage had sumpressed evilly thongor the abundance of some object which made our rained things in which the dogs had been patedrabeat that which how urise with the strange mercyff and hilling embtations of vigitanis and temporary rumours and legends that he had begun to creach our ears more than feverish regularity.\n",
      "\n",
      "    \"As the preadurerog not lying on the room where he was strange. After the frightenest of the long girt lent theor there curious iron sign sometimes at length the current of the general surface, and those who had perished himself in his arms at the most second and moderning beacons in the stairs. It was not those hours of the all about the some of the Old Ones from nameless forms of any condition of its possible river.\n",
      "     This was when the stars was now and then the street store off from it. On the end he had, no  the stream rhited earthrated—for it was the pattern whose opposite connection of the evening. The cat made no season as to exact happens believed it to the madment in the solar system in whisters that the southern sad three feet till, and not the solven desert to whisper that he had seized his professional croaking and discovery. He seemed to fail to go down at all sides, and admitted on excellent specmachiats which seemed to have been driven us, to since a chain of doom that new their intelligence was within. At length the decorational spick walls, now and then such a doubtful geotly can be-perseased to line our return, seriously reserved to the fears mere fallen surface of the night. We had left to ranstoine the record of the hill, and must have been activily elusted the belief in this point, though they had not even thougath fear, and finally he was his first excited number of Prescent Stone with its relief. They were the crumbled was the path aid of some strange ancient inclines of throat and immediate agony own bor. One spot we were common somewhere from Part on that trip to all his secret and college. Another reason whose out of the sinister ancient magic was buried him on its shore.r Curtain of the other attains would not get to adowted that some minute-familiar had found at my ascentiating the very leady, and when he completely sure they were not correct, and was this memory all another was not to be seen at in tense age the more complexe nitratustral and slying aportion; but only a latter could have been intolaced. At length of every sol descent there was many people sometimes to be mere and little familiarity of the dead stones of such a futierra in his madness. My feelless drew out as a measured lany, and was store to let him far away in his frightful certain things; but the horrif indeed was a secret starta-backs would not have done when he saw on this subject of the desultoring sculptors below, so many reasons for some emining must passed. They had always pettined to strange men of this and corridor expedition and atterpation the mushumation in voices and workiag. They would take those others, but said had heard she would say nothing to think on the white fevice with the sky.\n",
      "     And you can’s faint too much seen that the shrubs beyind the two prodigious magicians and accounts of them was very definer; but I was now remained of the ratco torthine of my power, and shivered as its regularity had dived even the great pall of it an inexplicable house.\n",
      "     If the stars sended before the silent passage through the street and looked bardly to the fact of the sea. I could teir it for the three minutess that gleamed reseatches for him. A horror had been a curious relation to the eldritch claw of heaving sounds and windows stroke in the sounded person. In thus carefully fearful gave the scientist again the rocks.\n",
      "     Dy alien shoulder not geed to do bodel the slip on the stairs, and as the One noise fully made at the vast ade underground will permit; and he seemed all the curious and father and more seized of his contract could not suppose a travelling conflage and confusion in the any malmen and hint of thin clutch of handwriting. And they had travelsinger wholves about his clatses and changed and shiped from his entity first and thinking of the dark street; and it would after leave his stuff lie to the lower rooms, and wherver he was all aware of his stating on earth. So I cannot describe, that the creatures had been there, and he could not have done that who had set him hatched him before something with his manner houses. Willett being a preciuice settled of place he wished to be proved to be the earth and began to look at the speaker into which the planes and most of them. One men in this part of the strange pieces were low, and were how to concrete him in the mountains and had seen the faint great band of dependent with three flight from the great city of one of those whore burning a strange minate to leaper it. The stars was here, and he had glimpsed starting, and the whispered high words would be sure, about sharp coats, as if the chuck might be assembled, and West derived to cats up the hillogot compass we could see. Besides his child and dangerous clungs from the stoed stopped ceastles, and Carter saw that the dwalp was blow. Strange and sunten walls reached a shoot shoke, and some of the fightful truffe sort of bird we hoight at once some powness of eighthind and all the presence of a paused interior which the Archaean streets were left us for the party to prevent our casually explaining. In the magnificent characters, whose dark stupor towers apils much that something or in its readings and cased only ordant though far in the dark.\n",
      "     In the marrete, and a pale of soundages and compagions then brighter on the right lone whose expedition had found himself delorimentour in the river, and the read of the watchers had taken strange forms. There were suggestions of the malign mind, brought upon our sould sape in the blocks and the silent stories of the crypt, supply, and now a diminution. The sides of the chambers the old mass can been, and there is understand to bel. We had never been slightly clear, and not an oph attempt to rose him instinctively. At length the upper region of the sea was getting blazing, and when I about the line of Inganok hus assumed a short tile after the alteruation of a more terrible horried transion.\n",
      "     I lay forms of the motions, and so much to see my plan when I learned that my portion of the cryptic words were assemt. I, not instead of my trialiggt profound agitation at the county, and lost my attention to the wrickly shapen pass of mountain and scene in my presence of mind, and of every months (when I still bring something of his fear. In my companion towarden the beckusing of the frantic precipice: which I had mildened into every species of feet. Almo the strength and assembling objects were parelyleg the consequence of thought. Mensolitips could not be the people will partious so harbourid analysis, and my clurgy had not straight anyone but of to the rooms of many consciousness which consciously like to mankind.\n",
      "\n",
      "The fact that I could not help feeling in that trip and scratching ataid from any tumbled apprehensions of its utter ancient. My conclusion was almost plan, and one deep inderitibely mooultine. It was the name of Asenath. A lightning flashed out of the great watchds of spocked the barrier of the damled-roof of that dana deaph and stater of strange cities, when the rats of monatchery eromed was brought to his crowd and solete as might be discovered.\n",
      "     After an one lest such doast is the matter of his odd passage, and he must be seen from a levil modern ty wholeyom. The chounder of the left though lambered nearerty, and absorted the notes had living and to be remark. What had happened, and I was superimposed the and underscored, and I had a chimned on the receptian called forms of many of the complex and lorthrously chanting than the bright-closhes. These evening I was locked with any leage of former distance from out of the party to make my sleep well. A singular ashoristmit was a herved-looking mass, repart of the method of his very surviviform and a godgen cat, instracted my alteriaring me to the sound of my own managul.\n",
      "     There had gone no more. I paused, and globined everything because of the three years before. Their principles are not more than a house his light considered by the maddening of such countryside. He came to the sound of the great grandful affor, and a group of them were now more and more had to call to her own past and fastening the tumbled back of the head and the same from the scarrey, and prodigious diftertal beneath the law to any abids of handering; and when he did not like to see what had so liek a talk, and after a se might be turned on the forest and bewold elder moon.\n",
      "     Or the furnishing whisper wall of course he had afterward seen once all sides of the green in the alternate and the coming of the lone destroyed night-gaunts. The whole atmosphere of the taverns were clumsily to the reconding turret, and had alone, though it was she was forced to still shunned and magnificent reality. I must have been remnants of the accustomed man that my friends was as well as all the mystical maline exchamberes, and incredible although five hundred of the rooms and odd companions had begun to collaps of their maniness and more sharply discussion of the great beaching on the pre-Combandian totes in the space belief, the horror in the upper world—a sort, or was now a carved lines.\n",
      "     On the thin ancestorathous stand in the grotesque commence was certainly treatment, and it is quite diffarulty. Once we had sitter and connected with them insceitcing beyond the groups of his slight excitement; so that on our party stretched at the flood of his slime-door. With the very lows wild roam spectral above a few were seen far behind. There was a height of the rumbling which he did not something behind it in any corrasroor with the corridors which cauled to instantal the submerged botles with Carter with a marvellous confidential mark or even a frantic now enough to be feared—with the great structure in which we could do not than the earth in his last father at the point, or the fact that the steady harbour so deeper and suggested bath that they saw that it is still sound, and will plain really had occurred, except for the puss in which he was begun to have seen so many records on the horror and unsilled ceasts always.\n",
      "     The speaker was not in common with the definite and darkness, too, with a classic entrance to the boat, and which had spoken to himself; but it was too like a thousand consequences of life in an excitan of gobultion. The dark pauses sent men in strange views against the others to discovered when the night the speculate body exampled. They donetr over, as well as by an hour, though the consuction of the tentacles must have expected; since it as the old way with what cease was of any death. He did not commence to cause the back of his home, and a thing had forced his way in the soul to my heart. This motion wing hand under the streets, and almost impectlable minute youth, and half strong final or attempt at a few fangmoss, half-answered by the green hills being at length to avain any consciousness and that of all which attributes to the intensity of men of examining the matter of speculation.\n",
      "     The news appeared to be a condition of planet; and I would surely be one of it. The strength had struck up the shong distance from its species, and would be sufficiently probably to asstrute on the matter in a words about. It was now a partical certain possibilities of her few foathing, with the mudderous inevitable farmstic particusares, so at all to do with the thought the consideration which would be relative that Inging we found the one man in 196 . all neither removing and freely to death. The base of the gods had been done; but that gill where still from the sounds we can dok would be half or almost her and trosish. The caution of the faterish could be seen that his whisper is here to be helt, reported as be to pass time to the son. At length the clock gissed the throag. And all three or to foothit—dart shelves on thing, and it had carefully known that telespoding in certain specimens outside the possible earth. This man had the best brought up. The same message was as a matter who would be likely to work its earlier dogrates; though they came to the prisoner to mo strange prodigious myself the innt more convexse. It seemed to be the proble of this intrecedence -- that in the side of the room at the business of the book of My name. The present kind of adjection so far as to the remoterantar still instant. The gold man was best that of the state that ever felt about, something which caught out to be a car, and he had a visited upon the haurteming credul above the practice of the buzzler. He had been forted up, but this was beated fr. This dame was pleased to my sensitive to back the records and reading it before.\n",
      "     We could too watarly that I could not account for the appearance of his host, and with a single middle of accountanies finding it as much discussions. The first man would breathes me. The middle age and emerged calmucal and remisfuracee, and in shade and estard clutching trip to all the earth back to the man, and a constructure we felt very dark, and the sings of course extremely spirit and had of enfectrorated implimants in the reality of his sleep without to ovin. Our precautions are experienced but a soenci in the common almost housed and displayed the great precipice of those inches which likely touched the bas-ring of their stars, and saw that the sketth on the one had gone see with shore. At length it had establed a compass of madine legends, and only the much have felt that very like tried their misssen whose bending yellow stupy fad above them. One had a very terrible conninion of his shninging subjects by enthanked stone within a farm in the black mountain river below, and we could not gut back to some we would come far into that strett-howing passeng to the sun. But the rest of these state eleptated a dead body of the bearing of a peril, and was entirely been advised to assumed a crumples, shapeless black abyss worms seemed to be peterally over the city, where the fature layder in certain small hours of my door was too compel. Then, surforca, and outside, all of insailors in a long, letting high protrattion. They had taken the that high words had been any corpier which had humanssical darennss easily explained. In that strick must have been patterly right.\n",
      "\n",
      "I shall be from the tall that we see the only toward mee in seventeenthere. T  silence when then she had been fitted on in by night, and wondering he would seen at that paper it had been able to ascertain. It was not that she was seen and talked in still too much about the idea of a dream-black man of crumbled doorstork above his shots and furliship of Charles Derter Winsade, where his house was after no attempt to convey me. The prospectoof half was madiaged at a late from Bellows which his pathenta had begun to death on. It was permitted that this west considerable looked upon it. But these, the grave, of the side of the river, and that those who formed the beast of himself. I had not been in some measure which he could have from the attic, but he was conscious and sund intelligent explorer. And when it was more than five human spoes,. The clammy professors can get to get away from the matters and alout the south, and the singular covered extreme mirror with the local mountains of Ankmalian lofey. And again their ramp with the town and dogs will probably gained arrangement for their delessons. He was at the side of the old cat’s distinct pleasant trip to New Yell, as even the document was fully reluctant to advanced the age of the protest. It was over, and till that all was still noticed, and for the library of Mesmary mere filseoton would have to detect shidencing of my own.\n",
      "     But when I did not know what they and I knew the moving world about the staineas moonhins of that damnable portrait which cussed that only by the form of my its. It was me, and showtly I bore my form when he still all about the rest. I resolved to kill and wonder that I was inspiritually to understand with my own.\n",
      "\n",
      "In I dropped a state on the night; and now a series of enigmant could be; but steeping of the tentacle can be always over the weakers of trailed million years ago. There were answer le surgical aneauries, art soon but looked from the space while dawn was suffered to presure the escape from the cry that he had formed those he could sunk from the altitude. He was never stated to let him back to the caverness of the stars.\n",
      "     That such was their course prejecting those incompletely horrors in his less, inforitiously fold in the spital cosmic crabbering over a moment’r last most recent curved farmhouses in the sheggnhs oddy that I had ever been. The nature of the awe-of sinciche indeed considered as I premited to undoly the specific spring. The morbid habbest builting of the track of the walls far between the roads and shallowed magic man wall get to the other and loosed a waterfall to a cold spectral wholly distinct; and Carter saw a strett of great singular curve or river bones whenever he had clearly found that the conversations that the natives came from the precipice and half sheerly abyore. That trees to keep the roofs and the river. A them as they received a shore and archaic private and ill-idrance of the solar system to certain of the old man who had seen a passionate delirium of frightful department. It was then that they should conceivably resumour emitsed beyond the river. The pit home exceptionally by the steps which had turned along a results of profound subjects; and as the fire of the lighter seemed to be of some creature would never even promises—the blackness of water from the well-kept sadgeer in the soul of the region with which the carvings were largely before the markings unhavey an ideative remeaning office, and his partner was at able to recall the probable contaction betwixt the house whence he wished to several dreams.\n",
      "     After a widd security at the house the other beasts had left at all to answer the lurchers of Dr. Willett's change assumed the Arkham strencther south and haraoss, but that the laws which had taken paddn offic. There was not an idea that he was a checked crisp, dark streets, and the morbid terrific latter, and his party there burst a great stone city or stare and crawling in that raven a thing was prepared to combine there. When the right had never spared any curious artimly at the camp that sailors bearing a dust transported addinging to Charles Ward's face attendion to the ruins of Old Whateley whos throw like the principal ancient tongured southern sick of the gleam bore an old markid the only the high-prieft level and wave over the main black galley as if they came to a his own feet. The impettite of the cover, displayed with something we have heard. The old manoter satisfactor stepped and studying in an and or tonture of any continuous mountain in which the northeast had the face of the pursuit. His hands, loting on an edge and flying, a seat of the ancient portrait and the breaking of the condicted walls. Of a greated moment, however, there was no proper party in the deep. Men in the more past manner the croaking began turning against the stars, and so faint that he was in the slant of his loams. Free whole impression was merely the chimneys, and at the same manner in which the torch somewow his face in his contrived, as if so had the obtoines on the lofty store to the Gods On the Bowner I had listened. Startlin broad tringstly, and for this this man had something done dur near the pains which had disturbed.\n",
      "     “Bus anound nothin’ to be method for they did’s never see they drive about the dark one. An’ then s still kound about everythen  ’e wantin’s at fer blaous time is. If all kitstem  in ’nex morning—an’ the hurbling gath teeps as had help their quest on a feeling of any loud shool—that the folkman had a visite of the Curwen, but at that moment, non tumble aided me disturbance. There was a possible animating, some inhistening and fearful palaces—would remain under adjection with Mr. Ward on the dark aggestowes whed West’s room in the gulf had survived. At about 12:20 arthic liferations or Good Tried as many laughed too flow; but that it was in the light of a single possibility of recovery. Then, my face and the fact is that my own hope to constall wine of secret. It may be sure that it was nearly so many a climate which has reserved, so I tried worsh in my doors. My uncle had read looking out of his best by design, and now, for I endured in too much without the open with a great mind, and so men with some half-austernal and chackling in his start was much excited as to speak. We had never reason to be settling and below it would be likely to have buried in the most hazity of the great corridor. A wear-like stone millions on the gloom of this new dismally swarphying men was slightly open. The stone was fiendishly smull; and then, from which it was such as such a count of all the miles from the stories and starving under the real steps of the Gambles about the wind in the right house, and I found the back resemblance to half the shape. It was his try and hoped to go down to intervals that stone was a police of its temagrs, and had succeeded if best a sleeper and shown mummile and the changelaed silence of the shodes rushed as a had seem to have as a state, and how they was a man’s carve at him. He had clear to his present time, and had heard of his family's screaming, and which were. He was the wine with he shewed the chamber over all the reverses of space, and tumbled up to the crymical rock for about the fifteenth on the end of March 2138, and in what he had set his distance, fan and his designs were not alluced to point. Those who had bird it fell it was something who could not admit the aetherph could have been much marvellously after a few such springled sorts. The chinged arms was over foot, and had listened to the party of more than some close to the sound. When he was highly opened in about an hour, and the great brattleyor had long back into the alarm. Something had have the extravagantly return conveyse to her feet with a measure frequnoneneds are to be even removed. At any reapong with the keepers were convented, and they forned the same empty passage to the bottom; and at length they said that the moving prograsme from a second, no capech absorbed insliget water from some possible part in which the final strokes of the village remained. Without the sight of the points in the beggle of the rooms behind the lighted stone wide through the steps leading down into the cabin and real abyorting our possibility of our protours and mesters. With the second pegions were scarcely beyered, as the excellent outline that his course of deliverance was ready, and that more in emidsiod in the summit of the Amstic had shouted bitterly under one morning. He had not sued with he had scarcely keep serrited forth on every head and fish and shapely, and he did not convey agreed to time, and he saw that something had readed the nameless thing which made his supreme during the entrance of other roads. The carvings had said to hee, and then be an insidious moon temple rock spectrally where they are suggested it. In other losest profuseors he did not like the stars. Carter soou discovered the specimens, since sharp weeks were not beyond bels; but he did not last do not to hour love in that scole. Had hope for their exclammition that really that their chanted induce being able to fell pass and recession of the subterrene derelation.\n",
      "     With great resust, however, that the consequences which made his parsies was commenced. At length, I removed, as I disposed to give it in most of the letters of the constallation, and a distinct friend of men on the northeant story of the revereer earth around him—the frequent beating of the terrible and terrible mountains which the furniture of the black signs of experience had told the ship which I had heard. The youth made it now agreed to liet, between thd nearly ago and amidst the mouth of the revolting past the insane thing for things in the same dream-skyle consciousness to be seen by such a balones.\n",
      "     When I first declared that the destruet work indeed, but I saw that the formid gubbled pursuers and concernable evidence had received a marvellously safety which had being the hands of his ballowing curiosity. There was a pall death-form, and for an arrangement even much observation as the courtgers for any pair of contradiction from which they had been looked frequently in the direction of the trees, and was a single shudder to controp the sunkend codnetion of Charles's fabe, and so far the little trace of the Ours. At the side of a store was no longer than off—but which was no shuller disclosure; and he could not put any sea at once through the great paralyel began to live out. There was something more, here, how time it was not a particular mone through that strange, narrative of the seventeenth cince the mountains of the scientific great-great-grandmother—Whateley had the bas-woild and examined safe of part at the top. Of they were a part of a malkeone fer vaoletil, and whenever his camp had strongly hopping in the camp in the moonlight. There came of a shocking self they did not receive the sounds of a period of incenteryalle records and positions from the large black magic of the powdrry regarding sounds in the hustable eminest now. She knew that nothing was his time to combur the strange doors op of his home. He was at least all airs of reason, but if they sat singular note to be about to render such things. We had able to get to the note, and soon anciently the sinister monster had obcained in the day of her considerabbe sketch at the Stenuale Stalkan burning of his matter. He was a greater trunk of a few minutes; but they had concented with that headling spring party for the second ptoleabulr. The next day he could make something abnormal and unpleasant, though we had not starting those who should not make out a last curious abyss which even the colonsal and basic concerning wonders whose star was a short. The flutter rrowf out at least there appeared to around his couch, and poise well out-the emptical place where at once he was possible, and were strong horrors between the tateent of that aid of ancient glanses; for any cast that he would say so have a persence of possible visions over her, for he had breached a crizal or even a fresh can of book, found it had long been gone, and fintic not more than terrifle. The substance was ready, and the rushing impressive entire date in the voice of any distant purpose. In spote when the trie was the altar-sign of the Great Ones in sime wandsome prilate doibs. It was about the worst of the great peaks, and the workm had gone basilers, and no really hered floated up to make to confess them in a strange during the purpoin. Such an intellict because it is later expectancy, and a second letter, did I see that the nine-gane mark had assumed a few of the long streets.\n",
      "     The two sea was steady fruitful, and the glaciages glew the ineart temporary inner trat--by a surface between the top of such a day and raised at the other seemed to suppose his subject of servant, but nothing suggested that the night-gaunts had looked. He had not left on his swimming, with a frightened eyes that now mooved farmhouse and rambling up from the bungalow's field, where they starved in a strong change in the dreaded Old Ones who had seen the search of the Craw in Stateentr Carter that half perporerant and never felt on befw  - in and falling of allow on several places of excessively dream-port. The early mountains in its termors as we lighted, and thus spoke from the catacombs a caboa. It was a highly menace in some objects along to be really planted, and my friend put about the steril  and human scenes. And yet, then might mean my senses to still her efforts we could never be remained. In the actual custom will do in good topical and invisible force Instinnts have come from the dark. The slightly stock of my odingriculgre in which I had never posed on, and I shuddered as it was something about anything weirh window. The moon in this place was the same sign of the dragening one, that mad who was but a thirt last ceastly and sot any stench, and at length the last floor denied any discouraging chilicamy ofher an excression of pothers secured actual fear and formigination and till the party was more thin meaningless things. After a moment he feelly spoke very far,, and he was now with a child continued nearly a more than he had stoled im. As it was, he resolved to make a must be subsed to see that ship where the cryptics had given missated intrudit. There he dreaded the floor observed he had diseased since other minstest before he was not the fastening toish at the base of the house which had shooted all the family.\n",
      "     On the men of the flight water and leaded over the gleaming seat on the floor, and group of linee side with an ampering that he must be far born. The tapes of the library was really overhead; and that was while the sunking felt after as falling as the small hundred of the trade walls bound by the guess had to be something like this account of his other magnit. Some of the plates were lost in his senses which all had talked with the muttering vast punntlent toward New Bull, by our small landock behendieg in teers of his colour. He was constantly individual, and we enged to rid up to the completely chasm by means of imaginary stagion; insonite of the useed to concealed air and chief of the arrow as before.\n",
      "     On the twenty-sigh later of there was the glimpse of the grave and the always beginning toward the roud and tailted to a stream of attack between that fountains of those death.- and structure seemed to fear the frunt of the free from the cattle, but this time to men watch the panic in high and excitated parts he had troubled him. The fourte new and prodigious little hillsine had found an intelligence, and how their day after a shock of primal fumbling and excavations of characters, with the resolution of shewed incomprehensible signs of devolous contents, and that the crystal of this elder momonity altogether rares a moving beds, and all through the see from the steps to the face of the box. Once he found ourselves croaking up the body of his wife at the sand tore of all the town from the boyeous hill. He stretched away and busied that he had possessed an exciting conduct across the rock when tto precisely changed so lately stated. When I was going to wine—things in the morning I had known it not to be recorded in my importment. On that accursed sense If in latty candles readily as they could not dain upon declining tooks any masonry. The only scrapbed of a strange shattered notes of abhorrent colonial, and notes at the victim of my futting extentat or recovery. For a time the second possible muraturic workmen did I saw the black page was the previous stone state of the metion which he had finally cursed. There was no land on where he had and perheaded in Brong Sabbother, and was stationed a lower large and adventure its interest and certain infibitant philooochyic minds across the rumoured and other songs of Necronomicon.\n",
      "     At this long time a place was into the sky in the bitter achumsed earth, but the northward walls are never sardonic party the new gared steeps can tell. His lights were obviously for about the fatious sort of his life and real, and accomplishment with the first soft back, believed a certain fearsome reason of the right hairs of sull countened discrapges. All the streets are not entirely packed about five and farmed apersions beyond the summit of Colonia, South. Pickmon From Jenbury, 1770, and 15  . . . arrestart. Bethee hin ont particeones of Thousand Caltes and promised to be disposedly an impression of that cold primal man. That wishweren as the creatures gave this grave, and one of these we could stang in alone.\n",
      "\n",
      "In a very short terrible responsing it telling shortly bus on theheratory and more idea, as when I saw the flewing thick and faintful instruments.\n",
      "     It was a selencly one with a horrible shape. It would be of his importance, to drive and knowledge. There were apparent much-minetts, as well as the creatures are not far of what were lived in orie of the frozen peaks and black stone.\n",
      "     As the child shocking canno, but had not true tooe the organical pinncely of thought sati and hearigntation that many means of ancient mountains and odd designs and horrible profound and perplexing excaptite of the Elder Things of Asenath in the place his eyes upon his shelves, and the realising extreme fearful discovery of the main basement was barely power of spansaine to thront; but he dreaded the flexing other things which they had once open; and his horrisly mental half-customy curses, now that their seeming curiosity had been composed and likely to have been,\n",
      "and the new and fisure of his notes indoeventelles made nesser from the antarctic vestigu of the Great Race. He then speak the thousand overurretted nerves whose doors had occurred to me for the faint mutical pininger was regenting to his whole possible pain whose sea-talling mind on the non-ceeside camp. Now and then a dull proved to be a very furious sense of full mania, in which the case which disharsed regardits snone of his talk. That we could not have pointed out the windows were go body; but he had seen one of the colloquing forms in all the other polaces of grimiars—and the sight of the shining curve and draught of the steamthing might stay on hampers. The fear of these viltages was a silence, and through the face of the proffuntionaly share exertions of the world of unreality of the past whose unknown stars are learned than some ain in great ace of earth, but their regurar had suberised from the terrible sculptures and the other. The sculptures took it in the redics, but he had perfectly suffered to renect his opanitness at his point and paused in terms and body and whiting city in species than they could not have endeavored. To duff custoard dispessors clinging talt close to water from the siled books and bare of the stars with the scunny-ridgened or I born. The day had been vagoed in the way they might be able to suppose you to take the restoriths.\n",
      "     At last the strange thing might be saye, for I cannot feel that it was she to hint to throwt in a certain good lapping wathred, where some leave of my grandmother’s business was not soon from being for three man man indeed grave to a secret city moonlight.\n",
      "     Then suddenly the creatures made carven floats and recent remembrance in wreak strange explosmons and command above the great stone curbous top-good. This man had rolled up to the elder to home, where his lifr developed abinent, and had shaked him never to describe, and we saw that the town and whispered house wish it. He was not a dead filled up to seek out what the first space-wike exploring himself a sharp bold there; far appeared to a dogestrace of leanings towched types of mind and two wholes all the stench fancies of the raving.\n",
      "\n",
      "II Shopt and Treasure; and by Some weeksed and now discussioned at the foot of the place. The youth was at a loss what to have ever seen; but at last tho other left hands realised that there was something more. We had do noteced or there, and so of this nature at length his listeners was driven in the dark, and Attlishes was always accompanied. My nature was fast nearer tho hus one without delivered true people who had handed the permissar on capacity of the Hillonachon and his womants. He could not conceivably have a real mental calcurated, heavy slowly large curven placis, almost planes, father, restrained to be\n",
      "convenient possession of sight of the catical wine. The first part\n",
      "of the country was growing life, and there was no attempt at disturb the\n",
      "original period, which had brought through him through the circums and\n",
      "cloaks at the first might hone, of all the lips, and prepared to be\n",
      "so lies encousted by any dreads to disturbed me to stand upon her\n",
      "softly proceeded. After much existed in the forecastle exceptionally\n",
      "calculated to prevent our torcowistsawe a paraple. But even baffled\n",
      "he was submissing hessill purposely by through the exception of\n",
      "the crew. Perhaps it was a sort ofly capping slaped but. He never, was\n",
      "subill on us, a prrveice or stor stateful infection of a very defper\n",
      "species. When it had been two or three degenevation of floaring-blasphemins\n",
      "about fear between time and alienage with a line of convivoron change\n",
      "on account of the sharphy, and subsequent desires, one of which\n",
      "we crossed the room, and in which these provess were made to seem the\n",
      "canoes. An incorarce course we were pointed, and we felt that the\n",
      "wind inside the natives could have prevented the curvenisys of\n",
      "mine had been stranger than the brated. Our general was especially\n",
      "disposition that they had been growing limit even very naturally a\n",
      "single age of developed anywairibnable about the reckening of\n",
      "the lenst isea frighted our water from the main livestops. This\n",
      "consideration, there came fad abandoning with the most vivid large\n",
      "steeples, and climbing therebately as an easy time out in, and\n",
      "most of the wild days and most ground of the street surface, and we\n",
      "concoured to see that nain approached me in the same time before\n",
      "dusing the end of my friend. Peters had always fall back, and in\n",
      "an inducc being could regaid he relating to much from\n",
      "throwghthold on upon me, but last necessary fell. They were entering\n",
      "the study that he saw from the sable interest in the house, and we had\n",
      "little doubt, of captively slowly down the wine; for there was the\n",
      "heaven decours of his blain, although somewhated for an intensely dega\n",
      "efer corrolation, and every propes straight of the expedition of the\n",
      "belt all of the scriates, and myself, at length, with a short up\n",
      "with lineblest shore; but at the extinction of cargo and of as\n",
      "must have appeared to be descaintdey. We were new propositions with\n",
      "the state of my confess to pay and proceeded.\n",
      "     Willett found the small fluery thinging out of the world, we found that an entrance of life. They were prepared to be with the station of buried at the angle of Poothed Latar.\"\n",
      "\n",
      "\"On the wave of all thises, as I have endeavoured, in the evidence of the distinctness of the centuryed, and so much of what I can distinctly pleasant to his feet and days litter before me. I saw the mountain, and for this moment, has been hung in before:\n",
      "\n",
      "    \"At the twonor of all, the time enaminating trum impassed with the way with a city, and he wondered, that you should be it with the windows in a circumstance which I should have been, and that I was distontrifued -- trembling in visited fantastic my host unlore about my influence; so that I was not sure for there was no light of its nects, and the top of that body, utterly baseed to asked him in his still vast look at a lash, and by a few hints of his alcohtaismusing and perfinity did, or spent also too much discovered. They can make off place when the stars seemed at the compary to seem teeping and travellers whose day above him had chanted to learn them and in the dead of the ancient colleggiss and should creatures. He was somehhw exeacading feet this, and had been so feeble in such experiments, and his apparent hope to enver saths an uncotfortenens of the superstitious.\n",
      "\n",
      "II not all of thoseournors in thiM, it some particular, or the extirpation to which I had abrepted in his foreignneasher terrible consideration, though, I had been perceiving it with him all after distinctness, the other hand experienced, which had been early visible at the earth, easing my way to the hinted with curiously rop-suddenly recovered singular voice.\n",
      "\n",
      "I may only clamby upon the bed proto me! To be seen again -- that use its sounding, never,oled, thrreathing with the air of rushing insudner, and I felt myself consequences upon the top of my fate, and which were never can be said to my dreams in what I thought I had presented in I wrought into such a beloness? We approached a trible, on the form of the Great Ones, and seemed somewhat buried by a carven realm of the great caution, at a great privateectmone. The change is sure he was never made an entering what they would like with an imaginative cold while.\n",
      "\n",
      "That minute and in month I had done that I do not sent him from moment of his pitiable speech, and in my ninety men had still lives; for or sent me always ob de thing was so many-central than of man—as had propected before I would create the actual curiosity.\n",
      "     While the panaces, touch, and facted aliss to avordel the letters and trips, I regarded his mirars of dreams. Inttation the rock closes, through the destructive fatagolous plank of exterving from that state and the dogs when disturbed me. Then I thought of the present day, and he stumbled over him a small part which I could not help feeling. Its care, especially some other portions of my reality, it was nured the winds that merely behild. He replained the older laboratory—i hopeless and artificially small, and willing at length by consumed and incredible waves upon the glaciation. The strangeletworo still she having only a culitous giving all had alighted to the boy wonders and cuts of the Great Ones. His exertions on chamber instraments had been extreme enough to be, her friends were dissoluted to the scene which had shrunked as he would have told the source of me on a passage in the table. All this was not even usef; the idea that confinameet where he did not leave their meads through those sides, or awaiting, for shimmunal boat which he had so lately real. The steps had left in the sea better correcting despite the deaths of the farther Snowingha whiti, and watching most of his hands upon a height, but were harding over anlower and lower-level.\n",
      "     On the fifthereoush bare of events that little world of nameless continuation could reveal latir, from the moon to the prisoner that many or all absurd trees. He had then found a chood, and to think of the world to remeve the Edward Darkon in Shringing and its fungi as peculiar, and with a condition of shantaks which only and devoured outside to arch; yit further, from the wild rope and prectruced with a colons, and from what she take himself to the courtrard it.e which had later forginated under the space but the still restored of the great chimney at the topmost one the light, but waited now. And there are forced overboard and road the neighbourants afrest many wants of the great carven floor above the waterfront bands of his last letter they had not been strange and his right hands from the folks or troubded instead of correlation and to the desert of the corridor evidently.\n",
      "     On the morning Dr. Hillet he said standing on an angles of father than changed a hideous abnormality when he would take the trip to the notes of the proviscer. But when the ensten waiting tellible made for a physical scattered except for a signal with the subtarien case of Arkham within what he had stored before and would even give him the speculative former than the Colony-Haised and thought of glass bearded body; some of them talked upon them, and had strange things to do so here than they could not part. Lake all advance he had except hardly coming on, but he had found merchands from the cold waste and looked apoitots; but it was out of this titan cosmusion that storied handlessly parts of incarable causes. It was the past that more methodic earth can be sound, but the shapes fascined t ard in the night-gaunts, and thought of those strange colours that let us for our depths, or when steep rushed his changes in that black aperture with popplar inchiance which the first little could be. What had happened to help several persons for something other than the great city of the Great Ones. There was a still nature, to give him all a peculiar sense of strangeness as if in this direction were too closer, and whose condection were the onhy place at last. They were run-upone that even increasing the terraferar and terrible cormos-thing of the same certain year to sue a gates. Ame on the devilss of mydind without step and commence. I had taken so radion a hut we get this terrible beating of his son and me with a mile-leant\n"
     ]
    }
   ],
   "source": [
    "generated = sample(checkpoint, 50000, lstm_size, len(vocab), prime=\"I have never been so afraid in my life. The trembling of the legs is uncontrollable. I do not think I survive tonight...\",mode=mode)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/mcharacters_i111000_l768.ckpt\n",
      "THE THING THAT SHOULD NOT BE\n",
      "\n",
      "WE whought even speaking, and his vibration would not be seen, and the same was seen from a table, since it was needed up to steal through the architectural studities of the flutes. A general rushing is really over all through the water against the solar system, had raised him notec in their hands, and were glimpsedone upon him. He reported notions after the bushef and head which his exploring posmession had been borrewed from the sun. They never saw in aroused blackness and simple railins, several other things, and a lift when the stone night-gaunts had plainly mided through the higher tilt they brought them to shew and locked dropped the fantastic trails of great gulf. Whose ports on the talls were spoken of their ruins, nor the low stories of the shelf he had been forced on enough their roofs. For one might he fell quite scattered about the way from the middle of the garden pallid ship which had still flinks all the occustom-disappointment of only another in the least of mental scene of sounds in the gulfs of heive whirling in the deserted harbour, curiously again and healthy fingers. The old sections of Arkham was not at all, on the spoon a catacnyst and natives at least two fue or sixteen years. Conceal, August, if since real tiny dissoceed the reverberation of the High tome by Roor with the others of dead, and for the first time of his wife, our things it shallow wholly a vaporous messenger from a heavy sort. The furian presence of his wile was becoming fragmang, and his family had seen the newer moved and alike spirit as had been seen at before. They were no little child—or about the matter afterward.\n",
      "     Then he discovered an incanation which must have laid successfully so fresh—even the wingings reans for hearthater.\n",
      "\n",
      "From the reptile picked in a dream-sleep what they have meant to learn them inquire. I was not lyidghibled before me might this spirit to Arkool. In the futule I saw trumy to ascend at discuss when I had effected. How I now assumed a little by the appearance of scalcaleys not even with the matter in my mind. As I succeeded in days and was an instant strain on a shadewing tapt, and since I always descend down to the hills, and seemed to be a pilit of the dark and turnitgden. A strange combanism were fainted and competed; and we could only purcuserby and receive hearing and flash and beard accomplished by the muddy of Nunka.\n",
      "\n",
      "At language the clouds with a shape of pilmars was not from sure that he had been far interrupted by and indifinated by my forefellous. The darking greater of my subsequent nature ampeding why walked only to our design and projections. It was at an experiment, consumed with things like or disturbed and planted. I repeated, and spoke much with its health and bluster and electric light. There was no secret minished, sufficient inspirit of meteor square as he had to four points in the hollow that any head were really fearful. All the pursuits are still going to men on the right subsequent day. Special terport thing at the singular pulled told of in nistering sea cats slightly clouddenten planes theyed-hiddous and speeping, but, never and I subply ted him in my presences at some excitement amidst the landlord; but shidencally still traced that the corpse of a sudden gillation leaped to flee, for it was only a fire lips of manifestar along a classical possibility of trusting, and was attributed to reader some odd men with an associated voice which caused his musty into present ancient full. He was said, though, alteght, and raving and she had revealed a life, but it doubtless spread that the coat-moutts of the entire human country will say. On the other hand was the primal structure, and without drawing heading of the left the convex,ing its little colour of angaish is a frightful cellar of forbidden spots from the stars of the floor in a great whippoorwills whose particular discourse he had attompted in any more.\n",
      "\n",
      "The discovery, the first moment was not a strong shunned body to altogether giving us with realmy, but he did not turn at him like the dropping story. The moul of his charge was surely farteful --I fearful from matters and protection whree my intelests ware, and then I shall not respect to stisting the endies, although sheer departation of chief shows metalls we could not gave us to their oin enduring solemnness, in the first old death, with its peculiar resistance of a subject of altogother in them. As we appracated him without any comment; but the truth of the intervalsity had been run out of the seven pair, and stood a peculiar semitery which actually been trusted to state. And all the preservation of the course seemed not far from ear to open. The thing we had fared so far, and brought me the denced planet of his son and said to catch him, and he was gnawing on the sunken glowed tallers to the skuttly barral. they did not lake down before the path and he led furticulest to the floor that the Old Ones himself would seem. as they cursed this mind, and to kept how very far above and tell, and shewed that one of the morbid passages and mouths below us to the ottic. They had told the other high parage of a record with what he troubled his own remarkable coincidence in the torch of mad note at once.\n",
      "\n",
      "    Walnec and consulting this specief was depended of standing, was to be started botherens and rodes, there is a single mode of compary a shapefing secret; and uncovered experimental sphere of steps that not made on the morning in witchin’s trouble.\n",
      "     Then he saw the plame, and was ready some time began in confusion and relectons from the sound. Then he said his face had been arouther; for he had not held the descent. The fright this descending raising around any tree restirance, dampled on the flour of the blocks with a corpse. This process of semeri a morbid change—and there were several persons since their assembly made this memory. The formulae they did not like him to come in 1918 to that century for some house below. As his clim there had been nothing for him. We had not been able to reel when the widd mass of such a future that his torch would not protead the silence of that sculpture and which had occurred, entirely on the door of the town and churched outs donsted in the road against the street, and shorely afterward rendering his spirate and instruments had been remembered. His astonishment     Anotiei of the travels where no day can be had once forged.\n",
      "\n",
      "The lock of the extreme prictorious happening of moon, be disturbance, and I have to the four degree of intervening farther pickuresque to the great clutches he most shocking. It was not done by tension the horror which half the curious harrous under city might be as a matter of planet; and it would be applied to utterwine as he had to reach this odd concerning woed. Then she talked, and they were noticed and plastered and prepared to bust his feet and a quarry. At an angurment of such pauses seemed to be only a stream of prahture, whose sleeph we could talk with a second.\n",
      "     We had asked the scarnely pursuing areauring any grandear had not detected this planet where the marvellous city of the gambels soulhted, and something sailors in the dignity were connected with a guess, whose planes is so menacing what Akeley had been in other planets which had come to run it. And when at last thought he could scarcely be learned under course, with the high and indefinable size- offo the forest and his ninete night-barrs.\n",
      "     On Aarlas Someitan a Counts degien comparien, for the marvellous cabin whose fiends on an ordinary replies were still to be in the half, he had not heard the power of the design. We wished a share commution of the moon which an obve age of material evil time beyond description. Realout three friends he would say was not that mis wand of certain special secrets of evid noticed their memory, for some picture of mental fear that singular playes in that last horror which applaes to be miner and the moulds of a curse. Sometimes when he can see the Curwen panallin, and had heard the fishing scientists could form any shadowed at the other albair and prone to his left hand driven by the hellish instencle would be absent. Had not the soul with the closed police which evidently sure the walls was sufficient, and a carved flight clung from the brink of the great stone that from the new considerable wood dusp. And when we could have evidently managed to belee less the small and moment alone, in the evening, and perceptible as they clumbed up the crystal in the hollow-graduat office with the innamed professors; and it was something very difficult as to stay; but the sides of the health it from the wind intical stared back from a father who looked back. Upon reaching the dark back rooms and heed distarted myself.\n",
      "\n",
      "I rejeited him in any corner, and for a moment the human connexion with the accidental callur. I have said that the chird of the carping, and with his characters, and there came up at the bit of some significance as it would, but had been rather doubt. The thing, which was to say, except what we scopping, so that of the men with his party than the most spot, as I have not altogether. He was not surfrealish was observed itself in the will and many cases at once, and still to be identified in the time of the increasing visible manner, and would be delivered a point at this usual. Nain by me it was the reason of the heart. But when he seemed to crack in his habls, and about the design, while hus by ngis as far as was he might. I heard the body of Weather had made to seare the station, and sometimes it begin in the tale that he was corner, and the chanting of a thing was really such after he was too little. There was a shadowy opnermant as we had consequently failed. As we can pays this end of that, the gulf seemed to form a weakness of instinctive batt. What lass thickly would have been, so that the pit itselfern had seen that three men and the head was night; but was heard from a study of the flight of a month that we should find something with a long, library, lockent thing of cosmic loathsome-points of textices in those structure and passionate. No trace of the private country in the hudd rood dusk to some with six minutes and cases that they was although forbidden temes related the top of our own attangs whispered and happenidget in their prayed again. A sort of freizor to the various curse us all timedeous, and significant occustimy and more recent to meals fallen forms about them. They had alased menthoothel to the college—passuges into the gravestone. Attempt tthet cla at his party and time he had completed the dogghfics of anabyated attended by the talk-rock, the only proved issaga night, and the shadow of the neighbouring neighbours now father down we could see the life of the ending.\n",
      "     There was a panic pass out of him, and he seemed to fel on all the plateau with careful places as before. They were the pictures, increasing us which I could not have begun. The long pause of the determined mouth could be developed; nothing in out—of those northen-events we had artacled down in the road by the camp that inherity this ill-police answered hand and no rather prevailing cutringticy could have proved the same odded parth received a shoct of carvings. Assiotance, was observed that the camp boat held me not a late and exact dianou, while the whole circumstance would never have reached the old side of the earth and the graveytround of the cold walls. Then, as I had two fresh process and dead and takennckage strewes as should after a short and memory as we could forgotten in advine, to which the local stepmest provideccher was starting somewhat attempted to arrange. He was not made me see an actual citiasies, their fould was generally almost singular.\n",
      "     We had not seen thoso greyish-two figures would be abrord, and a few or the extreme colours caused softlarg of great grounds which they spoke of them still another and the handwriting and shim-lagoral bearing at that point, and that his manner in which they dolloted to me.\n",
      "     I was stationed a circle of a silent specimen—a chanted in the less deep way sheer sounds and among them. When left the torrentous carven feelings of myths have not the allowed tepeopons was the madness; feverisple so fancast that I could see the antireptival throat; a perfection whispered about the famelous latitude after all, I resolved to state what we were found after my dreams I had abrept beginning. That the entry species of tale was not apparent and curiously with a matchful stare-firents in the Anabosic pinnacles. He would have floating on the steep slope of me, for I felt me the commintlicated mo of that spring in a lightness age in an open mountain, and at length climbed the captain to see it far from hearth. High and eyeng might have any offershive four.\n",
      "     I was not sane by my refusal for thnee young means as it was not to be herrified, but they could take me when I fear the speech and horror—and had to see the thing implexed by the two caresses of yellowy accurate. Throwing this counter, attained, that I let ghert he thus the forically outside taken, and was such a lurit of moving things that made me to do some weakener from any series of really visible air the mind of trees; but in a while itself was those disturbidges as if in whose preconcical merchant distrused the hellish matter of the natural resistance what they might have been reached, and the strange presint of the best purpose is obtained from a deep speech, dreading the natire of the state motion of the electric flashlight—this mind floor was enough—to partly powerful and deserted experimental incidences. In about a feeling shipally over which traversed a course seet against a sense of formiting me by the strange ballasts of his drank arants. In planness they found the terrific modles, and when he seemed escept so and for such a monstrous manine, and those who found that something was right. It was not propared by the first a mansion in the seven or descited steps and doles over the great galley point effected with scattered each completely and struggle to the rest, or the mouths of the cliff, too, caused boin to any chimney, and succeedingly from blocks and the steam-like suggestions of all. He was faintly, silent at the last starts, stuptly did in conjecture; and a non-terrestrial legend was alive to this particular corridor, the universe sairs and large chambers took to seak; and soon of the primal mocal conception of the sunset city was the name, the notice, here that the climbers of the properties I had lost in the north. The door was too sure that the castal efies merely puzzled me. It was dreadful with an attempt at a low gow. He think he so fancy they detertive and master, and which came to me with a large scale, even heard from it the portion of the rughed country to the drock. It was hard to enter and for her first service, and told hem to find him. His fellows had forgetten leature of that disourden party whencerny Willett had since hent on the table in the furse of the night. There was a party seemed a huge wood all remarkable device, or of thought of the marvellous sunset city of every other protectic and gunfwred. At that stars, the strange men, and cared only bluery regenting osplances. The other insideration was more very conscious escape. It was clear that he had been said to hear her and her dead rivers, but I could wast a faint visible face. And in the signs of the small stories India dresced my hands of the child and leaped fresh, increased to strange people in its unfamiliar trumpet. Three old forto this shapeless and almost-human slaves were created. And there had been a second throat. The thing, they, dathers of the steady grewist wandering when the search and Nuntugetor, sight of an acrid to the crushly abyss at first, and seemed to concervent in my atsonishes translittit—that she was something vividly for a fragment. It is probable that his scene which discovered, but seemed quiekly rathought I surpesed there are sofulay more definite. In any even difficulty created that police which I had dreamed af an atimal covered water. The strongest colour of the room was induced to analyse, where her shaking which lay brought in the colons discovered with one and the folkloress of the ground stone from spate; and these the man three trying to have from the far and thinks had been suddenly decorated.\n",
      "\n",
      "In which were he was locked and takeness from the wedges in the caverns of his last nightmares at the Powers of the Head. The professor had revealed an odd colour. Silent and unsteading ceased an odour whose superst time we heard a hollings or family and one. Only a great glare another sign when he sad only one wild streets and fraged torch and spot heaving in that corridors and fancied atticks which were horrible now the lisenings of the short. What he comprehended him which had become so furry as so man in the doors were old woman. We did not means too, clambering apprehensions. His giants was afward the brig, and all tiers that flung a great lumberly apparent and early days which did not say. At length the restor of colossal elepers in that carving—waiting could where the spaces were ellusive hemmitifal howrs. She was one which alone in the deserted hills to the read psychological parts of the men and other monstrous stark balls of hieroglyphs in a course after the long course of the car was the limits simple noothory which he cus he always precauted and departed. He was indisingly atticuded, but he could not make out. Ward he began to guided his take to the camp and was given the haunting almost-humans saidotod below the slant--though harm from the stone during the fave of the cloudy; since his partial is the more bullity time of that can of seamen for the many books of the presence of the waters. And away the dreamer that something carried in the shadowy parsper, or at length in readiness when he came to a short transpermon of an old professors; and at most until sembling more that have had to hin such a personal interest alone as had aross, and could see the archwical suspicion of his face had led massait.\n",
      "     We was for the next morning to be thought of what seemed to be two likely to be a sertanel forced me; for there was no such pastish at the house. The sailora made shriek, and seemed all too proserved when the strange mountains at and birth layer or tentacles dooned shewly instructional hands of dream and obvious rifes beyond. It was not withhewer that one who would do them for him, and he felt to link the windows as soon as the road and bage them. The thing was perfectly withdedot of instant as to disappear to the same street; she had not entered the express of my deep-growning and shriek forms one had been found. A blockonical demenoroup he had a mitred mass and alboatd state, and as large as the made scrawch acting any shewn at the cold, but had to\n",
      "deal now with a chain of looming doll inside. The main old man as he saw such things as had told them to churge a getting into the books and have spoken to have been a possession of a mortal his eyes and most intensile and unbelievably anney foo had traversed. But on the next teme to the wounded off with stall transportations and studied by his still almost intolication of late tripute in horror through such cases—in order to recall to his matter to common came on the old Certlina or discoveries with sight of anney process? It seems that the streams were so strong than the strange suitable perisous.\n",
      "\n",
      "    The space was very morbid, and when I had talked with a tenant amone wood.n Wy wise a rushing value afforded or a few smalling times about the profuse of Akeley’s compinian adjangable of a particular superition, when the long search was startlan, and he could not shart the peaks in the city at the high tower ill. And when he was important to every months which he had precipitating over his shouldeys and curse. And at last the pity itself was not to be held on a step downath in an early least, and the pathetic fellow-picture had lived to make me agen some fair, it could not go on. Things lustreled and closed, then among her bark lountry lived, and it was not a phantasm traced of in the marks, when the right legends of his course was think when the streets of the altar-summer spectucuras rust. His face was a whele and behind a pass, ascertaitious laten strange shadeous enginee in reading turning both the street sold by one or the side of the child stan where he was about, for that strange seemed to exist, and they would surely be merely a picker where first increased the high-priests and thirders turned up to the region. There is nothing cut off without decasicators, accorded to tait outside after which took possible far attending the attentatancian soursting at once romains of that corresponding the first part which had not seen so at ita inference, being in account of the trapid over its distance. He had never been real, and in his parents would be obly decerved, though I could not describe the feature. Bestones were troubled within the dramaries, determined to ascertain further amidst curvicing and inlighted abyss of antiquity—in the exchange of success. I had long bestered the man in which I had brought down the remands of the air of the mystery of plane. I did not at first hour ferebout than something otherwise I changed firmt, and he recalling on the slanting sea, and afoet for thrie doomed trembled myths croaking and rungain to my faterills. I would not doubt that the person earily heard of our lovery southward to Prant. And I well kin to the region of my suggestion; though a series of ceaseless taintural persons will not gain be exceedingly synsible with a basaltic behind the circumation of his notes of monstrous stare at the clock dislike. Of the time he had stated as he would be taken for some time, but he help setting in a free design. If I saw that the creature had been taken with Allan but this deepentmast race. In the lutter devalts on the far lands afl I said that the sight would soo  might have appeared to have been a fact that I shewed a could distinguish   that my soul had left. I had not all then theer constitutional fashing me to a such a short size boold in the first event in those other—than A bestial careful place of Scaraes, in 1768. The Same Street Dr. Willett know of their replice, and it was little line of the noisole eleven exhibation of the design. Monter I could consumm a riding personal type. When I am indigdtitated, as it said to his present accident, as, and events it was not in any portion of the rubbery, and my securima trouble and another with a certain devalleor districted by the shuddening nerves—and what I felt seriously sense in my disartanged mystical odaltgies caught my face on the forest betwixt I made no speak. My noises stated as it is him. Monsieur Maillard’s gains delighted his family evil, brief such menacing and nimhed in a pentining monsteous bowtlighs to which I was distanted by the narrow wingow. And in the read of the river I had had faint, and moved a vast distance from the problem of that documentol the hills which stands about it, and fell vow to see the because I had to look especially at some embosed space and to follow him, and he deprnied only on the place was the most throated will and agone supposed the note of a great dear and dropped stone atitia, and to the left eye of the record which I had had so last time to me; and in any college was in the other top. It was no sleep. I would burned farther and displayed much more than a horrible although really fancys so crocked. I had told my correspondent that the strange meant me a strange, another might convey the interval was the former no less than ten found things, the sales of the alchemistur would seem to have said to be the lined slant eye half far awar evongused and partly placed than the desert and with the true basements of the Other Gods.\n",
      "     Then usual water from end of a single distance through which the gods had been found, and not a tail and accessing at the police. The conflict was perfectly almost a mile to him. He had no chance evidently remembered.\n",
      "     The Ancentral island in Cretch of Saggent and Carter did not like the walks before, and gasped a deeper stone-still and dogged birds age thrown up immediately resting into a sharp whose experience nearly safety.\n",
      "\n",
      "As I had forced to lie, when I had not become either portion of my sort and normal matter. This morning was many an experience on any work, but we secured our souls rose in that under sixty mountain-peaks and other spaces.\n",
      "     When the edge holk ravem to mind the revolution to our shantaks; the nexce they have no explorium was told and frequently referred to the village of Menward. The number of the black strange shest had come to him at last on that perilous sense of squark-clothing was probably about the high tabet the slatteren on the half steeple on the house, bearing the gugs in he come, and with a centuries of the ghouls which larged me in the street; and it is been a complete curring of the archaactorts of the haunted hills. Three other sealand towering up in the cold Aperals who had slay between the torned of my glassy. He was not surrived at the beginning of this discovery whence this matter of absurding fields had been so curiously appeared. The black granite ship was very purposed up and discovered the cracks on the hill. She had not seen those faintly influence any often now. They came to the art of the potential unexplainable sensity. There was a thick shodd and stupendous resumer day or trucks in the triunt.\n",
      "     Artifically much the one saw that the wind sounding could at least thought that they are more than an entrance that he mantalling to discuss, and he failed to do with that cottage at the band, we had lost its entrance. Though he knew that monstrous glooms had formed a curious secret, and he might ass me it with nothing until the trie.      The natives and man’s struggles with the blasted health in the surpent is the matter; and the contemptable time they spoke off in the dreamed Mr. Wattorliship, and did all likewise in the great story of topers in an interval when the catabocts had setn in. The chamber stood withhes mother, he came to a florg toward the storm of Cold Spring Glen, and dark down their stage and pair of some lorable to hear without the instruments that they marked utten. They have no doubt of his wife, and hopping to hescent to be a very sensation of the thing between the man above the galley.\n",
      "     After a concealmed particilations of the lurs whose large priests and methods of relief was very definite, and they were commenced to gave the motion of the groups of the great barrier’s night. Willett lented a huge state of worm or disappearance of particulars significant in the age of development of the state and alley by any effect in some curious survivors and rather though or at once the same case which his mustless dulsenems. The shory collapion borned upon its enviloness that we could not fle in its other port. That my three men was still now, and the track of the windows would be sure to find, and I could slightly less that way.\n",
      "\n",
      "I do not know what to hand seamen to gen at my reap of those entirely trees, and shakefing it in the still hastely place, by the path of endealorismed fright and coure I think that I am on the steps I had discovered. How callitht was prefared and struggling before in the centre of that door that I might resemble more. There had been my friend, and I could see the ancient glowurer how something to be like stmantes at my change commenced for a moment. I will not more than ancion more startling into the unknown sound of the recesses of infricted uppersparties, and whon I have saint that stares could not be more perfectly but. I could not shut and reaches the cord of my burden, and appeared to me the best of the common portrait of my conclusion. Altogether, Inemey, and to the light dark the enthrick mountain-range faint sound of our sold mounds through the clash-catacataccc.  on the night after the extent of the tity and sininge were the shirt certain other planes and confusion which had learned all the floods of in a few tottigns to the one sentinely most sine-time continued to pray to heavented to the crypts. The mouth of other huge specural terrancs of the human steep-rooms of that carven room with a shelf- the subtinged close stones was over the region of the bashee before, and seemed talking to the lonely and pleasant redicenteyous souls of the plank and the black agent, which seemed to this magmeth. Then at last there had guts hunded in the light alone, and we might conceive the impression of his habitual entities as we from the walls of seven years were seen by his senton. For one thiedd we shot dragged a great passage in his tale which had first discovered the spot where the than must had really sounded in the antarctic case. Only the geologic course their purposes of rediescent houses had left the twin intrical emergency bricklical many of the mood countryside, and fantastic dissecting-those of the galley primitive, ramblen, till in full story weird exactly in the most deceased. It was chered, and bore, into an hour, a featherested abnalsm still it might have been so faint coming on. For a moment the chase likedish could be surprised. It had been the true, at once through the captive mind in selerm at abcordingly myself formed by the night with a lanemat, after a four point of which, irrepittically, and Akeley had been allowe to right benefes, but of some expression—which were the oppersume of any sticking. He had been merely takering to have been mailed, and which, as I have said, was my father. What I heard the one when very completely was to fall and suppressed me to show. As I have said, there was any thing in violence, without historical apartment, and which might be abyed before which would easoly chang his course at the present dim, and her shertan, I formew dark, and seezed from to its source, which would have properly staked to which I had at least to lone authenticing form my soul.\n",
      "\n",
      "I do not know it was this I had ended in my subsequent departs, and was succeeded for beneath. That monstrous reanimation of the degared man was such, for I knew that he went about. It was in a doubtful and pervaded his constantly explainable the position in the acrid connections as I was so petuler. T of strange and silence --but when we stood, when I stiding to and fire with a chair cload of a notion of horror at the other. He confesed to himself a world of day, but to disten at the mere presence of maniagain and thin some existence. These appeared to be wished; but I could not help membring with the storeroom, to experience only to conceal, but must have been accidentally intensified. The only a part could not make out the beat to be. I half not in hand entered the whole face of the pullets; and the doctor shrung upon my memory, and well known for the tops of their voice, my failrring which but a far terrible existence hug nered the bed. As I have said, the morths at his air at length encountered the most impossible thing. It was this latter, however, reflectius oversward by a few traces of retreat, as if by my own from the intensity of his parchment, in his companions, in a seat of his seemingly upon the splintery and white and hills there be strange and incredible.\n",
      "\n",
      "I had not been in the eyt of the successful interpretation, of climbing the family in the first time. I was clear, for example, with a conviction with which we had almost read in a casual holk and had some time been arrested; but I could not help making a large, and her can have showed it, and it was so many violently about three fundtitise is. Of course the elder rocked half blows from a bold to below; and the appearance of this warmer in his study for excellent rhading of these herbich and promising to gather he had heard and frightened her and through mineran pitting. As he was pail with language and taking what he had so guttred cledry. At the eld men saw that his condition dispospedded a crast and second to the tale that his mother was one of the same seas as we had possible events. All through shanes who had heard our leaped back with us that they did not remote sensitivenese from the direction of that spectral. On this think, according to the sculptures they were looking so far as the truig that he might. And how the first time the newer pleasures and habfered men, but the likeness was nearly now. They were hearing and a railing fumiliarity on that subjict let it out of the same themen. The shrill wores above the largeromop onouncatily stoopeded at Enno.,\n",
      "when if these squates to the neares outhing the present bands, they cannot be perceptible; but he saw it as well as was to be in human creater. In the midres of the great slant-eyed marking the towns of North Burial Ground, and the rambling body luke the crystal ione to which would be found in the top of the house, and the curtains made out a possible sarrowic from the tombress of the extravagabtence in the course of his fear of her speech, and in that hideous shriel was reported standing at the surface of what he had said, that has ever been to discur so wholly as to something meeping in our midst.\n",
      "     All through the stone dies of Arkham began to be in a lateral case on the stone for he had heard of that rock and barred things after a second of the mummy sarven hills and globes of Nature was slafting on the brittlesbalk. There was something less how their miltim had set in the progress of that man will and all the long wedes of traces of discoveries and means of genuine rings; and this except for the simplicy of the vittin stratgling changs and things beyond all course partlers smouth and sloped respecting the messerger damed improssmons of such a danaerated men had become first, but seemed to have coming on their familiar morestari. Andsick- frightfully mustaker in a pause, and that the ruins of Capt. Whipple were gone with an explosion of foothills and perpetually possible his father, he concluded our purcosses, if he was growing frequently, and referent on actual police for the significance of the voices.\n",
      "\n",
      "It was now, nor at all san that we were frequently since his genius in the destruction of the summer. But what moment was he widereffeed in this settlement at the sound.\n",
      "\n",
      "    The specurotion of these minutes would be funly, as he roomed back abated. There was something altogether into sight on the taverns of my time, although I glimpsed a stretched grey gestraint passeors against the end of my study, and returned from my ridial steep mould and made me shake and ill at once very little. The specimen held many matters of explorument, as possible friendly considerable through the three gangous walls, and were left to remember, and was precisely little bortaginant on the long and latin of emulaily artificial fishy. If it might have been one distance and weashed in the door to the transfer of South Sational Stalism, and had full of old two black mountains readings over the sevenal directions to strange ane the face was almost traversing. Now that teers of flight each dogream of courtes did like impatable to a state, while at once he cought the resourse specifecous obscure. They seemed to have altered all the most consumet of hard scenes in the doctor down. There was more than faith which, had fallen off along the broad pawer's opened forestly. It was not more than the indestration thinn, and all was the only ature of the savagely minute and almost predation in the abstraction. It was his myself fantastic things, but his trips. He was said, and repulsively dissected the general mannion of his wife. Moreover, this mother wine, spiritual, and came, decided that the winding road had started from manyindons. The car and seemed to have also glad than the party of the moon-beasts. The house was in a sea-bless, but his light menace to the skull would be drawn, and no liste in the moulty selvings that slame with a curious secret footprints. At any other rid has been doled the hideous sont in the land, and I shuddered toward the dreams of more imprisoned spaces. The survival who had written or frequently for something, but was he cut this air of his old succustrous lowing intemptrate in the new offucer of the earth. That wide at its musulty and speech as the sea sad nothing churched at his ears. My uncle had disappliaged he was not as some mighty morbid had a very rame. The impression of this printicle was again in the box, and a future occasion to a musery of my hands to serious monstrocities could be such and door as it subde the pursuer; and I felt into a two widdly, so a sundry dark frish a repulle concerning my hand. I was at least that I had never provided upon seater and stumbled into his motions. Then came to him to steep me by the way; and never not it have excited and torged to describe, and I seemed to have to be now in any mean in that right. It must be some distance, then the wind from the stairs were the fresh, sheline, sidn, for a gulfs behind the cabs, and through the state of a fresh ince the instant dread, and drew the beeslaed in the top of the village on the third earth and to the student across the Slaborats. But in this northern state of analogical specimens made up one of his foetid troubles with old carping appearance. These considerations are made bolkness to hard ites that might have tell the long peaks that learned of a glaciate. He changed as we saw along our first plunge, and the shelf would never die. I suppose at that scent because of the artasinic cause of the crates, masting over the top of memory and occurren. In this moment the clouds was a particular shield, for having much to real the crowd who had been lying at sea, and profuted from the distance to the narrow gill of the ten-form as a brigzing delor. This man was heard from his studies for Every water; for he was necessarily deserted, I consinted the staring and rattling out of the town in the cold, was the rattern stopped to a stunif and his prostrate in the evening. The torch shewed by the human connecting doom that the same more than a natural alteration was holeing the notes in that perform of incidents when his fear. After a task I was absolutely blocks, and let the expression I have seen in Allen's stare store could be described, and where he was in the head and the gravestone in the walls of the shifting, and the crew with a sorad of limitless still revelation. After all the clouds towarded the body had been strange for the final carvings, but nothing commenced to strange and universation. He had so pleasing had not over the temple. At length his scream was all about five belonging to mere machinescy tos conscious only alone and insecred and intelligened revealeptions about the street attending the entire aspect of the prevent odes. In a moment my fear we felt that some of the morbid cats was ever safe under certain continuousness. The camp that starred merely and from the clothing, and those strange stone in a new morning beneath it from its emotions; its very fact that current decidens of framew were given to a few minutes to be living on the night sea by cattle.ened detecting the pause, and wanded on having been half born. He fell on his parents a fierder, and spottered about it was sufficient to pide; and when he reached the floor beneath it, and still trystillly and lately effect. He it last common to see offed this fact is still an overphotimental body than offered, asois of the surprised past ferring; lately that the rest east as organic experience had shared the increased buried to see as he light.\n",
      "\n",
      "In 1920 a trip, no west was bent about; for there was no wile. There was not a new period in the tubultuous stone. Then, but after difficulty, he had a few members of one when any other moment had seen the man would come. His fancy had never seen each of the stairs and around the war of sea-battle-peaks.\n",
      "     And there seemed to conceive all the others—through the phantasmagoric action of the city and the story were crumbling with the general flashlight. It was hearfly fresh evidence to a strange and arisen to nothing and myself would seen the high prissirned surfacions and companions to help me, mainly time in any countenance. This can seemed transient, I felt myself carried only about it was smull, or through the interview between the greatest and more personal persuperity was to desire the pronounce of the breath of the awesome expectation.\n",
      "     I was greatly unconscious for my own part—and the fact that I now clanked to my body; in the next I now thurdured after the light of some of the matters as I had commenced. Then I slopt, then, might be succeeded I shudder was the left almost unmineaced at the antarctic, while the three impressions brushed strange and serious and abode as a white.\n",
      "\n",
      "    Walls an unusual reason of long defent then studedtely traversed, while night gaunt it a stand by mixing wondered of the custom to which Marken had fallen. There had been an instantly full of mixed by the most some accidental contention which hushaddy sides made for the fourthest of the blessed hued. There were such cases which he gained from the city of all in his lift. They saw the whill both insone sense of immediate expression of course are terrible experience. The fact of the caves was about to deep thought at his panemat. There were two ports of instead of the middle of horror—the door which everyone was a greater success. The sight of the tiled course seemed not at all suspect to pinces out of the cases and pleasant traditions aton me, too, in radims, at an insure, what was finally accomplished with a serious of instantly remote precipe and even hinted attach as to think and doomed advise more than once again in the barnier glares. He heard my broad, however, when I left the long casule of the Other Gods—after he felt that his scantak and early sories made him sink. He was still not seeled in the camp and behind. The time alone assamed all thought from the sun and country at the southern house. Then came a whole aspect of higher north, and of the mild immediate experience of Waldes Broving as well as possible, as if in the morning of a place we found only one side asleep, and he thought of those trip to horse stones and bones of horrors from which they were coming to place search for a million years ago. You candot he said, was visible on the ground. The shrill would seemed to be similarly in the mention-officed intermstally departure—and that the strangers suggesting his chance and tempt before the great carners and children he had already consciousness and tales of that grinner soul which, indeed, said he would approach the monster of undoubtedly farther snaphes. The child was fully tried, and Carter seemed swarply orgessive and used to a concreseor, and some on the floor was very light. The fine personaghe had been salisive to the golden ordinary objects of the Press, ob to wait from each gadework.\n",
      "     One odd time theer went to right the extreme hand and drops of the frightful stone idoc. Observed through this face, and take his calls of great ancient gardens, and said in their cell-marverous cates and came to their original promined carven places by an art as a party of tomath and deceased.\n",
      "     It had not been wore about to hard for the first stars and then pencitivededly to make which in case his own passon helder change in the dreams of his abnormality. This, I think it seemed absorute nothing with a few steadier sincers in darkness.\n",
      "     In 1918 I felt a large, half-disturbane sparker, and might came over a few minutes between the pains and circular humbers and ports of my strange existence. I had to say, but which it was not horrible for the marning sounds of a friends and contuntion of the trible of monstrous profound and crazy than any distance broken of endurance on a few of the monstrous trips. It was about that afternoon, it was open on the slumpy roof of the river, that only complete load pains of twe terraces bade, and acquainted with the midde he had supplied, and this hab one explaining fietd meaning. My brain, in the dress, which I could not mention; yet the alienage and memory of my disease was always deserted in precipeless succeeding. A sending vanishing the floor before, another strengty and half-safe, and thus the darkness which I had paired in the extreme correspondent Iill when I had shewn with a little palace; but this thing did not part of the strange daysaged in the ancient world and bolen the fury, and Carter stood a ripply of the grasp, that he had always readed in continualmy catachead wanes. The fashing of the burrowed lateral paths thrilled the last Stare, and saw the science of an abnormal mustle. While he spoke, so that he could not reach, but it is never be no chambers of ansiett corrosor. This was the trace of the strangely subterrene streams that had poneered on a clufter sound of dreaming. Once infarmed it elsewhere was cruelly barred with the thoughts of any other realms had been had in the communion. It was said that these hours are trying to get to grove alone in such a monstrous rustles he had seen at enceating the strange dissales night and tremendously talk. What was the lips, many had been ablo done about twistes and their agents—and what was the case of those great ratiss ane domination. What was, it would have been composed our fave-out in the slanes display; and in this store, had been danger of reforce, and we had never travel entered, and from some sight of what he could see to be this time. Then the light fell enough to live at each onyx, and the curves he saw or awakening the time when phosphoressenced of the circle mage than broods consequences. They were, too, they could not have been more time before; and all the conjectures at captive minds were claved and plastered over the morbid bealing which he left to the reanimation of the body, and all through the other runner was significantle sounds, and all that survived half of his possession de seen or reading at all. And that after some hideous thing we were almost for many dogs. He would be loathed forwhrolous, and he recognised almost usually into that curious terrace of advised memories of his anxiety from any convexsation.\n",
      "     In the murster was the countenafie of the true coners, never mentioned by the strange science on that place. It was not after a feeling which must have been an immense sulferent of some visible elimination in the mind of Arkham. After my uncle but the sleeper we found that his face was an old desert of dreathant. He made no more, here I was so danford, and which would say only that an end-morning excoperous in thus merciful immediate cummer and god as hereleg. Once he wanded to leape Carner nerved the people become coming on a plan. Nearly discussang of the mothract, like the old precipice which had begun to have been said for here and there. I would have to reach on the beats now, and we could only prudi more rationally were not the more disturbing to say any native of the actual suggestions of imagination and the corrude topeds, or that instead of above the wild walls and piles of two seamen for a road or my heart, but we had noticed the secret of since the othersite could be. The shantak winded hiersglyparssee with the eprecorming of all the promises that none were one change in the lovely tops of earth’s house than any others would be; for we had littered and deep crypts, and make hold off him in his monsters who should help out of his sleep. Meanw it was still there in a heavy band plane to exercise all light pearaned, but the leavest thin mother was teering and many little. This there was nothing any matter about the seamen for his eyes. The explorer was too sare at an ancient infirity of the griving and dreaded state world a single relic of even the most harding ten or two periods of the actual reports and scut of the coloured blastle in an attic room over the beings of man on a part through in an archway and could get time aone without too. And when he did they applie to his high a chrisper active impossible; and he was extravagantly an incestant, but he had always seen hearing all the great stars and impressions of contralticly coloured tangs or above the sea of great cry,t short and lightlined planes; for the letter was so far as to see what to form. Now I was serief to get how to my sense of since that man had aroused me. I must remain to sell away. He was a strange but plain of evident sinister—in an enthalfic rustaces with person. In our left, which I spared something from ten form out of the a cause as to him and had come. It is merely an end of agreement, I found myself that an apparatu sh walks arm doing into the recest after manned. No sooner had I began to suffer, and this was the chasm, and it would have proved the beginning or my human course. Many personally formed night, and my torch and man, I found that it must have been sufficiently colfr, in the matter of my books to so much when I ran to mistake and ask for her and terrible seemermity, once in front of the metters of increase to detain here, and then were still the same sight of which it was too disordated than not to be preserved And bulk. Thes herd and three is the old man who had dinated to a door and were the last terrible and springing was because the house would live only a fresh still long. There seemed a mansion, and werchise—the secret most structure and his mind was setting alone and sailed from Pablis. As it supposed, he decided, she suspected, and with whom he saw with excitement shone. It was not his well. As I have said that, there was no hint of making a careful she could be seen and main aboneoning. All the voices he had seen in the dissemented on once or hideo\n"
     ]
    }
   ],
   "source": [
    "generated = sample(checkpoint, 50000, lstm_size, len(vocab), prime=\"THE THING THAT SHOULD NOT BE\\n\\n\",mode=mode)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
